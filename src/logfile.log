Data Reading ...
dataset.shape: (860063, 6)
dataset.keys: Index(['TweetId', 'Text', 'CreationDate', 'UserId', 'ModificationTimestamp',
       'Tokens'],
      dtype='object')
Data Preparation ...
DataPreperation: userModeling=True, timeModeling=True,preProcessing=False, TagME=False
DataPreperation: 1 sampled from the end of dataset (sorted by creationTime)
DataPreperation: Length of the dataset after applying groupby: 143749 

DataPreparation: Processed docs shape: (143749,)
processed_docs.shape: (143749,)
documents.shape: (143749, 3)
Topic modeling ...
TopicModeling: num_topics=30,  filterExtremes=True, library=gensim
adding document #0 to Dictionary<0 unique tokens: []>
adding document #10000 to Dictionary<51587 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #20000 to Dictionary<76418 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #30000 to Dictionary<103425 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #40000 to Dictionary<121574 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #50000 to Dictionary<141111 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #60000 to Dictionary<156972 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #70000 to Dictionary<170960 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #80000 to Dictionary<185471 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #90000 to Dictionary<201551 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #100000 to Dictionary<212544 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #110000 to Dictionary<228412 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #120000 to Dictionary<239768 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #130000 to Dictionary<252399 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
adding document #140000 to Dictionary<261370 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
built Dictionary<266226 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...> from 143749 documents (total 3140638 corpus positions)
starting a new internal lifecycle event log for Dictionary
Dictionary lifecycle event {'msg': "built Dictionary<266226 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...> from 143749 documents (total 3140638 corpus positions)", 'datetime': '2022-05-27T15:42:22.365391', 'gensim': '4.2.0', 'python': '3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}
discarding 166226 tokens: [('charity:water', 2), ('my Master', 1), ('Internet - Uruguay', 2), ('funny face', 2), ('coldcuts', 2), ('Get FREE', 1), ('Duke Nukem Forever', 2), ('feature bloat', 2), ('embrace & extend', 2), ('drexler', 2)]...
keeping 100000 tokens which were in no less than 1 and no more than 28749 (=20.0%) documents
rebuilding dictionary, shrinking gaps
resulting dictionary: Dictionary<100000 unique tokens: ['Beautiful Things', 'Earth', 'Scorpio', 'We', 'author']...>
using symmetric alpha at 0.03333333333333333
using symmetric eta at 0.03333333333333333
using serial LDA version on this node
running online (multi-pass) LDA training, 30 topics, 5 passes over the supplied corpus of 143749 documents, updating model once every 2000 documents, evaluating perplexity every 20000 documents, iterating 50x with a convergence threshold of 0.001000
PROGRESS: pass 0, at document #2000/143749
performing inference on a chunk of 2000 documents
1742/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.007*"Apple" + 0.007*"iPad" + 0.006*"Facebook" + 0.005*"U.S" + 0.005*"Twitter" + 0.005*"US" + 0.004*"iPhone" + 0.004*"Google" + 0.004*"Nuance" + 0.004*"Music"
topic #19 (0.033): 0.007*"Twitter" + 0.006*"Thanksgiving" + 0.005*"North Korea" + 0.005*"wikileaks" + 0.005*"Obama" + 0.005*"Facebook" + 0.004*"will" + 0.004*"video" + 0.004*"US" + 0.004*"Wikileaks"
topic #6 (0.033): 0.008*"Facebook" + 0.007*"U.S" + 0.007*"Google" + 0.007*"WikiLeaks" + 0.006*"China" + 0.006*"Twitter" + 0.004*"North Korea" + 0.004*"Green" + 0.003*"Black Friday" + 0.003*"YouTube"
topic #15 (0.033): 0.005*"Movie Review" + 0.005*"Twitter" + 0.004*"Facebook" + 0.004*"China" + 0.003*"UK" + 0.003*"post" + 0.003*"England" + 0.003*"Art" + 0.003*"video" + 0.003*"PM"
topic #28 (0.033): 0.008*"video" + 0.007*"Facebook" + 0.007*"YouTube" + 0.005*"Twitter" + 0.005*"Wikileaks" + 0.004*"Mashable" + 0.004*"lol" + 0.004*"TSA" + 0.004*"China" + 0.004*"Android"
topic diff=28.830570, rho=1.000000
PROGRESS: pass 0, at document #4000/143749
performing inference on a chunk of 2000 documents
1964/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.012*"Facebook" + 0.009*"US" + 0.009*"WikiLeaks" + 0.008*"horror" + 0.007*"tweet" + 0.007*"Ireland" + 0.006*"Irish" + 0.006*"GOP" + 0.006*"help" + 0.005*"Wikileaks"
topic #1 (0.033): 0.015*"Obama" + 0.013*"TSA" + 0.009*"tcot" + 0.009*"GOP" + 0.009*"iPad" + 0.007*"politics" + 0.006*"House" + 0.006*"WiFi" + 0.006*"Hospital" + 0.005*"Senate"
topic #22 (0.033): 0.018*"Gracias" + 0.013*"Blog" + 0.011*"users" + 0.007*"Hollywood" + 0.007*"iPhone" + 0.006*"iPad" + 0.005*"stitches" + 0.005*"elections" + 0.005*"Obama" + 0.005*"who"
topic #18 (0.033): 0.011*"cablegate" + 0.010*"ha" + 0.010*"Wikileaks" + 0.010*"Asia" + 0.008*"Pakistan" + 0.007*"hay" + 0.007*"China" + 0.007*"who" + 0.006*"WikiLeaks" + 0.006*"Assange"
topic #14 (0.033): 0.024*"tcot" + 0.007*"Pakistan" + 0.007*"Obama" + 0.006*"US" + 0.006*"T Magazine" + 0.005*"Gays" + 0.005*"Prince William" + 0.004*"Afghan" + 0.004*"India" + 0.004*"Pentagon"
topic diff=0.340387, rho=0.707107
PROGRESS: pass 0, at document #6000/143749
performing inference on a chunk of 2000 documents
1987/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.015*"Black Friday" + 0.011*"Thanksgiving" + 0.011*"North Korea" + 0.009*"TSA" + 0.008*"U.S" + 0.007*"to show" + 0.007*"DADT" + 0.007*"free" + 0.006*"Canada" + 0.006*"Happy Thanksgiving"
topic #3 (0.033): 0.015*"AP" + 0.012*"London" + 0.011*"Portugal" + 0.011*"Ireland" + 0.007*"Venezuela" + 0.007*"police" + 0.007*"UK" + 0.006*"China" + 0.005*"British" + 0.005*"German"
topic #0 (0.033): 0.015*"Haiti" + 0.012*"security" + 0.011*"people" + 0.010*"Yahoo" + 0.010*"Palestine" + 0.009*"TSA" + 0.008*"Kinect" + 0.008*"Gaza" + 0.006*"Egypt" + 0.006*"Sky News"
topic #1 (0.033): 0.018*"TSA" + 0.017*"Obama" + 0.015*"GOP" + 0.009*"tcot" + 0.009*"House" + 0.008*"ppl" + 0.008*"politics" + 0.007*"iPad" + 0.007*"gop" + 0.006*"app"
topic #23 (0.033): 0.071*"AP" + 0.012*"via" + 0.010*"turkeys" + 0.008*"Pope" + 0.008*"Mom" + 0.008*"Twitter" + 0.007*"Amazon" + 0.007*"Facebook" + 0.006*"Disney" + 0.006*"lol"
topic diff=0.238682, rho=0.577350
PROGRESS: pass 0, at document #8000/143749
performing inference on a chunk of 2000 documents
1980/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.014*"Blog" + 0.009*"ipad" + 0.009*"We" + 0.008*"members" + 0.008*"Saudi" + 0.008*"will" + 0.007*"book" + 0.007*"users" + 0.007*"Stars" + 0.007*"who"
topic #13 (0.033): 0.010*"Facebook" + 0.009*"App" + 0.007*"AP" + 0.007*"military" + 0.007*"U.S" + 0.007*"WikiLeaks" + 0.006*"Twitter" + 0.006*"will" + 0.005*"charged" + 0.005*"Irish"
topic #28 (0.033): 0.018*"read" + 0.013*"Video" + 0.012*"Facebook" + 0.012*"Q&A" + 0.010*"TSA" + 0.010*"YouTube" + 0.009*"music" + 0.009*"Windows Phone 7" + 0.008*"Twitter" + 0.008*"Google"
topic #19 (0.033): 0.023*"Israel" + 0.015*"Palestinian" + 0.012*"Time" + 0.010*"Twitter" + 0.010*"wikileaks" + 0.010*"Today" + 0.009*"Fire" + 0.009*"Washington" + 0.008*"Qatar" + 0.008*"Money"
topic #14 (0.033): 0.036*"tcot" + 0.009*"Pakistan" + 0.008*"Obama" + 0.008*"US" + 0.007*"NYT" + 0.007*"Iran" + 0.007*"teaparty" + 0.006*"Police" + 0.005*"Wiki Leaks" + 0.005*"TSA"
topic diff=0.385562, rho=0.500000
PROGRESS: pass 0, at document #10000/143749
performing inference on a chunk of 2000 documents
1991/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.030*"AP" + 0.024*"Vermont" + 0.010*"via" + 0.009*"Pope" + 0.009*"lol" + 0.009*"Day" + 0.008*"turkeys" + 0.008*"Dallas" + 0.007*"Twitter" + 0.007*"hair"
topic #28 (0.033): 0.093*"YouTube" + 0.073*"video" + 0.014*"read" + 0.013*"Video" + 0.008*"TSA" + 0.008*"Holiday" + 0.008*"Facebook" + 0.007*"Cyber Monday" + 0.007*"Twitter" + 0.007*"Windows Phone 7"
topic #13 (0.033): 0.008*"military" + 0.008*"Facebook" + 0.008*"App" + 0.006*"AP" + 0.006*"WikiLeaks" + 0.006*"Twitter" + 0.006*"state" + 0.006*"U.S" + 0.006*"Julian Assange" + 0.005*"will"
topic #3 (0.033): 0.017*"Portugal" + 0.010*"police" + 0.009*"London" + 0.008*"Ireland" + 0.008*"read" + 0.007*"AP" + 0.007*"UK" + 0.007*"Mexican" + 0.007*"cover" + 0.006*"China"
topic #24 (0.033): 0.020*"Google" + 0.019*"Oh" + 0.017*"Groupon" + 0.014*"who" + 0.013*"Leslie Nielsen" + 0.011*"Arizona" + 0.011*"Yes" + 0.011*"funny" + 0.011*"tcot" + 0.010*"facebook"
topic diff=0.178030, rho=0.447214
PROGRESS: pass 0, at document #12000/143749
performing inference on a chunk of 2000 documents
1992/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.064*"para" + 0.058*"un" + 0.023*"como" + 0.021*"en" + 0.019*"la" + 0.016*"más" + 0.015*"dos" + 0.013*"es" + 0.012*"se" + 0.011*"Un"
topic #3 (0.033): 0.015*"London" + 0.012*"Portugal" + 0.010*"police" + 0.009*"cover" + 0.007*"Ireland" + 0.007*"UK" + 0.007*"Mexican" + 0.007*"men" + 0.006*"AP" + 0.006*"companies"
topic #23 (0.033): 0.033*"AP" + 0.017*"lol" + 0.015*"Vermont" + 0.011*"haha" + 0.009*"hair" + 0.009*"Disney" + 0.009*"turkeys" + 0.009*"via" + 0.008*"thoughts" + 0.008*"Dallas"
topic #9 (0.033): 0.014*"twitter" + 0.013*"TSA" + 0.012*"will" + 0.012*"airport" + 0.011*"Americans" + 0.010*"play" + 0.010*"we" + 0.009*"time" + 0.009*"do" + 0.008*"talk"
topic #12 (0.033): 0.041*"God" + 0.019*"Who" + 0.019*"al" + 0.013*"laptop" + 0.013*"NPR" + 0.010*"left" + 0.010*"update" + 0.009*"us" + 0.008*"Lol" + 0.008*"next"
topic diff=0.155579, rho=0.408248
PROGRESS: pass 0, at document #14000/143749
performing inference on a chunk of 2000 documents
1993/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.019*"blog" + 0.014*"photo" + 0.013*"China" + 0.012*"NATO" + 0.012*"Obama" + 0.012*"Japan" + 0.011*"Toronto" + 0.011*"cancer" + 0.011*"Chicago" + 0.011*"up"
topic #27 (0.033): 0.031*"iPad" + 0.029*"iOS" + 0.019*"iPhone" + 0.012*"cc" + 0.012*"Complexo do Alemão" + 0.012*"DVD" + 0.012*"release" + 0.011*"Amazon" + 0.011*"Black Friday" + 0.011*"post"
topic #2 (0.033): 0.012*"EUA" + 0.012*"Report" + 0.010*"Tom DeLay" + 0.009*"Jewish" + 0.008*"bit.ly" + 0.008*"You" + 0.007*"Boston" + 0.007*"Gulf" + 0.007*"NFL" + 0.007*"Wall Street"
topic #25 (0.033): 0.063*"today" + 0.033*"out" + 0.025*"lol" + 0.022*"us" + 0.018*"Daily" + 0.017*"Thanksgiving" + 0.016*"video" + 0.014*"tweet" + 0.013*"turkey" + 0.012*"people"
topic #6 (0.033): 0.034*"jajaja" + 0.012*"WikiLeaks" + 0.011*"U.S" + 0.009*"China" + 0.008*"Study" + 0.007*"Google" + 0.007*"Obama" + 0.006*"Health" + 0.006*"Pres" + 0.005*"Europe"
topic diff=0.157189, rho=0.377964
PROGRESS: pass 0, at document #16000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.011*"military" + 0.011*"Rome" + 0.009*"DADT" + 0.009*"Portland" + 0.008*"weather" + 0.008*"Facebook" + 0.008*"Julian Assange" + 0.007*"founder" + 0.007*"will" + 0.007*"Blair"
topic #19 (0.033): 0.028*"Today" + 0.022*"nice" + 0.019*"Israel" + 0.015*"yes" + 0.013*"big" + 0.013*"Tea Party" + 0.012*"Time" + 0.011*"Music" + 0.011*"internacional" + 0.011*"Wow"
topic #21 (0.033): 0.026*"Chicago" + 0.018*"blog" + 0.014*"Toronto" + 0.013*"NATO" + 0.013*"Obama" + 0.012*"photo" + 0.012*"China" + 0.012*"up" + 0.011*"condoms" + 0.010*"Nation"
topic #4 (0.033): 0.016*"North Korea" + 0.016*"Obama" + 0.016*"U.S" + 0.015*"South Korea" + 0.012*"WikiLeaks" + 0.011*"Cancun" + 0.010*"White House" + 0.010*"S. Korea" + 0.009*"GOP" + 0.008*"N. Korea"
topic #20 (0.033): 0.021*"Twitter" + 0.021*"Watch" + 0.018*"live" + 0.017*"Florida" + 0.015*"Hope" + 0.015*"families" + 0.014*"Nokia" + 0.014*"Check it" + 0.014*"dia" + 0.013*"TV"
topic diff=0.100828, rho=0.353553
PROGRESS: pass 0, at document #18000/143749
performing inference on a chunk of 2000 documents
1997/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.033*"God" + 0.021*"Who" + 0.020*"al" + 0.018*"NPR" + 0.014*"left" + 0.012*"DREAM Act" + 0.011*"local" + 0.010*"poor" + 0.010*"Who's" + 0.009*"National"
topic #28 (0.033): 0.063*"video" + 0.058*"YouTube" + 0.018*"Video" + 0.014*"read" + 0.012*"Cyber Monday" + 0.010*"haha" + 0.009*"music" + 0.009*"journalism" + 0.008*"Facebook" + 0.008*"past"
topic #17 (0.033): 0.025*"attack" + 0.025*"North Korea" + 0.024*"artillery" + 0.018*"South Korean" + 0.017*"Ireland" + 0.017*"South Korea" + 0.014*"Reuters" + 0.012*"Irish" + 0.010*"Guardian" + 0.010*"bailout"
topic #15 (0.033): 0.017*"Thai" + 0.013*"snow" + 0.012*"USA" + 0.012*"travel" + 0.011*"here" + 0.008*"Thailand" + 0.008*"flight" + 0.007*"xfactor" + 0.007*"demo 2010" + 0.007*"NHS"
topic #13 (0.033): 0.013*"military" + 0.012*"DADT" + 0.010*"founder" + 0.009*"repeal" + 0.009*"AP" + 0.008*"Facebook" + 0.008*"Rome" + 0.007*"Julian Assange" + 0.007*"weather" + 0.007*"Portland"
topic diff=0.112896, rho=0.333333
bound: at document #0
-23.086 per-word bound, 8904914.3 perplexity estimate based on a held-out corpus of 2000 documents with 20435 words
PROGRESS: pass 0, at document #20000/143749
performing inference on a chunk of 2000 documents
1996/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.039*"AP" + 0.012*"Pope" + 0.011*"turkeys" + 0.011*"Dallas" + 0.009*"bit" + 0.009*"lol" + 0.008*"Disney" + 0.008*"Film" + 0.008*"Gay" + 0.008*"ok"
topic #27 (0.033): 0.034*"iPad" + 0.031*"iOS" + 0.022*"iPhone" + 0.020*"post" + 0.016*"DVD" + 0.016*"Oprah" + 0.016*"Line" + 0.014*"blog" + 0.012*"Black Friday" + 0.011*"Amazon"
topic #4 (0.033): 0.019*"South Korea" + 0.019*"U.S" + 0.018*"North Korea" + 0.017*"Obama" + 0.012*"WikiLeaks" + 0.012*"White House" + 0.011*"S. Korea" + 0.009*"Cancun" + 0.008*"Sen" + 0.008*"N. Korea"
topic #3 (0.033): 0.024*"London" + 0.019*"police" + 0.018*"UK" + 0.014*"Portugal" + 0.014*"track" + 0.012*"Cuba" + 0.012*"Travel" + 0.010*"protest" + 0.010*"students" + 0.009*"demo 2010"
topic #25 (0.033): 0.076*"today" + 0.048*"out" + 0.028*"Daily" + 0.025*"us" + 0.022*"lol" + 0.019*"turkey" + 0.015*"people" + 0.014*"Twitter" + 0.014*"Top" + 0.013*"tweet"
topic diff=0.131944, rho=0.316228
PROGRESS: pass 0, at document #22000/143749
performing inference on a chunk of 2000 documents
1997/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.013*"military" + 0.012*"DADT" + 0.009*"repeal" + 0.008*"weather" + 0.008*"Portland" + 0.008*"will" + 0.008*"founder" + 0.007*"target" + 0.007*"state" + 0.007*"Julian Assange"
topic #8 (0.033): 0.032*"US" + 0.032*"Iran" + 0.025*"Wikileaks" + 0.014*"to win" + 0.011*"Internet" + 0.011*"WikiLeaks" + 0.010*"Afghanistan" + 0.010*"Brasil" + 0.009*"American" + 0.008*"China"
topic #21 (0.033): 0.028*"Chicago" + 0.024*"cancer" + 0.019*"blog" + 0.017*"up" + 0.016*"NATO" + 0.015*"Obama" + 0.013*"China" + 0.011*"Russia" + 0.011*"Retweet" + 0.011*"Japan"
topic #7 (0.033): 0.110*"Twitter" + 0.065*"Facebook" + 0.017*"kids" + 0.015*"Tweet" + 0.011*"NYC" + 0.009*"website" + 0.009*"People" + 0.009*"Netflix" + 0.009*"Thx" + 0.009*"Temperature"
topic #3 (0.033): 0.022*"London" + 0.019*"UK" + 0.015*"Portugal" + 0.015*"police" + 0.011*"students" + 0.010*"Cuba" + 0.009*"protest" + 0.009*"track" + 0.008*"cover" + 0.008*"Death"
topic diff=0.115491, rho=0.301511
PROGRESS: pass 0, at document #24000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.067*"tcot" + 0.037*"AM" + 0.018*"Obama" + 0.010*"teaparty" + 0.010*"ocra" + 0.010*"Bush" + 0.009*"Alaska" + 0.008*"Palin" + 0.008*"Sarah Palin" + 0.008*"US"
topic #27 (0.033): 0.027*"iPad" + 0.026*"iOS" + 0.021*"post" + 0.021*"iPhone" + 0.017*"Oprah" + 0.015*"Amazon" + 0.012*"DVD" + 0.012*"blog" + 0.011*"call" + 0.011*"Black Friday"
topic #0 (0.033): 0.062*"Haiti" + 0.017*"cholera" + 0.017*"UN" + 0.015*"security" + 0.014*"people" + 0.012*"election" + 0.012*"Asian" + 0.011*"Senator" + 0.011*"earthquake" + 0.010*"writing"
topic #26 (0.033): 0.034*"India" + 0.031*"news" + 0.017*"New Zealand" + 0.017*"mine" + 0.014*"NASA" + 0.012*"Korea" + 0.011*"miners" + 0.010*"energy" + 0.009*"China" + 0.009*"home"
topic #24 (0.033): 0.027*"Leslie Nielsen" + 0.027*"who" + 0.024*"Yes" + 0.016*"Arizona" + 0.015*"Oh" + 0.015*"right" + 0.013*"Google" + 0.012*"me a" + 0.012*"media" + 0.012*"Nice"
topic diff=0.094847, rho=0.288675
PROGRESS: pass 0, at document #26000/143749
performing inference on a chunk of 2000 documents
1995/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.038*"Iran" + 0.035*"US" + 0.022*"Wikileaks" + 0.016*"to win" + 0.013*"American" + 0.013*"Afghanistan" + 0.012*"WikiLeaks" + 0.011*"Internet" + 0.009*"China" + 0.009*"Israel"
topic #16 (0.033): 0.032*"Black Friday" + 0.022*"Sarah Palin" + 0.014*"Here" + 0.013*"Turkey" + 0.013*"Corp" + 0.012*"newspaper" + 0.012*"DWTS" + 0.011*"Canada" + 0.010*"scanners" + 0.010*"to show"
topic #13 (0.033): 0.015*"friend" + 0.010*"perfect" + 0.010*"military" + 0.009*"weather" + 0.009*"DADT" + 0.009*"will" + 0.008*"AP" + 0.008*"Portland" + 0.007*"Julian Assange" + 0.007*"report"
topic #9 (0.033): 0.016*"we" + 0.015*"will" + 0.015*"TSA" + 0.013*"Americans" + 0.013*"airport" + 0.012*"time" + 0.011*"play" + 0.011*"GOP" + 0.011*"MSNBC" + 0.010*"Republican"
topic #5 (0.033): 0.023*"dems" + 0.015*"student" + 0.014*"season" + 0.013*"NBC" + 0.011*"race" + 0.011*"hostage" + 0.011*"high school" + 0.010*"Dow" + 0.010*"yesterday" + 0.010*"Good"
topic diff=0.086248, rho=0.277350
PROGRESS: pass 0, at document #28000/143749
performing inference on a chunk of 2000 documents
1997/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.028*"USA" + 0.026*"PM" + 0.021*"travel" + 0.016*"snow" + 0.013*"here" + 0.012*"CNN" + 0.009*"Scotland" + 0.009*"house" + 0.008*"flight" + 0.008*"car"
topic #23 (0.033): 0.035*"football" + 0.031*"AP" + 0.011*"Pope" + 0.011*"turkeys" + 0.010*"beautiful" + 0.009*"game" + 0.009*"Video" + 0.008*"Day" + 0.008*"NBA" + 0.008*"phone"
topic #10 (0.033): 0.063*"para" + 0.048*"min" + 0.046*"un" + 0.034*"como" + 0.022*"en" + 0.016*"dos" + 0.016*"la" + 0.016*"ll" + 0.014*"hoy" + 0.013*"es"
topic #17 (0.033): 0.033*"Ireland" + 0.026*"North Korea" + 0.022*"Irish" + 0.016*"Reuters" + 0.016*"jobs" + 0.014*"BBC News" + 0.013*"South Korea" + 0.013*"attack" + 0.012*"South Korean" + 0.012*"debt"
topic #6 (0.033): 0.015*"China" + 0.014*"Study" + 0.013*"U.S" + 0.008*"Pres" + 0.008*"Obama" + 0.008*"rich" + 0.008*"World" + 0.007*"via" + 0.007*"California" + 0.007*"BP"
topic diff=0.064679, rho=0.267261
PROGRESS: pass 0, at document #30000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.043*"iPhone" + 0.041*"Apple" + 0.038*"iPad" + 0.038*"via" + 0.023*"Google" + 0.020*"Christmas" + 0.019*"iTunes" + 0.019*"app" + 0.016*"Facebook" + 0.015*"Android"
topic #22 (0.033): 0.052*"Blog" + 0.020*"We" + 0.015*"re" + 0.013*"learn" + 0.013*"week" + 0.012*"one" + 0.010*"who" + 0.010*"will" + 0.009*"change" + 0.009*"health"
topic #12 (0.033): 0.050*"God" + 0.032*"Who" + 0.023*"America" + 0.017*"NPR" + 0.016*"local" + 0.013*"green" + 0.013*"left" + 0.013*"DREAM Act" + 0.012*"shop" + 0.012*"Who's"
topic #18 (0.033): 0.051*"Wikileaks" + 0.050*"WikiLeaks" + 0.035*"wikileaks" + 0.025*"cablegate" + 0.021*"China" + 0.015*"US" + 0.015*"Wiki Leaks" + 0.012*"documents" + 0.009*"cables" + 0.009*"Asia"
topic #21 (0.033): 0.045*"CNN" + 0.028*"blog" + 0.027*"pour" + 0.021*"NATO" + 0.020*"Chicago" + 0.019*"cancer" + 0.016*"dans" + 0.015*"Jobs" + 0.015*"photo" + 0.014*"Retweet"
topic diff=0.169949, rho=0.258199
PROGRESS: pass 0, at document #32000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.064*"today" + 0.047*"lol" + 0.033*"out" + 0.024*"us" + 0.023*"Daily" + 0.017*"photos" + 0.017*"people" + 0.016*"you" + 0.015*"turkey" + 0.015*"tweet"
topic #27 (0.033): 0.025*"cc" + 0.025*"post" + 0.024*"iPad" + 0.021*"Amazon" + 0.020*"iOS" + 0.019*"iPhone" + 0.015*"DVD" + 0.015*"blog" + 0.014*"Black Friday" + 0.013*"Oprah"
topic #12 (0.033): 0.050*"God" + 0.032*"Who" + 0.021*"America" + 0.019*"NPR" + 0.015*"local" + 0.013*"left" + 0.012*"green" + 0.012*"Who's" + 0.011*"shop" + 0.011*"DREAM Act"
topic #16 (0.033): 0.036*"Black Friday" + 0.023*"Sarah Palin" + 0.017*"Here" + 0.012*"Turkey" + 0.011*"Canada" + 0.011*"scanners" + 0.010*"newspaper" + 0.009*"DWTS" + 0.009*"HIV" + 0.009*"gay"
topic #2 (0.033): 0.023*"NFL" + 0.019*"News" + 0.013*"Texas" + 0.012*"You" + 0.011*"Tom DeLay" + 0.011*"Boston" + 0.010*"Gov" + 0.010*"English" + 0.010*"Report" + 0.009*"songs"
topic diff=0.083873, rho=0.250000
PROGRESS: pass 0, at document #34000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.056*"Wikileaks" + 0.049*"WikiLeaks" + 0.032*"wikileaks" + 0.023*"cablegate" + 0.022*"China" + 0.017*"US" + 0.014*"Wiki Leaks" + 0.012*"cables" + 0.011*"documents" + 0.009*"Pakistan"
topic #13 (0.033): 0.015*"AP" + 0.013*"military" + 0.010*"friend" + 0.010*"Portland" + 0.009*"DADT" + 0.008*"child" + 0.008*"will" + 0.007*"weather" + 0.007*"history" + 0.007*"debate"
topic #1 (0.033): 0.070*"TSA" + 0.031*"GOP" + 0.026*"Obama" + 0.022*"ppl" + 0.018*"Senate" + 0.013*"album" + 0.013*"House" + 0.010*"Republicans" + 0.009*"gop" + 0.009*"Bill"
topic #21 (0.033): 0.045*"CNN" + 0.031*"blog" + 0.024*"Chicago" + 0.021*"pour" + 0.020*"cancer" + 0.017*"NATO" + 0.016*"Retweet" + 0.015*"up" + 0.014*"Jobs" + 0.014*"photo"
topic #8 (0.033): 0.037*"Iran" + 0.037*"US" + 0.024*"to win" + 0.021*"Wikileaks" + 0.012*"Afghanistan" + 0.010*"American" + 0.009*"pode" + 0.009*"Internet" + 0.009*"Israel" + 0.009*"leaders"
topic diff=0.086529, rho=0.242536
PROGRESS: pass 0, at document #36000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.083*"para" + 0.048*"un" + 0.037*"dos" + 0.037*"como" + 0.022*"en" + 0.018*"Rio" + 0.016*"se" + 0.015*"Lula" + 0.015*"min" + 0.014*"la"
topic #1 (0.033): 0.077*"TSA" + 0.032*"GOP" + 0.027*"Obama" + 0.022*"ppl" + 0.018*"Senate" + 0.014*"House" + 0.012*"album" + 0.011*"Republicans" + 0.009*"Bill" + 0.009*"bill"
topic #11 (0.033): 0.047*"via" + 0.041*"iPhone" + 0.036*"Apple" + 0.034*"iPad" + 0.026*"app" + 0.025*"Google" + 0.020*"Christmas" + 0.016*"Facebook" + 0.016*"iTunes" + 0.014*"Social Media"
topic #6 (0.033): 0.015*"Green" + 0.012*"U.S" + 0.012*"China" + 0.012*"Study" + 0.009*"France" + 0.008*"World" + 0.007*"BP" + 0.007*"Obama" + 0.007*"California" + 0.007*"Health"
topic #3 (0.033): 0.030*"UK" + 0.028*"London" + 0.023*"police" + 0.019*"Portugal" + 0.016*"Read" + 0.015*"photography" + 0.011*"Police" + 0.011*"students" + 0.010*"Death" + 0.010*"AP"
topic diff=0.056847, rho=0.235702
PROGRESS: pass 0, at document #38000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.023*"U.S" + 0.021*"Obama" + 0.019*"North Korea" + 0.019*"South Korea" + 0.012*"White House" + 0.010*"China" + 0.010*"S. Korea" + 0.010*"N. Korea" + 0.009*"President" + 0.009*"Korea"
topic #27 (0.033): 0.087*"Blogger" + 0.021*"post" + 0.017*"blog" + 0.016*"Amazon" + 0.016*"iPad" + 0.016*"cc" + 0.015*"DVD" + 0.012*"iOS" + 0.012*"iPhone" + 0.011*"release"
topic #2 (0.033): 0.022*"News" + 0.019*"NFL" + 0.015*"You" + 0.014*"Report" + 0.013*"English" + 0.013*"Texas" + 0.012*"Boston" + 0.010*"Airport Security" + 0.010*"Gov" + 0.010*"EUA"
topic #12 (0.033): 0.063*"God" + 0.035*"Who" + 0.018*"NPR" + 0.018*"America" + 0.017*"Celebrity" + 0.014*"Who's" + 0.013*"left" + 0.013*"local" + 0.011*"recipe" + 0.010*"sounds"
topic #23 (0.033): 0.040*"AP" + 0.029*"football" + 0.017*"game" + 0.015*"forex" + 0.011*"New Orleans" + 0.011*"beautiful" + 0.010*"turkeys" + 0.010*"team" + 0.010*"Disney" + 0.009*"Dallas"
topic diff=0.092238, rho=0.229416
bound: at document #0
-23.165 per-word bound, 9404864.9 perplexity estimate based on a held-out corpus of 2000 documents with 26137 words
PROGRESS: pass 0, at document #40000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.063*"God" + 0.035*"Who" + 0.019*"America" + 0.018*"left" + 0.016*"NPR" + 0.015*"Celebrity" + 0.013*"Who's" + 0.012*"local" + 0.011*"story" + 0.010*"recipe"
topic #18 (0.033): 0.061*"Wikileaks" + 0.060*"WikiLeaks" + 0.035*"wikileaks" + 0.028*"cablegate" + 0.020*"China" + 0.020*"US" + 0.014*"documents" + 0.012*"Wiki Leaks" + 0.010*"Pakistan" + 0.010*"cables"
topic #2 (0.033): 0.025*"NFL" + 0.022*"News" + 0.016*"Report" + 0.014*"Texas" + 0.014*"You" + 0.012*"English" + 0.012*"Nicki Minaj" + 0.011*"Gov" + 0.011*"Boston" + 0.011*"songs"
topic #29 (0.033): 0.177*"Thanksgiving" + 0.051*"Happy Thanksgiving" + 0.029*"holiday" + 0.025*"friends" + 0.025*"thanksgiving" + 0.025*"family" + 0.023*"tonight" + 0.023*"LOL" + 0.014*"Harry Potter" + 0.012*"movie"
topic #17 (0.033): 0.037*"Ireland" + 0.033*"North Korea" + 0.025*"Irish" + 0.017*"attack" + 0.017*"South Korea" + 0.014*"Reuters" + 0.013*"artillery" + 0.012*"US" + 0.012*"South Korean" + 0.012*"Spain"
topic diff=0.061641, rho=0.223607
PROGRESS: pass 0, at document #42000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.029*"snow" + 0.023*"travel" + 0.018*"here" + 0.017*"USA" + 0.016*"PM" + 0.012*"film" + 0.012*"Thailand" + 0.011*"England" + 0.010*"Thai" + 0.010*"Paris"
topic #27 (0.033): 0.065*"Blogger" + 0.023*"post" + 0.019*"blog" + 0.016*"Amazon" + 0.016*"DVD" + 0.015*"Oprah" + 0.015*"iPad" + 0.012*"cc" + 0.011*"Target" + 0.010*"iPhone"
topic #4 (0.033): 0.025*"U.S" + 0.024*"Obama" + 0.017*"White House" + 0.016*"North Korea" + 0.016*"South Korea" + 0.016*"President" + 0.010*"China" + 0.010*"Korea" + 0.009*"S. Korea" + 0.009*"Cancun"
topic #28 (0.033): 0.160*"video" + 0.139*"YouTube" + 0.031*"haha" + 0.028*"Video" + 0.014*"read" + 0.014*"Holiday" + 0.014*"music" + 0.013*"Cyber Monday" + 0.012*"Photography" + 0.011*"News"
topic #17 (0.033): 0.036*"Ireland" + 0.032*"North Korea" + 0.024*"Irish" + 0.018*"attack" + 0.017*"South Korea" + 0.012*"Reuters" + 0.012*"jobs" + 0.012*"artillery" + 0.012*"Europe" + 0.012*"South Korean"
topic diff=0.064100, rho=0.218218
PROGRESS: pass 0, at document #44000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.055*"God" + 0.041*"Who" + 0.020*"left" + 0.016*"America" + 0.016*"NPR" + 0.015*"Who's" + 0.013*"local" + 0.011*"shop" + 0.010*"Celebrity" + 0.010*"recipe"
topic #9 (0.033): 0.022*"we" + 0.019*"will" + 0.015*"Americans" + 0.014*"work" + 0.013*"do" + 0.011*"time" + 0.011*"talk" + 0.011*"don" + 0.010*"TSA" + 0.010*"airport"
topic #7 (0.033): 0.126*"Twitter" + 0.083*"Facebook" + 0.020*"Tweet" + 0.019*"NYC" + 0.017*"People" + 0.015*"kids" + 0.014*"Social Media" + 0.010*"Thx" + 0.010*"Chinese" + 0.010*"social media"
topic #6 (0.033): 0.015*"Study" + 0.012*"China" + 0.010*"U.S" + 0.010*"World" + 0.009*"France" + 0.009*"Green" + 0.008*"News" + 0.008*"Will" + 0.008*"rich" + 0.008*"Health"
topic #26 (0.033): 0.029*"India" + 0.015*"news" + 0.014*"New Zealand" + 0.012*"Africa" + 0.011*"business" + 0.011*"mine" + 0.011*"energy" + 0.010*"EU" + 0.010*"buy" + 0.010*"killed"
topic diff=0.068037, rho=0.213201
PROGRESS: pass 0, at document #46000/143749
performing inference on a chunk of 2000 documents
1997/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.081*"GOP" + 0.063*"TSA" + 0.042*"Obama" + 0.023*"Senate" + 0.020*"ppl" + 0.020*"gop" + 0.016*"Republicans" + 0.013*"House" + 0.012*"bill" + 0.011*"Dems"
topic #28 (0.033): 0.132*"video" + 0.105*"YouTube" + 0.041*"haha" + 0.030*"Video" + 0.018*"music" + 0.017*"Cyber Monday" + 0.015*"Holiday" + 0.014*"read" + 0.010*"Live" + 0.010*"Photography"
topic #23 (0.033): 0.026*"AP" + 0.024*"football" + 0.021*"game" + 0.017*"haha" + 0.016*"NBA" + 0.014*"rock" + 0.014*"Day" + 0.013*"turkeys" + 0.012*"beautiful" + 0.012*"hair"
topic #15 (0.033): 0.033*"snow" + 0.029*"USA" + 0.018*"travel" + 0.015*"PM" + 0.013*"here" + 0.010*"England" + 0.010*"film" + 0.009*"BBC" + 0.009*"rape" + 0.009*"Paris"
topic #22 (0.033): 0.054*"Blog" + 0.019*"one" + 0.017*"We" + 0.017*"re" + 0.012*"basketball" + 0.012*"food" + 0.011*"water" + 0.011*"week" + 0.011*"WTF" + 0.011*"LOL"
topic diff=0.236645, rho=0.208514
PROGRESS: pass 0, at document #48000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.019*"season" + 0.017*"dems" + 0.017*"student" + 0.013*"NBC" + 0.013*"Support" + 0.012*"record" + 0.012*"high school" + 0.011*"fb" + 0.011*"Los Angeles" + 0.011*"NY"
topic #24 (0.033): 0.063*"twit" + 0.048*"who" + 0.023*"Yes" + 0.022*"Arizona" + 0.020*"right" + 0.020*"Leslie Nielsen" + 0.017*"Oh" + 0.016*"me a" + 0.016*"Nice" + 0.015*"a man"
topic #25 (0.033): 0.093*"lol" + 0.052*"today" + 0.030*"out" + 0.023*"you" + 0.021*"people" + 0.019*"tweet" + 0.019*"us" + 0.019*"ur" + 0.018*"twitter" + 0.017*"can"
topic #9 (0.033): 0.018*"we" + 0.018*"will" + 0.016*"Americans" + 0.013*"do" + 0.013*"work" + 0.012*"play" + 0.012*"MSNBC" + 0.012*"airport" + 0.011*"time" + 0.011*"talk"
topic #2 (0.033): 0.023*"LMAO" + 0.021*"Texas" + 0.019*"NFL" + 0.016*"News" + 0.015*"You" + 0.014*"Report" + 0.013*"Knicks" + 0.012*"Tom DeLay" + 0.012*"songs" + 0.011*"Gov"
topic diff=0.054011, rho=0.204124
PROGRESS: pass 0, at document #50000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.093*"lol" + 0.049*"today" + 0.028*"out" + 0.024*"ur" + 0.021*"you" + 0.020*"people" + 0.020*"us" + 0.019*"tweet" + 0.017*"can" + 0.017*"twitter"
topic #29 (0.033): 0.190*"Thanksgiving" + 0.056*"Happy Thanksgiving" + 0.056*"LOL" + 0.026*"tonight" + 0.025*"family" + 0.025*"friends" + 0.024*"holiday" + 0.022*"thanksgiving" + 0.018*"Turkey" + 0.011*"Harry Potter"
topic #13 (0.033): 0.022*"DADT" + 0.013*"Pentagon" + 0.013*"NW" + 0.011*"weather" + 0.011*"AP" + 0.010*"military" + 0.009*"debate" + 0.009*"Portland" + 0.009*"child" + 0.008*"risk"
topic #15 (0.033): 0.028*"snow" + 0.027*"USA" + 0.015*"travel" + 0.015*"here" + 0.013*"PM" + 0.011*"England" + 0.010*"film" + 0.010*"flight" + 0.010*"LA" + 0.009*"Paris"
topic #26 (0.033): 0.028*"India" + 0.021*"New Zealand" + 0.017*"news" + 0.016*"mine" + 0.011*"energy" + 0.010*"Barcelona" + 0.010*"Real Madrid" + 0.010*"killed" + 0.010*"business" + 0.010*"China"
topic diff=0.053722, rho=0.200000
PROGRESS: pass 0, at document #52000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.029*"snow" + 0.025*"USA" + 0.015*"travel" + 0.014*"here" + 0.013*"England" + 0.013*"PM" + 0.011*"film" + 0.010*"Paris" + 0.009*"LA" + 0.009*"John Lennon"
topic #19 (0.033): 0.049*"Today" + 0.025*"Time" + 0.023*"Washington" + 0.022*"Love" + 0.022*"nice" + 0.020*"Israel" + 0.019*"fear" + 0.019*"Music" + 0.018*"white" + 0.017*"Life"
topic #29 (0.033): 0.186*"Thanksgiving" + 0.055*"Happy Thanksgiving" + 0.054*"LOL" + 0.026*"thanksgiving" + 0.025*"tonight" + 0.025*"friends" + 0.025*"family" + 0.024*"holiday" + 0.019*"Turkey" + 0.011*"movie"
topic #28 (0.033): 0.264*"YouTube" + 0.197*"video" + 0.037*"haha" + 0.024*"Video" + 0.018*"music" + 0.012*"Cyber Monday" + 0.011*"Holiday" + 0.008*"read" + 0.008*"Live" + 0.006*"Photography"
topic #21 (0.033): 0.037*"Retweet" + 0.037*"Chicago" + 0.035*"CNN" + 0.027*"blog" + 0.021*"NATO" + 0.020*"cancer" + 0.019*"Friday" + 0.016*"Russia" + 0.016*"Illinois" + 0.015*"Omg"
topic diff=0.082074, rho=0.196116
PROGRESS: pass 0, at document #54000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.086*"God" + 0.041*"Who" + 0.027*"GOD" + 0.016*"Who's" + 0.015*"NPR" + 0.014*"left" + 0.014*"America" + 0.011*"Song" + 0.011*"guy" + 0.010*"Drake"
topic #24 (0.033): 0.052*"who" + 0.034*"twit" + 0.022*"right" + 0.022*"Oh" + 0.021*"me a" + 0.020*"Yes" + 0.019*"Leslie Nielsen" + 0.019*"Arizona" + 0.018*"a man" + 0.017*"Nice"
topic #26 (0.033): 0.025*"India" + 0.018*"news" + 0.018*"New Zealand" + 0.015*"mine" + 0.014*"Barcelona" + 0.013*"Real Madrid" + 0.013*"business" + 0.012*"dead" + 0.011*"energy" + 0.010*"Africa"
topic #28 (0.033): 0.237*"YouTube" + 0.189*"video" + 0.034*"haha" + 0.032*"Video" + 0.021*"music" + 0.013*"Cyber Monday" + 0.011*"Holiday" + 0.009*"read" + 0.009*"Live" + 0.006*"Black Eyed Peas"
topic #19 (0.033): 0.049*"Today" + 0.026*"Music" + 0.025*"Love" + 0.025*"Time" + 0.021*"nice" + 0.020*"Washington" + 0.019*"Israel" + 0.018*"right now" + 0.018*"Wow" + 0.017*"fear"
topic diff=0.036268, rho=0.192450
PROGRESS: pass 0, at document #56000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.106*"para" + 0.047*"dos" + 0.047*"un" + 0.037*"como" + 0.025*"en" + 0.021*"la" + 0.017*"ll" + 0.017*"se" + 0.016*"pour" + 0.014*"Lula"
topic #7 (0.033): 0.109*"Twitter" + 0.074*"Facebook" + 0.025*"Tweet" + 0.024*"NYC" + 0.019*"Lady Gaga" + 0.015*"People" + 0.014*"Social Media" + 0.014*"kids" + 0.010*"Thx" + 0.010*"Lmao"
topic #19 (0.033): 0.048*"Today" + 0.028*"Love" + 0.025*"Music" + 0.023*"Time" + 0.022*"nice" + 0.020*"Washington" + 0.020*"Wow" + 0.019*"right now" + 0.019*"white" + 0.018*"Israel"
topic #17 (0.033): 0.037*"Ireland" + 0.034*"North Korea" + 0.022*"South Korea" + 0.020*"Irish" + 0.018*"attack" + 0.015*"artillery" + 0.014*"South Korean" + 0.013*"jobs" + 0.013*"Reuters" + 0.012*"bailout"
topic #5 (0.033): 0.021*"season" + 0.018*"NBC" + 0.016*"student" + 0.013*"Fox" + 0.013*"record" + 0.013*"smile" + 0.012*"high school" + 0.012*"Good" + 0.012*"Football" + 0.012*"Los Angeles"
topic diff=0.059071, rho=0.188982
PROGRESS: pass 0, at document #58000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #26 (0.033): 0.021*"India" + 0.020*"news" + 0.018*"New Zealand" + 0.017*"mine" + 0.013*"dead" + 0.012*"business" + 0.012*"energy" + 0.012*"Barcelona" + 0.011*"Real Madrid" + 0.010*"Tonight"
topic #28 (0.033): 0.194*"YouTube" + 0.170*"video" + 0.033*"Video" + 0.030*"haha" + 0.021*"music" + 0.017*"Holiday" + 0.015*"Cyber Monday" + 0.010*"Live" + 0.008*"read" + 0.007*"Black Eyed Peas"
topic #0 (0.033): 0.148*"Haiti" + 0.032*"cholera" + 0.023*"Cholera" + 0.018*"election" + 0.015*"Atlantic" + 0.015*"UN" + 0.014*"tweeted" + 0.014*"Elections" + 0.013*"security" + 0.012*"Mexico"
topic #29 (0.033): 0.173*"Thanksgiving" + 0.056*"Happy Thanksgiving" + 0.055*"LOL" + 0.029*"tonight" + 0.028*"thanksgiving" + 0.026*"family" + 0.026*"holiday" + 0.026*"friends" + 0.019*"Turkey" + 0.014*"movie"
topic #3 (0.033): 0.042*"UK" + 0.034*"London" + 0.029*"police" + 0.018*"BBC" + 0.017*"students" + 0.014*"demo 2010" + 0.014*"Portugal" + 0.013*"Police" + 0.011*"protest" + 0.011*"British"
topic diff=0.040003, rho=0.185695
bound: at document #0
-24.501 per-word bound, 23743571.3 perplexity estimate based on a held-out corpus of 2000 documents with 40495 words
PROGRESS: pass 0, at document #60000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.042*"UK" + 0.036*"London" + 0.028*"police" + 0.016*"students" + 0.015*"demo 2010" + 0.015*"BBC" + 0.013*"Police" + 0.013*"Portugal" + 0.013*" a" + 0.010*"protest"
topic #2 (0.033): 0.024*"NFL" + 0.022*"LMAO" + 0.019*"News" + 0.016*"Texas" + 0.015*"Report" + 0.014*"Tom DeLay" + 0.014*"You" + 0.013*"songs" + 0.013*"Boston" + 0.012*"EUA"
topic #16 (0.033): 0.038*"Black Friday" + 0.030*"Sarah Palin" + 0.020*"Canada" + 0.019*"HIV" + 0.018*"DWTS" + 0.018*"gay" + 0.015*"Here" + 0.012*"Dec" + 0.012*"DJ" + 0.012*"If"
topic #21 (0.033): 0.067*"FARC" + 0.045*"Chicago" + 0.032*"blog" + 0.029*"CNN" + 0.027*"Retweet" + 0.024*"NATO" + 0.023*"cancer" + 0.019*"Toronto" + 0.017*"Friday" + 0.013*"Russia"
topic #24 (0.033): 0.058*"who" + 0.025*"right" + 0.023*"a man" + 0.023*"Leslie Nielsen" + 0.023*"me a" + 0.022*"Yes" + 0.021*"Nice" + 0.020*"twit" + 0.019*"radio" + 0.018*"man"
topic diff=0.047077, rho=0.182574
PROGRESS: pass 0, at document #62000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.048*"via" + 0.038*"iPad" + 0.036*"iPhone" + 0.031*"Apple" + 0.026*"Google" + 0.026*"Beatles" + 0.024*"ser" + 0.023*"Christmas" + 0.019*"iTunes" + 0.018*"show"
topic #20 (0.033): 0.034*"live" + 0.034*"TV" + 0.026*"Watch" + 0.021*"climate" + 0.021*"watch" + 0.019*"Climate Change" + 0.019*"interview" + 0.019*"world" + 0.019*"Hope" + 0.018*"climate change"
topic #16 (0.033): 0.037*"Black Friday" + 0.029*"Sarah Palin" + 0.019*"Canada" + 0.018*"gay" + 0.018*"HIV" + 0.017*"DWTS" + 0.014*"Here" + 0.011*"Dec" + 0.011*"DJ" + 0.011*"If"
topic #17 (0.033): 0.036*"North Korea" + 0.029*"Ireland" + 0.021*"South Korea" + 0.018*"China" + 0.017*"Irish" + 0.016*"attack" + 0.015*"Japan" + 0.014*"jobs" + 0.013*"artillery" + 0.012*"Reuters"
topic #9 (0.033): 0.021*"will" + 0.018*"we" + 0.016*"do" + 0.016*"work" + 0.014*"em" + 0.013*"airport" + 0.013*"Americans" + 0.011*"play" + 0.011*"something" + 0.011*"time"
topic diff=0.058176, rho=0.179605
PROGRESS: pass 0, at document #64000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.173*"Thanksgiving" + 0.056*"Happy Thanksgiving" + 0.049*"LOL" + 0.029*"thanksgiving" + 0.028*"family" + 0.028*"tonight" + 0.025*"holiday" + 0.025*"friends" + 0.020*"Turkey" + 0.013*"movie"
topic #4 (0.033): 0.027*"U.S" + 0.021*"Obama" + 0.015*"North Korea" + 0.014*"Cancun" + 0.014*"South Korea" + 0.014*"White House" + 0.013*"Korea" + 0.011*"President" + 0.011*"China" + 0.010*"TIME"
topic #3 (0.033): 0.088*"photography" + 0.038*"UK" + 0.038*"London" + 0.024*"police" + 0.015*"students" + 0.012*"Police" + 0.012*"demo 2010" + 0.011*"BBC" + 0.011*"Portugal" + 0.010*"Transgender"
topic #11 (0.033): 0.048*"via" + 0.038*"iPad" + 0.035*"Apple" + 0.034*"iPhone" + 0.026*"Google" + 0.025*"Christmas" + 0.024*"Beatles" + 0.020*"ser" + 0.019*"show" + 0.019*"app"
topic #5 (0.033): 0.020*"season" + 0.017*"student" + 0.014*"Good" + 0.014*"NBC" + 0.013*"Fox" + 0.013*"high school" + 0.012*"Iowa" + 0.012*"smile" + 0.011*"Wisconsin" + 0.011*"quote"
topic diff=0.048854, rho=0.176777
PROGRESS: pass 0, at document #66000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.057*"tcot" + 0.022*"Obama" + 0.015*"Bush" + 0.014*"woman" + 0.011*"teaparty" + 0.009*"sgp" + 0.009*"GM" + 0.009*"Alaska" + 0.009*"single" + 0.008*"Palin"
topic #17 (0.033): 0.035*"North Korea" + 0.024*"Ireland" + 0.022*"South Korea" + 0.022*"attack" + 0.018*"China" + 0.016*"artillery" + 0.016*"Japan" + 0.013*"Irish" + 0.013*"jobs" + 0.013*"Reuters"
topic #25 (0.033): 0.065*"lol" + 0.044*"today" + 0.027*"ur" + 0.026*"people" + 0.026*"you" + 0.026*"can" + 0.022*"out" + 0.019*"love" + 0.019*"us" + 0.019*"tweet"
topic #24 (0.033): 0.064*"who" + 0.028*"Leslie Nielsen" + 0.026*"right" + 0.022*"a man" + 0.022*"me a" + 0.021*"Yes" + 0.018*"Nice" + 0.017*"man" + 0.017*"Oh" + 0.015*"radio"
topic #20 (0.033): 0.036*"live" + 0.033*"TV" + 0.027*"Radio" + 0.024*"Watch" + 0.022*"world" + 0.020*"climate change" + 0.019*"Hope" + 0.018*"watch" + 0.018*"interview" + 0.018*"Florida"
topic diff=0.064299, rho=0.174078
PROGRESS: pass 0, at document #68000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.075*"WikiLeaks" + 0.059*"Wikileaks" + 0.028*"cablegate" + 0.028*"wikileaks" + 0.027*"China" + 0.020*"US" + 0.019*"Wiki Leaks" + 0.017*"NYT" + 0.014*"documents" + 0.013*"Tibet"
topic #20 (0.033): 0.037*"live" + 0.032*"TV" + 0.027*"Radio" + 0.024*"Watch" + 0.023*"world" + 0.021*"Hope" + 0.020*"interview" + 0.020*"Florida" + 0.019*"climate change" + 0.019*"Check it"
topic #28 (0.033): 0.155*"video" + 0.098*"YouTube" + 0.050*"Video" + 0.039*"haha" + 0.026*"music" + 0.018*"Holiday" + 0.017*"Cyber Monday" + 0.013*"Live" + 0.009*"Photography" + 0.009*"nothing"
topic #4 (0.033): 0.029*"U.S" + 0.024*"Obama" + 0.021*"S. Korea" + 0.017*"Korea" + 0.017*"President" + 0.016*"N. Korea" + 0.016*"South Korea" + 0.015*"North Korea" + 0.013*"China" + 0.012*"White House"
topic #13 (0.033): 0.017*"DADT" + 0.013*"child" + 0.013*"AP" + 0.013*"friend" + 0.012*"risk" + 0.012*"military" + 0.011*"weather" + 0.009*"Pentagon" + 0.009*"troops" + 0.009*"network"
topic diff=0.030398, rho=0.171499
PROGRESS: pass 0, at document #70000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.091*"para" + 0.050*"un" + 0.035*"como" + 0.028*"dos" + 0.025*"en" + 0.023*"la" + 0.019*"youtube" + 0.017*"pour" + 0.015*"se" + 0.015*"ll"
topic #11 (0.033): 0.061*"via" + 0.032*"iPad" + 0.029*"iPhone" + 0.029*"Apple" + 0.028*"Google" + 0.027*"iTunes" + 0.022*"Christmas" + 0.017*"app" + 0.017*"show" + 0.016*"Beatles"
topic #15 (0.033): 0.022*"snow" + 0.020*"USA" + 0.019*"here" + 0.018*"travel" + 0.014*"England" + 0.012*"film" + 0.011*"PM" + 0.010*"LA" + 0.010*"Paris" + 0.010*"sex"
topic #2 (0.033): 0.033*"son" + 0.031*"News" + 0.020*"NFL" + 0.016*"Texas" + 0.016*"Las Vegas" + 0.015*"LMAO" + 0.014*"Boston" + 0.013*"Report" + 0.013*"Houston" + 0.012*"knowledge"
topic #12 (0.033): 0.129*"God" + 0.041*"Who" + 0.018*"GOD" + 0.017*"left" + 0.015*"America" + 0.014*"guy" + 0.013*"Who's" + 0.011*"NPR" + 0.011*"Song" + 0.009*"next"
topic diff=0.039683, rho=0.169031
PROGRESS: pass 0, at document #72000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.037*"Black Friday" + 0.032*"Sarah Palin" + 0.021*"HIV" + 0.020*"DWTS" + 0.020*"DJ" + 0.019*"gay" + 0.017*"Canada" + 0.015*"Here" + 0.012*"If" + 0.011*"scanners"
topic #7 (0.033): 0.114*"Twitter" + 0.084*"Facebook" + 0.026*"Tweet" + 0.020*"NYC" + 0.020*"website" + 0.016*"People" + 0.012*"Chinese" + 0.011*"Social Media" + 0.010*"Lmao" + 0.010*"Lady Gaga"
topic #9 (0.033): 0.024*"will" + 0.018*"we" + 0.017*"do" + 0.015*"airport" + 0.015*"work" + 0.014*"em" + 0.014*"MSNBC" + 0.013*"Americans" + 0.012*"something" + 0.012*"play"
topic #14 (0.033): 0.065*"tcot" + 0.019*"Obama" + 0.015*"woman" + 0.012*"Bush" + 0.011*"teaparty" + 0.010*"Alaska" + 0.009*"single" + 0.008*"sgp" + 0.008*"Palin" + 0.008*"GM"
topic #19 (0.033): 0.046*"Today" + 0.039*"Love" + 0.023*"Time" + 0.023*"nice" + 0.021*"Life" + 0.021*"white" + 0.021*"Music" + 0.020*"Leslie Nielsen" + 0.020*"Wow" + 0.020*"right now"
topic diff=0.026998, rho=0.166667
PROGRESS: pass 0, at document #74000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.018*"DADT" + 0.014*"child" + 0.013*"AP" + 0.012*"military" + 0.012*"weather" + 0.012*"risk" + 0.011*"friend" + 0.010*"Pentagon" + 0.010*"troops" + 0.009*"network"
topic #12 (0.033): 0.115*"God" + 0.041*"Who" + 0.019*"left" + 0.015*"guy" + 0.015*"GOD" + 0.012*"Who's" + 0.012*"fashion" + 0.011*"America" + 0.011*"NPR" + 0.010*"next"
topic #23 (0.033): 0.031*"football" + 0.028*"game" + 0.022*"AP" + 0.016*"haha" + 0.015*"beautiful" + 0.014*"hair" + 0.014*"thoughts" + 0.012*"NBA" + 0.012*"team" + 0.011*"show"
topic #10 (0.033): 0.090*"para" + 0.051*"un" + 0.036*"como" + 0.031*"dos" + 0.025*"en" + 0.023*"la" + 0.020*"es" + 0.016*"ll" + 0.016*"se" + 0.015*"México"
topic #11 (0.033): 0.056*"via" + 0.033*"iPhone" + 0.033*"iPad" + 0.031*"Apple" + 0.026*"Google" + 0.024*"iTunes" + 0.024*"Christmas" + 0.019*"show" + 0.017*"Beatles" + 0.016*"app"
topic diff=0.051400, rho=0.164399
PROGRESS: pass 0, at document #76000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.071*"who" + 0.026*"Leslie Nielsen" + 0.024*"right" + 0.023*"Nice" + 0.022*"me a" + 0.021*"a man" + 0.021*"Yes" + 0.019*"man" + 0.018*"always" + 0.017*"Oh"
topic #6 (0.033): 0.016*"U.S" + 0.015*"China" + 0.011*"Cancer" + 0.010*"Pres" + 0.010*"BP" + 0.009*"NYT" + 0.009*"World" + 0.009*"News" + 0.009*"Study" + 0.008*"Will"
topic #16 (0.033): 0.037*"Black Friday" + 0.031*"Sarah Palin" + 0.023*"HIV" + 0.020*"gay" + 0.017*"DWTS" + 0.017*"DJ" + 0.014*"Here" + 0.014*"Canada" + 0.012*"newspaper" + 0.011*"AIDS"
topic #12 (0.033): 0.117*"God" + 0.040*"Who" + 0.018*"left" + 0.016*"guy" + 0.015*"GOD" + 0.013*"al" + 0.012*"Who's" + 0.011*"fashion" + 0.011*"poor" + 0.011*"America"
topic #2 (0.033): 0.042*"News" + 0.023*"son" + 0.022*"NFL" + 0.016*"Texas" + 0.015*"LMAO" + 0.014*"Las Vegas" + 0.013*"Boston" + 0.013*"Houston" + 0.013*"Report" + 0.012*"You"
topic diff=0.043567, rho=0.162221
PROGRESS: pass 0, at document #78000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.042*"Blog" + 0.022*"We" + 0.021*"one" + 0.019*"re" + 0.017*"see" + 0.017*"artist" + 0.013*"health" + 0.013*"WTF" + 0.013*"Post" + 0.012*"food"
topic #9 (0.033): 0.025*"will" + 0.020*"em" + 0.019*"do" + 0.018*"work" + 0.016*"we" + 0.013*"something" + 0.013*"mind" + 0.013*"play" + 0.013*"airport" + 0.012*"VIDEO"
topic #20 (0.033): 0.037*"live" + 0.033*"TV" + 0.028*"Watch" + 0.026*"Hope" + 0.025*"world" + 0.022*"interview" + 0.019*"Radio" + 0.017*"climate change" + 0.017*"watch" + 0.017*"homeless"
topic #10 (0.033): 0.098*"para" + 0.059*"como" + 0.052*"un" + 0.037*"dos" + 0.025*"la" + 0.024*"en" + 0.023*"es" + 0.019*"este" + 0.019*"se" + 0.019*"si"
topic #23 (0.033): 0.032*"game" + 0.029*"football" + 0.019*"AP" + 0.017*"beautiful" + 0.016*"haha" + 0.015*"net" + 0.015*"team" + 0.014*"thoughts" + 0.013*"hair" + 0.013*"show"
topic diff=0.041434, rho=0.160128
bound: at document #0
-22.647 per-word bound, 6569981.5 perplexity estimate based on a held-out corpus of 2000 documents with 49203 words
PROGRESS: pass 0, at document #80000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.080*"Wikileaks" + 0.071*"WikiLeaks" + 0.034*"wikileaks" + 0.034*"cablegate" + 0.024*"China" + 0.022*"US" + 0.018*"Wiki Leaks" + 0.013*"cables" + 0.013*"documents" + 0.011*"Pakistan"
topic #21 (0.033): 0.046*"blog" + 0.032*"NATO" + 0.026*"Chicago" + 0.026*"cancer" + 0.025*"CNN" + 0.020*"Retweet" + 0.019*"Russia" + 0.017*"FARC" + 0.016*"up" + 0.016*"condoms"
topic #1 (0.033): 0.083*"TSA" + 0.046*"Obama" + 0.043*"GOP" + 0.027*"ppl" + 0.022*"politics" + 0.019*"Senate" + 0.013*"House" + 0.013*"album" + 0.013*"vote" + 0.012*"Congress"
topic #12 (0.033): 0.103*"God" + 0.041*"Who" + 0.027*"al" + 0.018*"left" + 0.018*"green" + 0.016*"guy" + 0.014*"Chile" + 0.014*"GOD" + 0.011*"NPR" + 0.011*"poor"
topic #13 (0.033): 0.015*"DADT" + 0.013*"risk" + 0.013*"weather" + 0.012*"AP" + 0.012*"child" + 0.012*"friend" + 0.010*"military" + 0.010*"Pentagon" + 0.009*"law" + 0.009*"state"
topic diff=0.091623, rho=0.158114
PROGRESS: pass 0, at document #82000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.045*"Today" + 0.033*"Israel" + 0.031*"Jews" + 0.027*"Love" + 0.026*"nice" + 0.025*"Time" + 0.022*"Life" + 0.019*"white" + 0.019*"Wow" + 0.018*"fear"
topic #8 (0.033): 0.107*"Iran" + 0.041*"US" + 0.040*"Israel" + 0.033*"Afghanistan" + 0.022*"Iraq" + 0.017*"to win" + 0.015*"American" + 0.014*"Brasil" + 0.013*"Arab" + 0.012*"Egypt"
topic #18 (0.033): 0.078*"WikiLeaks" + 0.077*"Wikileaks" + 0.033*"wikileaks" + 0.032*"cablegate" + 0.023*"China" + 0.022*"US" + 0.018*"Guardian" + 0.017*"Wiki Leaks" + 0.014*"cables" + 0.014*"documents"
topic #27 (0.033): 0.036*"recipes" + 0.036*"post" + 0.035*"Education" + 0.031*"blog" + 0.022*"DVD" + 0.016*"Amazon" + 0.014*"iOS" + 0.014*"web" + 0.012*"release" + 0.011*"Oprah"
topic #24 (0.033): 0.072*"who" + 0.028*"right" + 0.025*"Leslie Nielsen" + 0.024*"Nice" + 0.023*"Yes" + 0.021*"me a" + 0.021*"man" + 0.020*"a man" + 0.015*"Oh" + 0.015*"actor"
topic diff=0.171503, rho=0.156174
PROGRESS: pass 0, at document #84000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.083*"WikiLeaks" + 0.080*"Wikileaks" + 0.032*"wikileaks" + 0.030*"cablegate" + 0.023*"US" + 0.022*"China" + 0.017*"Wiki Leaks" + 0.017*"Guardian" + 0.015*"documents" + 0.014*"cables"
topic #6 (0.033): 0.016*"U.S" + 0.015*"France" + 0.014*"China" + 0.010*"World" + 0.010*"Bloomberg" + 0.009*"Study" + 0.009*"NYT" + 0.008*"News" + 0.008*"BP" + 0.008*"Women"
topic #16 (0.033): 0.042*"Canada" + 0.035*"Black Friday" + 0.029*"newspaper" + 0.027*"Sarah Palin" + 0.021*"HIV" + 0.016*"DWTS" + 0.015*"Here" + 0.014*"gay" + 0.011*"MS" + 0.011*"If"
topic #13 (0.033): 0.019*"AP" + 0.017*"DADT" + 0.013*"state" + 0.012*"risk" + 0.012*"military" + 0.012*"Pentagon" + 0.010*"friend" + 0.010*"weather" + 0.010*"child" + 0.010*"law"
topic #7 (0.033): 0.132*"Twitter" + 0.086*"Facebook" + 0.027*"Tweet" + 0.020*"NYC" + 0.019*"COICA" + 0.017*"website" + 0.016*"New York Times" + 0.015*"People" + 0.012*"Chinese" + 0.010*"finance"
topic diff=0.022620, rho=0.154303
PROGRESS: pass 0, at document #86000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.129*"video" + 0.073*"YouTube" + 0.058*"Video" + 0.037*"haha" + 0.026*"music" + 0.024*"Cyber Monday" + 0.021*"Holiday" + 0.017*"read" + 0.015*"Photography" + 0.014*"ad"
topic #8 (0.033): 0.099*"Iran" + 0.041*"US" + 0.040*"Israel" + 0.029*"Afghanistan" + 0.020*"Iraq" + 0.018*"to win" + 0.017*"American" + 0.015*"Brasil" + 0.013*"Arab" + 0.013*"Election"
topic #7 (0.033): 0.130*"Twitter" + 0.083*"Facebook" + 0.027*"Tweet" + 0.020*"NYC" + 0.016*"COICA" + 0.016*"website" + 0.015*"New York Times" + 0.014*"People" + 0.014*"Thx" + 0.011*"Chinese"
topic #29 (0.033): 0.189*"Thanksgiving" + 0.079*"Happy Thanksgiving" + 0.035*"LOL" + 0.032*"friends" + 0.031*"Turkey" + 0.030*"thanksgiving" + 0.029*"family" + 0.024*"holiday" + 0.023*"tonight" + 0.011*"eat"
topic #6 (0.033): 0.016*"U.S" + 0.013*"China" + 0.013*"France" + 0.010*"World" + 0.009*"NYT" + 0.009*"Study" + 0.009*"Bloomberg" + 0.008*"BP" + 0.008*"rich" + 0.008*"Women"
topic diff=0.050404, rho=0.152499
PROGRESS: pass 0, at document #88000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.021*"season" + 0.021*"NBC" + 0.019*"high school" + 0.018*"student" + 0.015*"smile" + 0.015*"Wisconsin" + 0.014*"record" + 0.014*"EEUU" + 0.013*"Gold" + 0.012*"Football"
topic #23 (0.033): 0.035*"game" + 0.029*"AP" + 0.029*"football" + 0.017*"haha" + 0.017*"beautiful" + 0.015*"team" + 0.014*"hair" + 0.013*"net" + 0.012*"thoughts" + 0.011*"Kanye West"
topic #1 (0.033): 0.077*"TSA" + 0.047*"Obama" + 0.046*"GOP" + 0.033*"ppl" + 0.019*"politics" + 0.018*"Senate" + 0.013*"House" + 0.012*"Congress" + 0.012*"Republicans" + 0.012*"vote"
topic #17 (0.033): 0.036*"North Korea" + 0.027*"China" + 0.024*"Ireland" + 0.021*"Reuters" + 0.021*"South Korea" + 0.017*"attack" + 0.017*"Europe" + 0.017*"BBC News" + 0.016*"Irish" + 0.014*"US"
topic #4 (0.033): 0.029*"U.S" + 0.021*"Obama" + 0.019*"cooking" + 0.018*"Korea" + 0.014*"President" + 0.014*"North Korea" + 0.014*"Cancun" + 0.013*"White House" + 0.013*"S. Korea" + 0.011*"military"
topic diff=0.040602, rho=0.150756
PROGRESS: pass 0, at document #90000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.086*"Iran" + 0.040*"US" + 0.037*"Israel" + 0.028*"Afghanistan" + 0.021*"Iraq" + 0.018*"Brasil" + 0.018*"to win" + 0.015*"American" + 0.014*"Arab" + 0.012*"Saudi"
topic #18 (0.033): 0.086*"WikiLeaks" + 0.076*"Wikileaks" + 0.029*"cablegate" + 0.029*"wikileaks" + 0.024*"US" + 0.020*"China" + 0.017*"Wiki Leaks" + 0.015*"documents" + 0.014*"Guardian" + 0.013*"cables"
topic #3 (0.033): 0.041*"UK" + 0.031*"London" + 0.031*"photography" + 0.022*"students" + 0.020*"police" + 0.016*"Colombia" + 0.015*"school" + 0.014*"Read" + 0.013*"protest" + 0.012*"Police"
topic #17 (0.033): 0.036*"North Korea" + 0.026*"China" + 0.025*"Ireland" + 0.021*"Reuters" + 0.019*"South Korea" + 0.017*"attack" + 0.017*"Europe" + 0.016*"BBC News" + 0.014*"Irish" + 0.014*"Indonesia"
topic #26 (0.033): 0.038*"India" + 0.031*"news" + 0.022*"business" + 0.020*"New Zealand" + 0.015*"dead" + 0.015*"mine" + 0.012*"energy" + 0.011*"Africa" + 0.009*"killed" + 0.009*"Australia"
topic diff=0.037982, rho=0.149071
PROGRESS: pass 0, at document #92000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #7 (0.033): 0.127*"Twitter" + 0.088*"Facebook" + 0.027*"Tweet" + 0.022*"NYC" + 0.014*"People" + 0.013*"website" + 0.012*"New York Times" + 0.012*"Internet" + 0.012*"Social Media" + 0.011*"Lmao"
topic #26 (0.033): 0.046*"news" + 0.036*"India" + 0.022*"New Zealand" + 0.019*"business" + 0.016*"mine" + 0.014*"dead" + 0.010*"energy" + 0.010*"Africa" + 0.010*"killed" + 0.009*"Australia"
topic #12 (0.033): 0.094*"God" + 0.042*"Who" + 0.021*"al" + 0.018*"left" + 0.013*"guy" + 0.012*"green" + 0.012*"NPR" + 0.012*"GOD" + 0.012*"Who's" + 0.011*"poor"
topic #8 (0.033): 0.083*"Iran" + 0.041*"US" + 0.038*"Israel" + 0.027*"Afghanistan" + 0.022*"Iraq" + 0.018*"Brasil" + 0.017*"to win" + 0.017*"American" + 0.014*"Egypt" + 0.013*"Saudi"
topic #17 (0.033): 0.036*"North Korea" + 0.029*"China" + 0.023*"Ireland" + 0.020*"South Korea" + 0.020*"Reuters" + 0.017*"attack" + 0.016*"Europe" + 0.015*"Indonesia" + 0.014*"US" + 0.014*"BBC News"
topic diff=0.033490, rho=0.147442
PROGRESS: pass 0, at document #94000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.083*"lol" + 0.040*"today" + 0.028*"you" + 0.026*"people" + 0.022*"time" + 0.021*"ur" + 0.020*"out" + 0.019*"can" + 0.018*"love" + 0.017*"am"
topic #17 (0.033): 0.038*"North Korea" + 0.030*"China" + 0.022*"Ireland" + 0.021*"South Korea" + 0.019*"Reuters" + 0.016*"attack" + 0.016*"Japan" + 0.016*"Europe" + 0.015*"Indonesia" + 0.014*"US"
topic #15 (0.033): 0.032*"USA" + 0.018*"here" + 0.017*"travel" + 0.014*"snow" + 0.012*"film" + 0.012*"PM" + 0.011*"sex" + 0.010*"Hollywood" + 0.010*"England" + 0.010*"Paris"
topic #21 (0.033): 0.043*"NATO" + 0.036*"blog" + 0.030*"CNN" + 0.023*"Chicago" + 0.022*"education" + 0.019*"cancer" + 0.017*"dans" + 0.016*"Russia" + 0.016*"Retweet" + 0.015*"Toronto"
topic #14 (0.033): 0.095*"tcot" + 0.020*"Bush" + 0.019*"Obama" + 0.017*"teaparty" + 0.013*"ocra" + 0.013*"Islam" + 0.012*"Muslim" + 0.010*"woman" + 0.010*"Palin" + 0.009*"Alaska"
topic diff=0.024708, rho=0.145865
PROGRESS: pass 0, at document #96000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.030*"USA" + 0.019*"travel" + 0.018*"here" + 0.015*"snow" + 0.013*"PM" + 0.012*"Thailand" + 0.011*"Woman" + 0.011*"film" + 0.011*"sex" + 0.010*"England"
topic #6 (0.033): 0.017*"U.S" + 0.013*"China" + 0.011*"Security" + 0.011*"World" + 0.010*"Women" + 0.010*"NYT" + 0.009*"France" + 0.009*"Study" + 0.008*"Will" + 0.008*"Plan"
topic #24 (0.033): 0.077*"who" + 0.022*"Nice" + 0.021*"me a" + 0.021*"Yes" + 0.021*"right" + 0.021*"a man" + 0.019*"Leslie Nielsen" + 0.018*"man" + 0.017*"Arizona" + 0.016*"Oh"
topic #27 (0.033): 0.038*"post" + 0.035*"blog" + 0.021*"DVD" + 0.019*"Complexo do Alemão" + 0.017*"Education" + 0.016*"iOS" + 0.014*"release" + 0.014*"recipes" + 0.014*"Oprah" + 0.013*"Amazon"
topic #10 (0.033): 0.115*"para" + 0.064*"un" + 0.045*"como" + 0.042*"dos" + 0.030*"en" + 0.024*"es" + 0.022*"la" + 0.020*"se" + 0.016*"pour" + 0.016*"si"
topic diff=0.026084, rho=0.144338
PROGRESS: pass 0, at document #98000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.121*"para" + 0.064*"un" + 0.044*"como" + 0.039*"dos" + 0.029*"en" + 0.026*"es" + 0.021*"la" + 0.020*"se" + 0.016*"si" + 0.015*"este"
topic #28 (0.033): 0.106*"video" + 0.055*"Video" + 0.054*"YouTube" + 0.028*"haha" + 0.028*"Cyber Monday" + 0.025*"music" + 0.021*"Holiday" + 0.015*"read" + 0.013*"Live" + 0.012*"Photography"
topic #7 (0.033): 0.131*"Twitter" + 0.087*"Facebook" + 0.023*"Tweet" + 0.021*"NYC" + 0.014*"People" + 0.013*"Social Media" + 0.012*"Internet" + 0.011*"Lmao" + 0.011*"website" + 0.011*"Chinese"
topic #22 (0.033): 0.031*"Blog" + 0.021*"re" + 0.021*"We" + 0.016*"believe" + 0.015*"one" + 0.015*"food" + 0.014*"see" + 0.013*"reason" + 0.012*"words" + 0.012*"WTF"
topic #25 (0.033): 0.076*"lol" + 0.040*"today" + 0.026*"people" + 0.025*"you" + 0.024*"ur" + 0.022*"time" + 0.020*"out" + 0.019*"can" + 0.019*"love" + 0.018*"am"
topic diff=0.046885, rho=0.142857
bound: at document #0
-24.174 per-word bound, 18924290.1 perplexity estimate based on a held-out corpus of 2000 documents with 44486 words
PROGRESS: pass 0, at document #100000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.031*"U.S" + 0.020*"Korea" + 0.019*"Obama" + 0.017*"Cancun" + 0.016*"President" + 0.015*"North Korea" + 0.014*"Seoul" + 0.014*"White House" + 0.013*"S. Korea" + 0.012*"cooking"
topic #29 (0.033): 0.180*"Thanksgiving" + 0.066*"Happy Thanksgiving" + 0.037*"LOL" + 0.035*"friends" + 0.030*"family" + 0.028*"holiday" + 0.027*"thanksgiving" + 0.027*"Turkey" + 0.022*"tonight" + 0.014*"Harry Potter"
topic #25 (0.033): 0.069*"lol" + 0.045*"today" + 0.026*"people" + 0.025*"you" + 0.024*"out" + 0.022*"ur" + 0.021*"time" + 0.019*"can" + 0.018*"who" + 0.017*"love"
topic #18 (0.033): 0.084*"WikiLeaks" + 0.078*"Wikileaks" + 0.032*"cablegate" + 0.031*"wikileaks" + 0.025*"US" + 0.019*"Pakistan" + 0.018*"China" + 0.017*"Wiki Leaks" + 0.014*"documents" + 0.013*"Guardian"
topic #9 (0.033): 0.024*"will" + 0.024*"em" + 0.021*"do" + 0.018*"work" + 0.015*"Americans" + 0.015*"Chelsea" + 0.014*"we" + 0.013*"MSNBC" + 0.012*"talk" + 0.011*"soul"
topic diff=0.028205, rho=0.141421
PROGRESS: pass 0, at document #102000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.088*"WikiLeaks" + 0.081*"Wikileaks" + 0.038*"wikileaks" + 0.031*"cablegate" + 0.024*"US" + 0.023*"Pakistan" + 0.018*"China" + 0.017*"Wiki Leaks" + 0.014*"documents" + 0.014*"Guardian"
topic #7 (0.033): 0.134*"Twitter" + 0.083*"Facebook" + 0.025*"Tweet" + 0.019*"NYC" + 0.014*"Social Media" + 0.014*"People" + 0.012*"website" + 0.012*"Internet" + 0.011*"Chinese" + 0.010*"New York Times"
topic #25 (0.033): 0.060*"lol" + 0.043*"today" + 0.026*"people" + 0.025*"you" + 0.023*"out" + 0.022*"time" + 0.022*"am" + 0.019*"can" + 0.019*"ur" + 0.019*"who"
topic #6 (0.033): 0.016*"U.S" + 0.012*"China" + 0.012*"Green" + 0.011*"World" + 0.010*"News" + 0.010*"NYT" + 0.009*"France" + 0.009*"Women" + 0.008*"Study" + 0.008*"NBN"
topic #28 (0.033): 0.101*"video" + 0.063*"Video" + 0.050*"YouTube" + 0.027*"haha" + 0.026*"Cyber Monday" + 0.022*"Holiday" + 0.022*"music" + 0.017*"read" + 0.011*"Live" + 0.011*"Photography"
topic diff=0.108242, rho=0.140028
PROGRESS: pass 0, at document #104000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.030*"News" + 0.021*"English" + 0.019*"NFL" + 0.018*"Texas" + 0.018*"USD" + 0.017*"bit.ly" + 0.015*"EUA" + 0.014*"Tory" + 0.014*"Report" + 0.012*"LMAO"
topic #6 (0.033): 0.016*"U.S" + 0.012*"China" + 0.012*"News" + 0.010*"World" + 0.010*"Green" + 0.009*"France" + 0.008*"NYT" + 0.008*"Will" + 0.008*"Women" + 0.008*"Security"
topic #13 (0.033): 0.031*"AP" + 0.020*"Amanda Knox" + 0.014*"child" + 0.012*"law" + 0.011*"risk" + 0.011*"DADT" + 0.010*"state" + 0.009*"military" + 0.009*"Pentagon" + 0.009*"Breaking News"
topic #18 (0.033): 0.081*"Wikileaks" + 0.078*"WikiLeaks" + 0.044*"cablegate" + 0.043*"wikileaks" + 0.026*"US" + 0.019*"Pakistan" + 0.017*"Wiki Leaks" + 0.016*"China" + 0.015*"Guardian" + 0.013*"documents"
topic #22 (0.033): 0.033*"Blog" + 0.022*"re" + 0.020*"We" + 0.017*"Gracias" + 0.015*"see" + 0.015*"Real Estate" + 0.015*"reason" + 0.014*"food" + 0.013*"basketball" + 0.013*"RTs"
topic diff=0.106265, rho=0.138675
PROGRESS: pass 0, at document #106000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.078*"UK" + 0.032*"London" + 0.026*"police" + 0.020*"students" + 0.016*"Labour" + 0.014*"Police" + 0.013*"demo 2010" + 0.013*"British" + 0.013*"protest" + 0.013*"photography"
topic #8 (0.033): 0.063*"Iran" + 0.043*"US" + 0.034*"Israel" + 0.024*"Afghanistan" + 0.024*"Brasil" + 0.021*"to win" + 0.019*"American" + 0.016*"Iraq" + 0.012*"BBC" + 0.012*"pode"
topic #19 (0.033): 0.050*"Today" + 0.038*"Love" + 0.038*"white" + 0.028*"Time" + 0.023*"Life" + 0.020*"nice" + 0.018*"Leslie Nielsen" + 0.016*"Actor" + 0.016*"right now" + 0.015*"Wow"
topic #27 (0.033): 0.052*"blog" + 0.037*"post" + 0.019*"DVD" + 0.018*"Amazon" + 0.017*"web" + 0.015*"PC" + 0.014*"Oprah" + 0.014*"iOS" + 0.013*"release" + 0.012*"Education"
topic #2 (0.033): 0.030*"News" + 0.022*"NFL" + 0.019*"English" + 0.019*"Texas" + 0.017*"USD" + 0.016*"bit.ly" + 0.014*"Boston" + 0.014*"LMAO" + 0.013*"Report" + 0.013*"Tom DeLay"
topic diff=0.032801, rho=0.137361
PROGRESS: pass 0, at document #108000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.196*"Thanksgiving" + 0.063*"Happy Thanksgiving" + 0.031*"holiday" + 0.031*"LOL" + 0.031*"friends" + 0.028*"family" + 0.024*"Turkey" + 0.023*"thanksgiving" + 0.023*"tonight" + 0.015*"Black Friday"
topic #9 (0.033): 0.024*"will" + 0.019*"em" + 0.019*"do" + 0.016*"work" + 0.015*"Americans" + 0.015*"Chelsea" + 0.014*"White" + 0.013*"we" + 0.013*"MSNBC" + 0.012*"talk"
topic #4 (0.033): 0.046*"Mc" + 0.031*"U.S" + 0.019*"Korea" + 0.018*"Obama" + 0.017*"Yahoo! News" + 0.017*"President" + 0.013*"S. Korea" + 0.013*"North Korea" + 0.013*"Hoy" + 0.012*"Cancun"
topic #19 (0.033): 0.049*"Today" + 0.038*"Love" + 0.035*"white" + 0.031*"Time" + 0.022*"Life" + 0.020*"nice" + 0.020*"Leslie Nielsen" + 0.019*"Actor" + 0.018*"right now" + 0.015*"Wow"
topic #24 (0.033): 0.072*"who" + 0.026*"Leslie Nielsen" + 0.021*"Nice" + 0.020*"me a" + 0.019*"a man" + 0.019*"Madrid" + 0.019*"right" + 0.018*"Yes" + 0.018*"actor" + 0.017*"Arizona"
topic diff=0.031759, rho=0.136083
PROGRESS: pass 0, at document #110000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.064*"via" + 0.042*"iPhone" + 0.040*"iPad" + 0.039*"Google" + 0.038*"Apple" + 0.024*"Android" + 0.023*"Facebook" + 0.018*"Christmas" + 0.017*"app" + 0.016*"iTunes"
topic #4 (0.033): 0.039*"Mc" + 0.032*"U.S" + 0.020*"Korea" + 0.019*"President" + 0.018*"Obama" + 0.015*"Yahoo! News" + 0.013*"White House" + 0.012*"S. Korea" + 0.012*"North Korea" + 0.011*"Hoy"
topic #21 (0.033): 0.038*"NATO" + 0.029*"blog" + 0.024*"Chicago" + 0.022*"cancer" + 0.022*"CNN" + 0.019*"Arsenal" + 0.019*"Venezuela" + 0.018*"dans" + 0.017*"Retweet" + 0.016*"education"
topic #10 (0.033): 0.094*"para" + 0.075*"un" + 0.039*"como" + 0.031*"es" + 0.028*"en" + 0.026*"dos" + 0.024*"pour" + 0.023*"la" + 0.018*"se" + 0.017*"México"
topic #8 (0.033): 0.059*"Iran" + 0.045*"US" + 0.028*"Israel" + 0.022*"Afghanistan" + 0.020*"Brasil" + 0.019*"American" + 0.019*"to win" + 0.014*"Election" + 0.013*"Iraq" + 0.012*"BBC"
topic diff=0.087381, rho=0.134840
PROGRESS: pass 0, at document #112000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.032*"football" + 0.025*"game" + 0.024*"AP" + 0.022*"Spanish" + 0.019*"Italian" + 0.017*"team" + 0.015*"reply" + 0.014*"net" + 0.014*"beautiful" + 0.013*"haha"
topic #12 (0.033): 0.126*"Celebrity" + 0.074*"God" + 0.041*"Who" + 0.022*"al" + 0.014*"left" + 0.013*"green" + 0.013*"NPR" + 0.012*"Who's" + 0.011*"poor" + 0.010*"America"
topic #6 (0.033): 0.019*"U.S" + 0.013*"News" + 0.012*"China" + 0.012*"World" + 0.011*"Billion" + 0.010*"Women" + 0.009*"Will" + 0.008*"France" + 0.007*"Green" + 0.007*"Health"
topic #9 (0.033): 0.025*"will" + 0.018*"do" + 0.018*"work" + 0.015*"em" + 0.014*"Americans" + 0.014*"we" + 0.012*"White" + 0.012*"don" + 0.012*"Chelsea" + 0.011*"talk"
topic #16 (0.033): 0.041*"Black Friday" + 0.028*"Sarah Palin" + 0.021*"newspaper" + 0.020*"If" + 0.019*"Canada" + 0.019*"HIV" + 0.018*"DWTS" + 0.016*"Here" + 0.013*"AIDS" + 0.013*"gay"
topic diff=0.039654, rho=0.133631
PROGRESS: pass 0, at document #114000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.188*"Thanksgiving" + 0.066*"Happy Thanksgiving" + 0.036*"LOL" + 0.031*"friends" + 0.030*"family" + 0.028*"holiday" + 0.024*"Turkey" + 0.021*"tonight" + 0.021*"thanksgiving" + 0.015*"Black Friday"
topic #11 (0.033): 0.065*"via" + 0.041*"iPhone" + 0.040*"iPad" + 0.040*"Google" + 0.035*"Apple" + 0.024*"Facebook" + 0.023*"Android" + 0.018*"Christmas" + 0.017*"Beatles" + 0.016*"app"
topic #18 (0.033): 0.085*"WikiLeaks" + 0.080*"Wikileaks" + 0.035*"wikileaks" + 0.032*"cablegate" + 0.025*"US" + 0.025*"Pakistan" + 0.016*"Wiki Leaks" + 0.015*"China" + 0.014*"documents" + 0.013*"Guardian"
topic #14 (0.033): 0.063*"tcot" + 0.023*"Bush" + 0.018*"Colombian" + 0.017*"Obama" + 0.011*"woman" + 0.010*"teaparty" + 0.010*"Muslim" + 0.009*"Palin" + 0.009*"Alaska" + 0.008*"GM"
topic #4 (0.033): 0.032*"Mc" + 0.031*"U.S" + 0.020*"Korea" + 0.019*"President" + 0.018*"Obama" + 0.014*"White House" + 0.013*"Cancun" + 0.013*"Hoy" + 0.013*"S. Korea" + 0.012*"North Korea"
topic diff=0.019293, rho=0.132453
PROGRESS: pass 0, at document #116000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.029*"U.S" + 0.028*"Mc" + 0.019*"Korea" + 0.019*"President" + 0.017*"Obama" + 0.015*"Hoy" + 0.015*"White House" + 0.014*"Cancun" + 0.013*"S. Korea" + 0.012*"North Korea"
topic #23 (0.033): 0.029*"football" + 0.026*"game" + 0.022*"AP" + 0.019*"Spanish" + 0.017*"team" + 0.017*"Italian" + 0.015*"net" + 0.014*"reply" + 0.013*"haha" + 0.013*"beautiful"
topic #5 (0.033): 0.064*"fb" + 0.020*"season" + 0.019*"NBC" + 0.017*"Football" + 0.017*"student" + 0.015*"smile" + 0.014*"Sports" + 0.014*"EEUU" + 0.013*"high school" + 0.012*"Wisconsin"
topic #15 (0.033): 0.040*"USA" + 0.027*"French" + 0.020*"Thailand" + 0.016*"here" + 0.016*"Thai" + 0.016*"travel" + 0.015*"England" + 0.013*"sex" + 0.013*"snow" + 0.012*"Indonesian"
topic #6 (0.033): 0.026*"jajaja" + 0.018*"U.S" + 0.014*"News" + 0.013*"China" + 0.012*"World" + 0.010*"Billion" + 0.010*"Women" + 0.009*"Will" + 0.007*"BP" + 0.007*"NYT"
topic diff=0.035117, rho=0.131306
PROGRESS: pass 0, at document #118000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.109*"video" + 0.088*"YouTube" + 0.062*"Video" + 0.027*"Cyber Monday" + 0.025*"Holiday" + 0.023*"music" + 0.018*"haha" + 0.011*"Live" + 0.010*"Photography" + 0.010*"read"
topic #13 (0.033): 0.083*"Breaking News" + 0.021*"Rome" + 0.019*"AP" + 0.019*"DADT" + 0.010*"child" + 0.010*"risk" + 0.009*"state" + 0.009*"law" + 0.009*"Amanda Knox" + 0.008*"perfect"
topic #3 (0.033): 0.064*"UK" + 0.038*"London" + 0.025*"police" + 0.020*"Cuba" + 0.019*"students" + 0.015*"Police" + 0.014*"protest" + 0.013*"British" + 0.011*"demo 2010" + 0.011*"Colombia"
topic #19 (0.033): 0.049*"Today" + 0.049*"Love" + 0.028*"Time" + 0.024*"Life" + 0.024*"white" + 0.023*"Leslie Nielsen" + 0.017*"nice" + 0.017*"right now" + 0.017*"fear" + 0.016*"Actor"
topic #14 (0.033): 0.057*"tcot" + 0.020*"Bush" + 0.017*"Obama" + 0.014*"Colombian" + 0.011*"woman" + 0.010*"teaparty" + 0.009*"GM" + 0.009*"Muslim" + 0.009*"Alaska" + 0.008*"FBI"
topic diff=0.020660, rho=0.130189
bound: at document #0
-23.318 per-word bound, 10457704.5 perplexity estimate based on a held-out corpus of 2000 documents with 33817 words
PROGRESS: pass 0, at document #120000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.065*"UK" + 0.036*"London" + 0.026*"police" + 0.019*"students" + 0.019*"Cuba" + 0.015*"protest" + 0.015*"Police" + 0.012*"British" + 0.012*"Colombia" + 0.011*"demo 2010"
topic #27 (0.033): 0.051*"blog" + 0.049*"release" + 0.033*"post" + 0.020*"iOS" + 0.020*"Amazon" + 0.019*"DVD" + 0.018*"PC" + 0.015*"web" + 0.015*"Oprah" + 0.011*"Complexo do Alemão"
topic #20 (0.033): 0.044*"TV" + 0.042*"live" + 0.032*"Watch" + 0.028*"Hope" + 0.025*"world" + 0.020*"climate" + 0.019*"interview" + 0.018*"families" + 0.017*"Check it" + 0.017*"climate change"
topic #6 (0.033): 0.022*"jajaja" + 0.018*"U.S" + 0.013*"News" + 0.013*"China" + 0.012*"World" + 0.010*"Women" + 0.009*"Billion" + 0.008*"Will" + 0.008*"Health" + 0.008*"Study"
topic #11 (0.033): 0.059*"via" + 0.045*"Google" + 0.043*"iPad" + 0.041*"iPhone" + 0.039*"Apple" + 0.030*"Facebook" + 0.022*"Android" + 0.017*"Christmas" + 0.017*"iTunes" + 0.016*"Microsoft"
topic diff=0.038554, rho=0.129099
PROGRESS: pass 0, at document #122000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.039*"NATO" + 0.028*"Retweet" + 0.027*"Chicago" + 0.024*"CNN" + 0.022*"Venezuela" + 0.021*"dans" + 0.021*"blog" + 0.019*"cancer" + 0.017*"Russia" + 0.016*"Arsenal"
topic #18 (0.033): 0.084*"WikiLeaks" + 0.073*"Wikileaks" + 0.029*"wikileaks" + 0.026*"cablegate" + 0.025*"US" + 0.023*"Pakistan" + 0.016*"Wiki Leaks" + 0.014*"China" + 0.013*"documents" + 0.011*"Guardian"
topic #11 (0.033): 0.058*"via" + 0.044*"iPad" + 0.044*"Google" + 0.041*"iPhone" + 0.039*"Apple" + 0.031*"Facebook" + 0.022*"Android" + 0.017*"iTunes" + 0.017*"Christmas" + 0.015*"Beatles"
topic #12 (0.033): 0.105*"God" + 0.066*"Celebrity" + 0.040*"Who" + 0.023*"al" + 0.014*"left" + 0.013*"DREAM Act" + 0.012*"NPR" + 0.012*"poor" + 0.010*"America" + 0.010*"green"
topic #3 (0.033): 0.065*"UK" + 0.036*"London" + 0.026*"police" + 0.018*"students" + 0.017*"Cuba" + 0.014*"protest" + 0.014*"Police" + 0.013*"British" + 0.012*"Colombia" + 0.011*"demo 2010"
topic diff=0.021174, rho=0.128037
PROGRESS: pass 0, at document #124000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.084*"WikiLeaks" + 0.071*"Wikileaks" + 0.029*"wikileaks" + 0.026*"US" + 0.025*"cablegate" + 0.021*"Pakistan" + 0.015*"China" + 0.014*"Wiki Leaks" + 0.013*"Guardian" + 0.013*"documents"
topic #16 (0.033): 0.044*"Black Friday" + 0.030*"Sarah Palin" + 0.018*"HIV" + 0.017*"DWTS" + 0.017*"If" + 0.016*"Canada" + 0.015*"Here" + 0.014*"newspaper" + 0.012*"gay" + 0.011*"Dec"
topic #20 (0.033): 0.043*"TV" + 0.041*"live" + 0.033*"Watch" + 0.025*"Hope" + 0.025*"world" + 0.022*"interview" + 0.020*"climate" + 0.019*"families" + 0.018*"Check it" + 0.016*"Check it out"
topic #28 (0.033): 0.121*"video" + 0.071*"YouTube" + 0.061*"Video" + 0.024*"Holiday" + 0.023*"Cyber Monday" + 0.023*"music" + 0.017*"haha" + 0.012*"Photography" + 0.010*"Check" + 0.010*"Live"
topic #13 (0.033): 0.060*"Breaking News" + 0.022*"AP" + 0.016*"DADT" + 0.016*"Rome" + 0.011*"child" + 0.011*"risk" + 0.010*"law" + 0.009*"state" + 0.009*"report" + 0.008*"weather"
topic diff=0.040626, rho=0.127000
PROGRESS: pass 0, at document #126000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.051*"tcot" + 0.017*"Bush" + 0.016*"Obama" + 0.012*"woman" + 0.010*"Alaska" + 0.009*"FBI" + 0.009*"Muslim" + 0.009*"Colombian" + 0.009*"GM" + 0.008*"teaparty"
topic #16 (0.033): 0.046*"Black Friday" + 0.029*"Sarah Palin" + 0.019*"HIV" + 0.017*"If" + 0.016*"DWTS" + 0.015*"Canada" + 0.015*"Here" + 0.013*"gay" + 0.012*"newspaper" + 0.011*"Dec"
topic #1 (0.033): 0.051*"GOP" + 0.050*"TSA" + 0.042*"Obama" + 0.024*"ppl" + 0.021*"Congress" + 0.019*"Republicans" + 0.018*"Senate" + 0.015*"House" + 0.014*"vote" + 0.012*"politics"
topic #24 (0.033): 0.077*"who" + 0.028*"a man" + 0.022*"Nice" + 0.020*"always" + 0.019*"me a" + 0.018*"Yes" + 0.018*"actor" + 0.017*"Oh" + 0.017*"right" + 0.016*"man"
topic #26 (0.033): 0.052*"India" + 0.033*"news" + 0.021*"New Zealand" + 0.019*"NZ" + 0.015*"business" + 0.013*"mine" + 0.013*"Australia" + 0.013*"killed" + 0.012*"energy" + 0.012*"dead"
topic diff=0.056345, rho=0.125988
PROGRESS: pass 0, at document #128000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.047*"Iran" + 0.045*"US" + 0.031*"Israel" + 0.024*"Afghanistan" + 0.022*"to win" + 0.021*"Brasil" + 0.017*"American" + 0.013*"Iraq" + 0.012*"pode" + 0.011*"BBC"
topic #10 (0.033): 0.093*"para" + 0.073*"un" + 0.045*"como" + 0.034*"es" + 0.031*"dos" + 0.030*"la" + 0.027*"en" + 0.019*"se" + 0.016*"más" + 0.016*"si"
topic #7 (0.033): 0.376*"Twitter" + 0.060*"Facebook" + 0.021*"NYC" + 0.017*"Tweet" + 0.010*"People" + 0.008*"Online" + 0.008*"Internet" + 0.007*"Social Media" + 0.007*"photos" + 0.007*"Lmao"
topic #15 (0.033): 0.084*"Home" + 0.028*"USA" + 0.017*"here" + 0.017*"French" + 0.015*"snow" + 0.015*"travel" + 0.015*"England" + 0.014*"Thailand" + 0.012*"film" + 0.011*"sex"
topic #0 (0.033): 0.072*"Haiti" + 0.038*"Amazon.com" + 0.026*"cholera" + 0.022*"Sky News" + 0.021*"UN" + 0.018*"Mexico" + 0.017*"Asian" + 0.017*"Dutch" + 0.016*"Gaza" + 0.015*"Israeli"
topic diff=0.035108, rho=0.125000
PROGRESS: pass 0, at document #130000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.107*"God" + 0.043*"Who" + 0.038*"Celebrity" + 0.021*"al" + 0.015*"Who's" + 0.014*"left" + 0.012*"guy" + 0.011*"GOD" + 0.011*"evil" + 0.010*"NPR"
topic #27 (0.033): 0.054*"blog" + 0.032*"release" + 0.031*"post" + 0.021*"DVD" + 0.020*"Oprah" + 0.020*"Amazon" + 0.019*"iOS" + 0.017*"recipes" + 0.016*"PC" + 0.013*"Complexo do Alemão"
topic #11 (0.033): 0.056*"via" + 0.043*"Google" + 0.039*"iPad" + 0.039*"iPhone" + 0.035*"Apple" + 0.032*"Facebook" + 0.021*"Android" + 0.019*"iTunes" + 0.018*"Christmas" + 0.018*"app"
topic #29 (0.033): 0.190*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.041*"LOL" + 0.038*"family" + 0.032*"friends" + 0.025*"holiday" + 0.024*"thanksgiving" + 0.021*"Turkey" + 0.021*"tonight" + 0.014*"Black Friday"
topic #28 (0.033): 0.124*"video" + 0.066*"YouTube" + 0.062*"Video" + 0.024*"music" + 0.022*"Holiday" + 0.022*"Cyber Monday" + 0.016*"haha" + 0.013*"HD" + 0.010*"Check" + 0.010*"Live"
topic diff=0.022639, rho=0.124035
PROGRESS: pass 0, at document #132000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.042*"tcot" + 0.017*"Obama" + 0.016*"Bush" + 0.014*"Redskins" + 0.012*"Alaska" + 0.011*"woman" + 0.009*"Muslim" + 0.008*"Ohio" + 0.008*"FBI" + 0.008*"GM"
topic #3 (0.033): 0.071*"UK" + 0.035*"London" + 0.021*"police" + 0.020*"students" + 0.016*"photography" + 0.015*"British" + 0.015*"protest" + 0.014*"Cuba" + 0.013*"Police" + 0.012*"Colombia"
topic #24 (0.033): 0.076*"who" + 0.026*"a man" + 0.022*"Nice" + 0.021*"always" + 0.019*"me a" + 0.019*"Oh" + 0.018*"Yes" + 0.016*"right" + 0.016*"Arizona" + 0.015*"actor"
topic #12 (0.033): 0.108*"God" + 0.041*"Who" + 0.037*"Celebrity" + 0.021*"al" + 0.014*"Who's" + 0.014*"left" + 0.011*"guy" + 0.011*"GOD" + 0.010*"NPR" + 0.010*"evil"
topic #22 (0.033): 0.068*"week" + 0.038*"Blog" + 0.022*"book" + 0.020*"We" + 0.020*"re" + 0.016*"Vintage" + 0.014*"believe" + 0.014*"basketball" + 0.013*"one" + 0.012*"reason"
topic diff=0.029016, rho=0.123091
PROGRESS: pass 0, at document #134000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.055*"via" + 0.041*"iPhone" + 0.040*"Google" + 0.037*"iPad" + 0.034*"Apple" + 0.032*"Facebook" + 0.021*"iTunes" + 0.020*"Android" + 0.020*"Christmas" + 0.017*"app"
topic #12 (0.033): 0.108*"God" + 0.041*"Who" + 0.033*"Celebrity" + 0.020*"al" + 0.014*"Who's" + 0.013*"left" + 0.013*"Hawaii" + 0.012*"guy" + 0.012*"NPR" + 0.010*"GOD"
topic #26 (0.033): 0.047*"India" + 0.032*"news" + 0.022*"New Zealand" + 0.017*"business" + 0.016*"NZ" + 0.014*"mine" + 0.014*"energy" + 0.013*"killed" + 0.013*"Australia" + 0.013*"dead"
topic #5 (0.033): 0.052*"IM" + 0.027*"fb" + 0.023*"season" + 0.020*"student" + 0.019*"Football" + 0.018*"NBC" + 0.018*"smile" + 0.016*"high school" + 0.015*"Wisconsin" + 0.012*"Gold"
topic #27 (0.033): 0.080*"blog" + 0.029*"post" + 0.027*"release" + 0.021*"iOS" + 0.020*"Oprah" + 0.018*"Amazon" + 0.018*"DVD" + 0.015*"recipes" + 0.014*"PC" + 0.013*"Choice"
topic diff=0.022792, rho=0.122169
PROGRESS: pass 0, at document #136000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #7 (0.033): 0.297*"Twitter" + 0.064*"Facebook" + 0.026*"NYC" + 0.019*"Tweet" + 0.016*"Lady Gaga" + 0.015*"Founder" + 0.010*"People" + 0.009*"Lmao" + 0.009*"photos" + 0.009*"website"
topic #21 (0.033): 0.049*"CNN" + 0.041*"Retweet" + 0.033*"NATO" + 0.025*"Chicago" + 0.022*"cancer" + 0.017*"Friday" + 0.016*"Pope" + 0.015*"blog" + 0.015*"Toronto" + 0.015*"Venezuela"
topic #2 (0.033): 0.049*"bit.ly" + 0.037*"News" + 0.025*"NFL" + 0.023*"Boston" + 0.022*"English" + 0.020*"LMAO" + 0.016*"Texas" + 0.012*"planet" + 0.012*"EUA" + 0.011*"songs"
topic #16 (0.033): 0.041*"Black Friday" + 0.034*"chat" + 0.025*"cash" + 0.024*"Sarah Palin" + 0.017*"DWTS" + 0.015*"HIV" + 0.014*"If" + 0.014*"Here" + 0.013*"Canada" + 0.013*"Dec"
topic #29 (0.033): 0.195*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.035*"LOL" + 0.034*"family" + 0.032*"friends" + 0.030*"holiday" + 0.023*"tonight" + 0.022*"Turkey" + 0.022*"thanksgiving" + 0.016*"Black Friday"
topic diff=0.016903, rho=0.121268
PROGRESS: pass 0, at document #138000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.042*"Black Friday" + 0.031*"chat" + 0.024*"Sarah Palin" + 0.023*"cash" + 0.016*"DWTS" + 0.015*"HIV" + 0.015*"If" + 0.015*"Here" + 0.013*"Dec" + 0.013*"Canada"
topic #15 (0.033): 0.050*"Home" + 0.030*"sex" + 0.026*"USA" + 0.021*"girl" + 0.019*"here" + 0.016*"snow" + 0.015*"travel" + 0.014*"French" + 0.014*"England" + 0.011*"film"
topic #5 (0.033): 0.041*"IM" + 0.029*"fb" + 0.021*"student" + 0.021*"Bingo" + 0.021*"season" + 0.019*"Football" + 0.018*"smile" + 0.018*"NBC" + 0.017*"high school" + 0.014*"Wisconsin"
topic #17 (0.033): 0.043*"China" + 0.039*"North Korea" + 0.026*"South Korea" + 0.026*"Ireland" + 0.020*"Irish" + 0.019*"Japan" + 0.018*"BBC News" + 0.016*"US" + 0.015*"Korea" + 0.014*"Reuters"
topic #10 (0.033): 0.182*"Como" + 0.070*"para" + 0.061*"un" + 0.035*"como" + 0.029*"es" + 0.023*"la" + 0.022*"en" + 0.020*"dos" + 0.015*"se" + 0.015*"este"
topic diff=0.022947, rho=0.120386
bound: at document #0
-22.288 per-word bound, 5120473.4 perplexity estimate based on a held-out corpus of 2000 documents with 78501 words
PROGRESS: pass 0, at document #140000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.044*"live" + 0.039*"TV" + 0.033*"Check it" + 0.031*"world" + 0.029*"COP16" + 0.029*"Watch" + 0.028*"Check it out" + 0.028*"Hope" + 0.023*"interview" + 0.018*"families"
topic #26 (0.033): 0.038*"India" + 0.032*"news" + 0.022*"New Zealand" + 0.017*"business" + 0.016*"mine" + 0.014*"dead" + 0.014*"killed" + 0.013*"feeling" + 0.013*"Australia" + 0.013*"NZ"
topic #0 (0.033): 0.069*"Haiti" + 0.024*"cholera" + 0.021*"UN" + 0.019*"Amazon.com" + 0.015*"Asian" + 0.015*"Mexico" + 0.014*"Israeli" + 0.014*"election" + 0.014*"Gaza" + 0.014*"death"
topic #28 (0.033): 0.139*"video" + 0.063*"Video" + 0.056*"YouTube" + 0.026*"Holiday" + 0.024*"music" + 0.022*"Cyber Monday" + 0.019*"WOW" + 0.017*"Live" + 0.016*"haha" + 0.012*"Photography"
topic #14 (0.033): 0.051*"tcot" + 0.029*"hanging" + 0.018*"Bush" + 0.018*"Obama" + 0.010*"woman" + 0.010*"Alaska" + 0.009*"Redskins" + 0.009*"teaparty" + 0.008*"Palin" + 0.008*"Sarah Palin"
topic diff=0.072821, rho=0.119523
PROGRESS: pass 0, at document #142000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.039*"Home" + 0.027*"USA" + 0.026*"sex" + 0.018*"here" + 0.017*"girl" + 0.016*"snow" + 0.014*"School" + 0.014*"travel" + 0.013*"England" + 0.012*"film"
topic #27 (0.033): 0.083*"blog" + 0.035*"post" + 0.024*"Back" + 0.024*"Education" + 0.021*"release" + 0.020*"iOS" + 0.019*"Oprah" + 0.017*"DVD" + 0.017*"Amazon" + 0.014*"check it"
topic #7 (0.033): 0.261*"Twitter" + 0.062*"NYC" + 0.061*"Facebook" + 0.025*"kids" + 0.018*"Tweet" + 0.014*"Lady Gaga" + 0.010*"Founder" + 0.009*"website" + 0.008*"People" + 0.008*"Chinese"
topic #21 (0.033): 0.042*"CNN" + 0.040*"Retweet" + 0.034*"NATO" + 0.022*"Chicago" + 0.022*"cancer" + 0.020*"Friday" + 0.017*"Toronto" + 0.017*"Venezuela" + 0.016*"Pope" + 0.015*"Russia"
topic #3 (0.033): 0.060*"UK" + 0.036*"London" + 0.021*"police" + 0.020*"school" + 0.018*"students" + 0.016*"Colombia" + 0.015*"British" + 0.013*"photography" + 0.013*"protest" + 0.012*"Police"
topic diff=0.025758, rho=0.118678
bound: at document #0
-23.361 per-word bound, 10776698.1 perplexity estimate based on a held-out corpus of 1749 documents with 49577 words
PROGRESS: pass 0, at document #143749/143749
performing inference on a chunk of 1749 documents
1749/1749 documents converged within 50 iterations
updating topics
merging changes from 1749 documents into a model of 143749 documents
topic #21 (0.033): 0.041*"Retweet" + 0.038*"CNN" + 0.036*"NATO" + 0.024*"cancer" + 0.021*"Chicago" + 0.020*"Friday" + 0.018*"Russia" + 0.016*"speech" + 0.016*"Pope" + 0.015*"Toronto"
topic #28 (0.033): 0.130*"video" + 0.064*"Video" + 0.056*"YouTube" + 0.025*"Holiday" + 0.024*"music" + 0.021*"Cyber Monday" + 0.020*"WOW" + 0.020*"haha" + 0.016*"Live" + 0.011*"Check"
topic #13 (0.033): 0.027*"AP" + 0.025*"DADT" + 0.024*"Breaking News" + 0.022*"add" + 0.013*"Notre Dame" + 0.011*"Art" + 0.010*"child" + 0.009*"risk" + 0.009*"Pentagon" + 0.009*"Rome"
topic #10 (0.033): 0.115*"Como" + 0.072*"un" + 0.070*"para" + 0.039*"como" + 0.035*"es" + 0.028*"en" + 0.026*"la" + 0.018*"dos" + 0.017*"este" + 0.017*"más"
topic #26 (0.033): 0.036*"news" + 0.034*"India" + 0.020*"New Zealand" + 0.017*"business" + 0.015*"feeling" + 0.014*"mine" + 0.014*"banned" + 0.013*"dead" + 0.012*"killed" + 0.012*"Australia"
topic diff=0.015853, rho=0.117851
PROGRESS: pass 1, at document #2000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.054*"TSA" + 0.046*"Obama" + 0.042*"GOP" + 0.029*"ppl" + 0.019*"Senate" + 0.016*"Republicans" + 0.016*"vote" + 0.016*"Palin" + 0.016*"Congress" + 0.013*"House"
topic #27 (0.033): 0.079*"blog" + 0.051*"Back" + 0.034*"post" + 0.021*"iOS" + 0.019*"Education" + 0.018*"release" + 0.018*"Amazon" + 0.017*"DVD" + 0.016*"Oprah" + 0.013*"web"
topic #8 (0.033): 0.042*"US" + 0.042*"Iran" + 0.029*"Israel" + 0.023*"Afghanistan" + 0.022*"to win" + 0.016*"American" + 0.013*"Egypt" + 0.013*"Nato" + 0.011*"Iraq" + 0.010*"Brasil"
topic #21 (0.033): 0.037*"Retweet" + 0.036*"CNN" + 0.034*"NATO" + 0.022*"cancer" + 0.020*"Chicago" + 0.019*"Friday" + 0.018*"Russia" + 0.017*"Pope" + 0.015*"Toronto" + 0.015*"speech"
topic #26 (0.033): 0.032*"news" + 0.031*"India" + 0.020*"New Zealand" + 0.015*"business" + 0.015*"mine" + 0.013*"feeling" + 0.013*"dead" + 0.012*"NZ" + 0.012*"killed" + 0.012*"banned"
topic diff=0.026874, rho=0.116346
PROGRESS: pass 1, at document #4000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.025*"will" + 0.017*"work" + 0.017*"don" + 0.016*"kids" + 0.014*"do" + 0.013*"god" + 0.013*"talk" + 0.012*"soul" + 0.011*"play" + 0.011*"VIDEO"
topic #17 (0.033): 0.039*"China" + 0.038*"North Korea" + 0.031*"party" + 0.026*"Ireland" + 0.024*"South Korea" + 0.019*"Irish" + 0.016*"Japan" + 0.015*"Korea" + 0.014*"US" + 0.013*"attack"
topic #23 (0.033): 0.031*"game" + 0.025*"football" + 0.022*"RETWEET" + 0.017*"net" + 0.015*"haha" + 0.015*"AP" + 0.014*"show" + 0.013*"beautiful" + 0.013*"team" + 0.012*"Apple iPad"
topic #14 (0.033): 0.045*"tcot" + 0.043*"hanging" + 0.016*"Obama" + 0.015*"Bush" + 0.009*"woman" + 0.009*"Alaska" + 0.008*"Sarah Palin" + 0.007*"Palin" + 0.007*"Redskins" + 0.007*"teaparty"
topic #24 (0.033): 0.069*"who" + 0.029*"college" + 0.024*"fat" + 0.022*"a man" + 0.019*"Nice" + 0.018*"Oh" + 0.018*"me a" + 0.018*"Yes" + 0.018*"always" + 0.014*"actor"
topic diff=0.043716, rho=0.116346
PROGRESS: pass 1, at document #6000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.024*"will" + 0.018*"work" + 0.017*"don" + 0.016*"kids" + 0.014*"do" + 0.013*"god" + 0.012*"talk" + 0.012*"soul" + 0.011*"MSNBC" + 0.011*"VIDEO"
topic #17 (0.033): 0.039*"China" + 0.038*"North Korea" + 0.029*"party" + 0.028*"Ireland" + 0.024*"South Korea" + 0.020*"Irish" + 0.016*"Japan" + 0.015*"Korea" + 0.014*"US" + 0.014*"attack"
topic #25 (0.033): 0.046*"lol" + 0.039*"tweet" + 0.033*"today" + 0.029*"people" + 0.028*"give me" + 0.024*"you" + 0.022*"time" + 0.020*"now" + 0.020*"out" + 0.020*"twitter"
topic #20 (0.033): 0.045*"live" + 0.035*"TV" + 0.033*"Check it" + 0.031*"Watch" + 0.030*"Check it out" + 0.029*"world" + 0.025*"Hope" + 0.024*"COP16" + 0.021*"interview" + 0.018*"families"
topic #13 (0.033): 0.032*"AP" + 0.021*"DADT" + 0.018*"Breaking News" + 0.017*"add" + 0.011*"Art" + 0.010*"risk" + 0.010*"Pentagon" + 0.009*"Notre Dame" + 0.009*"child" + 0.008*"report"
topic diff=0.055740, rho=0.116346
PROGRESS: pass 1, at document #8000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.070*"blog" + 0.038*"Back" + 0.031*"post" + 0.026*"iOS" + 0.020*"Amazon" + 0.017*"release" + 0.016*"Education" + 0.015*"DVD" + 0.015*"web" + 0.014*"Oprah"
topic #23 (0.033): 0.029*"game" + 0.025*"football" + 0.019*"RETWEET" + 0.017*"AP" + 0.016*"net" + 0.014*"show" + 0.013*"haha" + 0.013*"team" + 0.013*"beautiful" + 0.013*"Kanye West"
topic #13 (0.033): 0.029*"AP" + 0.019*"DADT" + 0.016*"Breaking News" + 0.015*"add" + 0.010*"Art" + 0.010*"risk" + 0.010*"Pentagon" + 0.009*"report" + 0.008*"child" + 0.008*"law"
topic #5 (0.033): 0.026*"NBC" + 0.023*"student" + 0.019*"fb" + 0.019*"high school" + 0.018*"IM" + 0.017*"season" + 0.015*"Wisconsin" + 0.013*"Football" + 0.013*"smile" + 0.013*"NY"
topic #10 (0.033): 0.088*"Como" + 0.074*"un" + 0.073*"para" + 0.039*"como" + 0.031*"es" + 0.029*"en" + 0.026*"la" + 0.020*"dos" + 0.017*"este" + 0.016*"las"
topic diff=0.041177, rho=0.116346
PROGRESS: pass 1, at document #10000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.053*"Blog" + 0.028*"week" + 0.022*"We" + 0.020*"re" + 0.019*"book" + 0.016*"believe" + 0.015*"basketball" + 0.014*"change" + 0.013*"learn" + 0.011*"one"
topic #9 (0.033): 0.025*"will" + 0.018*"work" + 0.015*"don" + 0.015*"kids" + 0.014*"do" + 0.012*"em" + 0.011*"talk" + 0.011*"VIDEO" + 0.011*"MSNBC" + 0.011*"play"
topic #25 (0.033): 0.042*"lol" + 0.038*"tweet" + 0.037*"today" + 0.029*"people" + 0.024*"give me" + 0.023*"you" + 0.022*"out" + 0.022*"time" + 0.020*"twitter" + 0.019*"now"
topic #20 (0.033): 0.039*"live" + 0.037*"TV" + 0.030*"Watch" + 0.027*"Check it" + 0.026*"world" + 0.025*"Check it out" + 0.023*"Hope" + 0.022*"COP16" + 0.019*"interview" + 0.019*"families"
topic #19 (0.033): 0.038*"Love" + 0.038*"Today" + 0.033*"Leslie Nielsen" + 0.026*"Time" + 0.025*"Music" + 0.023*"need" + 0.022*"Life" + 0.017*"right now" + 0.017*"nice" + 0.016*"big"
topic diff=0.041744, rho=0.116346
PROGRESS: pass 1, at document #12000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.068*"blog" + 0.033*"post" + 0.032*"Back" + 0.031*"iOS" + 0.021*"Amazon" + 0.018*"release" + 0.016*"DVD" + 0.015*"web" + 0.015*"Education" + 0.012*"Oprah"
topic #0 (0.033): 0.075*"Haiti" + 0.028*"UN" + 0.025*"cholera" + 0.016*"Gaza" + 0.014*"Mexico" + 0.014*"Cholera" + 0.014*"election" + 0.012*"Israeli" + 0.011*"death" + 0.011*"Asian"
topic #25 (0.033): 0.043*"lol" + 0.037*"today" + 0.036*"tweet" + 0.030*"people" + 0.023*"you" + 0.023*"give me" + 0.022*"time" + 0.022*"out" + 0.020*"twitter" + 0.019*"now"
topic #1 (0.033): 0.070*"TSA" + 0.045*"Obama" + 0.038*"GOP" + 0.022*"Senate" + 0.022*"ppl" + 0.016*"Republicans" + 0.016*"House" + 0.015*"Congress" + 0.015*"Palin" + 0.014*"vote"
topic #8 (0.033): 0.049*"Iran" + 0.041*"US" + 0.028*"Israel" + 0.024*"Afghanistan" + 0.019*"to win" + 0.014*"Egypt" + 0.014*"American" + 0.012*"Afghan" + 0.011*"Iraq" + 0.010*"Taliban"
topic diff=0.043812, rho=0.116346
PROGRESS: pass 1, at document #14000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.027*"game" + 0.026*"football" + 0.015*"show" + 0.015*"haha" + 0.015*"AP" + 0.015*"RETWEET" + 0.015*"net" + 0.014*"Kanye West" + 0.013*"team" + 0.012*"beautiful"
topic #17 (0.033): 0.042*"North Korea" + 0.041*"China" + 0.026*"Ireland" + 0.026*"South Korea" + 0.020*"party" + 0.019*"Japan" + 0.019*"Irish" + 0.016*"attack" + 0.015*"Korea" + 0.013*"artillery"
topic #14 (0.033): 0.050*"tcot" + 0.028*"hanging" + 0.016*"Obama" + 0.012*"Bush" + 0.011*"woman" + 0.010*"Alaska" + 0.008*"Police" + 0.008*"teaparty" + 0.008*"Ohio" + 0.008*"Sarah Palin"
topic #25 (0.033): 0.042*"lol" + 0.038*"today" + 0.035*"tweet" + 0.030*"people" + 0.023*"you" + 0.022*"out" + 0.022*"time" + 0.022*"give me" + 0.020*"twitter" + 0.019*"now"
topic #16 (0.033): 0.048*"Black Friday" + 0.026*"Sarah Palin" + 0.019*"HIV" + 0.017*"newspaper" + 0.015*"Here" + 0.015*"Canada" + 0.014*"DWTS" + 0.014*"gay" + 0.013*"chat" + 0.012*"to show"
topic diff=0.048556, rho=0.116346
PROGRESS: pass 1, at document #16000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.027*"News" + 0.020*"bit.ly" + 0.018*"NFL" + 0.017*"Texas" + 0.015*"Tom DeLay" + 0.015*"Report" + 0.014*"English" + 0.013*"Tory" + 0.013*"You" + 0.012*"Boston"
topic #29 (0.033): 0.191*"Thanksgiving" + 0.058*"Happy Thanksgiving" + 0.036*"LOL" + 0.032*"holiday" + 0.030*"family" + 0.028*"friends" + 0.024*"Turkey" + 0.021*"tonight" + 0.018*"thanksgiving" + 0.015*"Black Friday"
topic #22 (0.033): 0.054*"Blog" + 0.024*"week" + 0.023*"We" + 0.022*"re" + 0.019*"book" + 0.015*"basketball" + 0.014*"believe" + 0.014*"change" + 0.012*"WTF" + 0.012*"Post"
topic #16 (0.033): 0.048*"Black Friday" + 0.027*"Sarah Palin" + 0.021*"newspaper" + 0.019*"HIV" + 0.015*"gay" + 0.015*"Here" + 0.015*"Canada" + 0.014*"DWTS" + 0.013*"read" + 0.012*"chat"
topic #20 (0.033): 0.040*"live" + 0.035*"TV" + 0.030*"Watch" + 0.026*"Check it" + 0.025*"world" + 0.023*"Check it out" + 0.021*"Hope" + 0.020*"climate" + 0.020*"interview" + 0.019*"COP16"
topic diff=0.026630, rho=0.116346
PROGRESS: pass 1, at document #18000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.026*"News" + 0.019*"bit.ly" + 0.018*"Texas" + 0.017*"NFL" + 0.016*"Tom DeLay" + 0.014*"Report" + 0.014*"English" + 0.013*"Tory" + 0.013*"You" + 0.012*"Boston"
topic #1 (0.033): 0.075*"TSA" + 0.048*"Obama" + 0.041*"GOP" + 0.023*"Senate" + 0.018*"ppl" + 0.016*"Palin" + 0.016*"Republicans" + 0.016*"House" + 0.014*"Congress" + 0.014*"vote"
topic #18 (0.033): 0.089*"WikiLeaks" + 0.081*"Wikileaks" + 0.036*"wikileaks" + 0.023*"cablegate" + 0.022*"US" + 0.019*"Wiki Leaks" + 0.015*"Pakistan" + 0.014*"cables" + 0.014*"China" + 0.013*"NYT"
topic #21 (0.033): 0.034*"NATO" + 0.032*"CNN" + 0.026*"Chicago" + 0.024*"Pope" + 0.022*"cancer" + 0.021*"Retweet" + 0.017*"Friday" + 0.016*"Russia" + 0.016*"Venezuela" + 0.015*"condoms"
topic #11 (0.033): 0.056*"via" + 0.050*"iPad" + 0.045*"iPhone" + 0.043*"Google" + 0.039*"Apple" + 0.033*"Facebook" + 0.025*"Android" + 0.015*"app" + 0.015*"iTunes" + 0.015*"Christmas"
topic diff=0.043022, rho=0.116346
bound: at document #0
-23.064 per-word bound, 8770238.1 perplexity estimate based on a held-out corpus of 2000 documents with 20435 words
PROGRESS: pass 1, at document #20000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.025*"will" + 0.019*"work" + 0.013*"don" + 0.013*"MSNBC" + 0.013*"talk" + 0.013*"kids" + 0.012*"VIDEO" + 0.012*"do" + 0.011*"Americans" + 0.011*"we"
topic #11 (0.033): 0.056*"via" + 0.052*"iPad" + 0.045*"iPhone" + 0.043*"Google" + 0.039*"Apple" + 0.032*"Facebook" + 0.025*"Android" + 0.016*"iTunes" + 0.016*"Christmas" + 0.015*"app"
topic #27 (0.033): 0.072*"blog" + 0.035*"post" + 0.031*"iOS" + 0.025*"Back" + 0.020*"Amazon" + 0.017*"release" + 0.017*"DVD" + 0.015*"Oprah" + 0.014*"web" + 0.012*"PC"
topic #17 (0.033): 0.044*"North Korea" + 0.039*"China" + 0.027*"South Korea" + 0.025*"Ireland" + 0.018*"Irish" + 0.018*"Japan" + 0.017*"attack" + 0.017*"party" + 0.015*"artillery" + 0.013*"Korea"
topic #13 (0.033): 0.030*"AP" + 0.020*"DADT" + 0.012*"risk" + 0.011*"Breaking News" + 0.010*"Pentagon" + 0.010*"report" + 0.010*"add" + 0.009*"child" + 0.009*"founder" + 0.008*"law"
topic diff=0.049999, rho=0.116346
PROGRESS: pass 1, at document #22000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.075*"who" + 0.020*"Oh" + 0.020*"Yes" + 0.018*"Nice" + 0.017*"college" + 0.015*"me a" + 0.015*"Arizona" + 0.015*"fat" + 0.015*"a man" + 0.015*"media"
topic #9 (0.033): 0.025*"will" + 0.019*"work" + 0.014*"MSNBC" + 0.014*"talk" + 0.013*"don" + 0.013*"kids" + 0.013*"do" + 0.012*"VIDEO" + 0.012*"we" + 0.012*"Americans"
topic #20 (0.033): 0.039*"live" + 0.038*"TV" + 0.029*"Watch" + 0.027*"world" + 0.023*"Check it" + 0.021*"Check it out" + 0.020*"Hope" + 0.019*"climate" + 0.019*"interview" + 0.018*"families"
topic #25 (0.033): 0.049*"lol" + 0.043*"today" + 0.030*"tweet" + 0.029*"people" + 0.025*"out" + 0.023*"time" + 0.021*"you" + 0.018*"can" + 0.017*"twitter" + 0.017*"now"
topic #1 (0.033): 0.074*"TSA" + 0.049*"Obama" + 0.045*"GOP" + 0.024*"Senate" + 0.018*"ppl" + 0.017*"House" + 0.016*"Palin" + 0.016*"Republicans" + 0.015*"Congress" + 0.015*"vote"
topic diff=0.019166, rho=0.116346
PROGRESS: pass 1, at document #24000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.072*"para" + 0.068*"un" + 0.053*"Como" + 0.043*"como" + 0.026*"es" + 0.025*"en" + 0.024*"la" + 0.019*"min" + 0.016*"dos" + 0.016*"eso"
topic #19 (0.033): 0.042*"Today" + 0.039*"Leslie Nielsen" + 0.034*"Love" + 0.026*"Time" + 0.022*"Music" + 0.021*"Life" + 0.019*"nice" + 0.017*"need" + 0.016*"right now" + 0.015*"Actor"
topic #7 (0.033): 0.194*"Twitter" + 0.081*"Facebook" + 0.035*"NYC" + 0.017*"Tweet" + 0.014*"kids" + 0.010*"website" + 0.010*"Internet" + 0.010*"Netflix" + 0.010*"social media" + 0.009*"Comcast"
topic #3 (0.033): 0.051*"UK" + 0.032*"London" + 0.025*"police" + 0.019*"students" + 0.015*"protest" + 0.015*"British" + 0.014*"Police" + 0.013*"school" + 0.011*"photography" + 0.011*"demo 2010"
topic #0 (0.033): 0.079*"Haiti" + 0.027*"UN" + 0.024*"cholera" + 0.020*"election" + 0.015*"Mexico" + 0.013*"Cholera" + 0.012*"Gaza" + 0.011*"Israeli" + 0.011*"Asian" + 0.011*"death"
topic diff=0.015430, rho=0.116346
PROGRESS: pass 1, at document #26000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.071*"para" + 0.066*"un" + 0.050*"Como" + 0.043*"como" + 0.025*"es" + 0.025*"en" + 0.023*"la" + 0.018*"min" + 0.017*"dos" + 0.016*"hoy"
topic #13 (0.033): 0.029*"AP" + 0.018*"DADT" + 0.011*"risk" + 0.011*"report" + 0.011*"Breaking News" + 0.010*"friend" + 0.010*"Pentagon" + 0.009*"law" + 0.009*"child" + 0.009*"add"
topic #12 (0.033): 0.065*"God" + 0.036*"Who" + 0.019*"NPR" + 0.018*"al" + 0.016*"local" + 0.012*"left" + 0.012*"Who's" + 0.011*"help" + 0.011*"America" + 0.011*"Lanvin"
topic #16 (0.033): 0.053*"Black Friday" + 0.029*"Sarah Palin" + 0.019*"HIV" + 0.018*"newspaper" + 0.016*"Here" + 0.016*"Canada" + 0.015*"DWTS" + 0.014*"gay" + 0.013*"read" + 0.012*"to show"
topic #1 (0.033): 0.073*"TSA" + 0.047*"Obama" + 0.047*"GOP" + 0.023*"Senate" + 0.018*"ppl" + 0.017*"House" + 0.016*"Republicans" + 0.015*"Palin" + 0.015*"Congress" + 0.014*"vote"
topic diff=0.014103, rho=0.116346
PROGRESS: pass 1, at document #28000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.076*"Haiti" + 0.026*"UN" + 0.024*"cholera" + 0.019*"election" + 0.014*"Mexico" + 0.013*"Israeli" + 0.013*"earthquake" + 0.012*"Cholera" + 0.011*"death" + 0.011*"Gaza"
topic #9 (0.033): 0.026*"will" + 0.020*"work" + 0.013*"MSNBC" + 0.013*"kids" + 0.013*"don" + 0.013*"Americans" + 0.012*"talk" + 0.012*"VIDEO" + 0.012*"we" + 0.012*"do"
topic #16 (0.033): 0.055*"Black Friday" + 0.029*"Sarah Palin" + 0.019*"HIV" + 0.017*"newspaper" + 0.017*"Here" + 0.016*"Canada" + 0.014*"DWTS" + 0.014*"gay" + 0.013*"sales" + 0.012*"read"
topic #17 (0.033): 0.044*"China" + 0.042*"North Korea" + 0.030*"Ireland" + 0.024*"South Korea" + 0.020*"Irish" + 0.017*"Japan" + 0.015*"attack" + 0.014*"US" + 0.013*"Korea" + 0.013*"Reuters"
topic #28 (0.033): 0.110*"video" + 0.061*"Video" + 0.059*"YouTube" + 0.029*"Holiday" + 0.025*"Cyber Monday" + 0.021*"music" + 0.015*"Check" + 0.012*"Photography" + 0.011*"Live" + 0.009*"haha"
topic diff=0.020794, rho=0.116346
PROGRESS: pass 1, at document #30000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.047*"Today" + 0.039*"Leslie Nielsen" + 0.031*"Love" + 0.025*"Time" + 0.024*"Life" + 0.019*"Music" + 0.019*"nice" + 0.018*"right now" + 0.016*"need" + 0.015*"Actor"
topic #24 (0.033): 0.077*"who" + 0.023*"Arizona" + 0.020*"Nice" + 0.018*"Oh" + 0.017*"Yes" + 0.015*"me a" + 0.015*"right" + 0.014*"actor" + 0.014*"a man" + 0.013*"media"
topic #1 (0.033): 0.083*"TSA" + 0.046*"Obama" + 0.044*"GOP" + 0.022*"Senate" + 0.017*"ppl" + 0.016*"Republicans" + 0.016*"House" + 0.016*"Congress" + 0.014*"vote" + 0.014*"Palin"
topic #9 (0.033): 0.026*"will" + 0.019*"work" + 0.013*"don" + 0.013*"we" + 0.012*"do" + 0.012*"VIDEO" + 0.012*"kids" + 0.012*"talk" + 0.012*"MSNBC" + 0.012*"Americans"
topic #5 (0.033): 0.026*"student" + 0.021*"NBC" + 0.020*"high school" + 0.020*"season" + 0.017*"Wisconsin" + 0.015*"hostage" + 0.013*"fb" + 0.013*"Willie Nelson" + 0.013*"Good" + 0.012*"hostages"
topic diff=0.048726, rho=0.116346
PROGRESS: pass 1, at document #32000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.034*"U.S" + 0.020*"President" + 0.020*"Obama" + 0.018*"Korea" + 0.016*"White House" + 0.015*"Seoul" + 0.014*"military" + 0.013*"S. Korea" + 0.011*"Sen" + 0.011*"North Korea"
topic #19 (0.033): 0.045*"Today" + 0.040*"Leslie Nielsen" + 0.032*"Love" + 0.024*"Time" + 0.024*"Life" + 0.021*"nice" + 0.019*"Music" + 0.019*"right now" + 0.015*"white" + 0.015*"need"
topic #3 (0.033): 0.048*"UK" + 0.034*"London" + 0.030*"police" + 0.018*"students" + 0.016*"protest" + 0.016*"photography" + 0.015*"Police" + 0.015*"British" + 0.014*"demo 2010" + 0.014*"Read"
topic #1 (0.033): 0.084*"TSA" + 0.045*"Obama" + 0.042*"GOP" + 0.022*"Senate" + 0.018*"ppl" + 0.016*"Republicans" + 0.016*"Congress" + 0.016*"House" + 0.014*"Palin" + 0.014*"vote"
topic #24 (0.033): 0.082*"who" + 0.022*"Arizona" + 0.019*"Nice" + 0.019*"Oh" + 0.017*"Yes" + 0.015*"me a" + 0.014*"actor" + 0.014*"right" + 0.014*"a man" + 0.013*"media"
topic diff=0.029416, rho=0.116346
PROGRESS: pass 1, at document #34000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.041*"AP" + 0.016*"DADT" + 0.010*"risk" + 0.010*"report" + 0.010*"law" + 0.009*"child" + 0.009*"friend" + 0.009*"Breaking News" + 0.008*"add" + 0.008*"Pentagon"
topic #17 (0.033): 0.045*"North Korea" + 0.042*"China" + 0.026*"Ireland" + 0.026*"South Korea" + 0.017*"attack" + 0.016*"Irish" + 0.016*"Japan" + 0.015*"US" + 0.015*"artillery" + 0.014*"Reuters"
topic #8 (0.033): 0.047*"Iran" + 0.043*"US" + 0.030*"Israel" + 0.024*"Afghanistan" + 0.023*"to win" + 0.016*"American" + 0.012*"Egypt" + 0.011*"Iraq" + 0.009*"Afghan" + 0.009*"leaders"
topic #12 (0.033): 0.075*"God" + 0.038*"Who" + 0.019*"NPR" + 0.014*"local" + 0.014*"Who's" + 0.014*"al" + 0.013*"left" + 0.012*"guy" + 0.012*"America" + 0.011*"DREAM Act"
topic #0 (0.033): 0.069*"Haiti" + 0.024*"UN" + 0.021*"cholera" + 0.018*"election" + 0.015*"Mexico" + 0.012*"Asian" + 0.012*"gold" + 0.011*"earthquake" + 0.011*"Gaza" + 0.011*"Israeli"
topic diff=0.033503, rho=0.116346
PROGRESS: pass 1, at document #36000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.070*"God" + 0.038*"Who" + 0.019*"Celebrity" + 0.018*"NPR" + 0.015*"Who's" + 0.014*"local" + 0.012*"al" + 0.012*"left" + 0.012*"guy" + 0.011*"America"
topic #29 (0.033): 0.198*"Thanksgiving" + 0.054*"Happy Thanksgiving" + 0.037*"LOL" + 0.031*"holiday" + 0.030*"family" + 0.028*"thanksgiving" + 0.026*"Turkey" + 0.025*"friends" + 0.024*"tonight" + 0.015*"Harry Potter"
topic #7 (0.033): 0.183*"Twitter" + 0.084*"Facebook" + 0.030*"NYC" + 0.019*"Tweet" + 0.016*"photos" + 0.012*"People" + 0.012*"Social Media" + 0.012*"kids" + 0.011*"Chinese" + 0.011*"Internet"
topic #24 (0.033): 0.084*"who" + 0.021*"Oh" + 0.021*"Nice" + 0.020*"Arizona" + 0.017*"Yes" + 0.017*"me a" + 0.015*"I'm going" + 0.015*"actor" + 0.014*"right" + 0.013*"a man"
topic #28 (0.033): 0.106*"video" + 0.063*"Video" + 0.054*"YouTube" + 0.031*"Holiday" + 0.025*"Cyber Monday" + 0.020*"music" + 0.015*"Photography" + 0.013*"Check" + 0.012*"Live" + 0.010*"Photos"
topic diff=0.014111, rho=0.116346
PROGRESS: pass 1, at document #38000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #26 (0.033): 0.044*"India" + 0.032*"news" + 0.020*"New Zealand" + 0.014*"mine" + 0.014*"dead" + 0.013*"killed" + 0.013*"business" + 0.011*"NASA" + 0.011*"energy" + 0.010*"Australia"
topic #29 (0.033): 0.200*"Thanksgiving" + 0.055*"Happy Thanksgiving" + 0.034*"LOL" + 0.032*"holiday" + 0.030*"family" + 0.027*"thanksgiving" + 0.026*"Turkey" + 0.025*"friends" + 0.023*"tonight" + 0.015*"Harry Potter"
topic #17 (0.033): 0.045*"China" + 0.044*"North Korea" + 0.030*"Ireland" + 0.025*"South Korea" + 0.021*"Irish" + 0.016*"attack" + 0.016*"US" + 0.016*"Japan" + 0.014*"Europe" + 0.014*"Reuters"
topic #9 (0.033): 0.026*"will" + 0.019*"work" + 0.014*"don" + 0.013*"do" + 0.012*"we" + 0.012*"VIDEO" + 0.012*"talk" + 0.011*"MSNBC" + 0.011*"Americans" + 0.011*"kids"
topic #11 (0.033): 0.063*"via" + 0.046*"iPhone" + 0.045*"iPad" + 0.038*"Google" + 0.037*"Apple" + 0.029*"Facebook" + 0.021*"app" + 0.019*"Christmas" + 0.019*"Social Media" + 0.018*"Android"
topic diff=0.040819, rho=0.116346
bound: at document #0
-23.177 per-word bound, 9486395.1 perplexity estimate based on a held-out corpus of 2000 documents with 26137 words
PROGRESS: pass 1, at document #40000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.014*"U.S" + 0.013*"World" + 0.013*"Green" + 0.011*"News" + 0.010*"Will" + 0.010*"Study" + 0.010*"China" + 0.008*"France" + 0.008*"Earth" + 0.008*"Health"
topic #27 (0.033): 0.075*"blog" + 0.050*"Blogger" + 0.032*"post" + 0.020*"Amazon" + 0.019*"iOS" + 0.017*"DVD" + 0.016*"cc" + 0.016*"release" + 0.015*"web" + 0.014*"Oprah"
topic #23 (0.033): 0.034*"football" + 0.025*"game" + 0.020*"show" + 0.016*"team" + 0.014*"Kanye West" + 0.013*"haha" + 0.012*"beautiful" + 0.012*"turkeys" + 0.011*"net" + 0.011*"Disney"
topic #5 (0.033): 0.026*"student" + 0.021*"season" + 0.020*"high school" + 0.020*"NBC" + 0.018*"Wisconsin" + 0.015*"hostage" + 0.013*"smile" + 0.012*"Good" + 0.012*"fb" + 0.012*"Willie Nelson"
topic #7 (0.033): 0.174*"Twitter" + 0.086*"Facebook" + 0.032*"NYC" + 0.020*"Tweet" + 0.015*"photos" + 0.012*"Chinese" + 0.012*"Internet" + 0.011*"People" + 0.011*"social media" + 0.011*"kids"
topic diff=0.028478, rho=0.116346
PROGRESS: pass 1, at document #42000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.055*"Black Friday" + 0.026*"Sarah Palin" + 0.020*"Here" + 0.019*"HIV" + 0.016*"newspaper" + 0.015*"gay" + 0.015*"Canada" + 0.014*"read" + 0.014*"DWTS" + 0.011*"allies"
topic #21 (0.033): 0.051*"CNN" + 0.031*"Chicago" + 0.028*"NATO" + 0.023*"cancer" + 0.022*"Pope" + 0.020*"Friday" + 0.016*"up" + 0.015*"Photo" + 0.015*"Retweet" + 0.015*"Russia"
topic #7 (0.033): 0.175*"Twitter" + 0.084*"Facebook" + 0.033*"NYC" + 0.020*"Tweet" + 0.014*"photos" + 0.012*"Chinese" + 0.012*"People" + 0.011*"Internet" + 0.011*"social media" + 0.011*"Social Media"
topic #13 (0.033): 0.037*"AP" + 0.014*"DADT" + 0.011*"risk" + 0.011*"report" + 0.010*"Art" + 0.010*"child" + 0.009*"Pentagon" + 0.009*"Breaking News" + 0.009*"add" + 0.009*"friend"
topic #22 (0.033): 0.061*"Blog" + 0.023*"We" + 0.021*"re" + 0.020*"book" + 0.016*"week" + 0.014*"one" + 0.013*"basketball" + 0.013*"food" + 0.013*"aren" + 0.012*"Post"
topic diff=0.032214, rho=0.116346
PROGRESS: pass 1, at document #44000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.195*"Thanksgiving" + 0.058*"LOL" + 0.055*"Happy Thanksgiving" + 0.031*"holiday" + 0.030*"family" + 0.028*"friends" + 0.025*"Turkey" + 0.024*"tonight" + 0.024*"thanksgiving" + 0.014*"Harry Potter"
topic #4 (0.033): 0.033*"U.S" + 0.023*"President" + 0.021*"Obama" + 0.018*"White House" + 0.015*"Korea" + 0.014*"military" + 0.014*"President Obama" + 0.011*"S. Korea" + 0.011*"Sen" + 0.010*"Seoul"
topic #5 (0.033): 0.025*"student" + 0.022*"season" + 0.019*"NBC" + 0.019*"high school" + 0.018*"Wisconsin" + 0.015*"hostage" + 0.012*"Good" + 0.011*"record" + 0.011*"smile" + 0.011*"Los Angeles"
topic #16 (0.033): 0.052*"Black Friday" + 0.026*"Sarah Palin" + 0.020*"HIV" + 0.019*"Here" + 0.015*"DWTS" + 0.015*"Canada" + 0.015*"read" + 0.014*"newspaper" + 0.014*"gay" + 0.011*"Bristol"
topic #2 (0.033): 0.040*"News" + 0.026*"LMAO" + 0.020*"NFL" + 0.018*"Texas" + 0.015*"English" + 0.013*"Tom DeLay" + 0.012*"You" + 0.012*"Report" + 0.011*"songs" + 0.011*"bit.ly"
topic diff=0.035941, rho=0.116346
PROGRESS: pass 1, at document #46000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.095*"Haiti" + 0.027*"cholera" + 0.026*"Atlantic" + 0.022*"UN" + 0.020*"Cholera" + 0.017*"election" + 0.016*"Mexico" + 0.014*"Israeli" + 0.013*"Gaza" + 0.011*"tweeted"
topic #15 (0.033): 0.031*"snow" + 0.028*"USA" + 0.021*"travel" + 0.017*"here" + 0.015*"Thailand" + 0.013*"PM" + 0.012*"women" + 0.012*"England" + 0.012*"film" + 0.012*"sex"
topic #28 (0.033): 0.147*"video" + 0.106*"YouTube" + 0.050*"Video" + 0.027*"Holiday" + 0.023*"Cyber Monday" + 0.018*"music" + 0.015*"Live" + 0.013*"Check" + 0.011*"Photography" + 0.010*"Tonight"
topic #22 (0.033): 0.059*"Blog" + 0.021*"We" + 0.021*"re" + 0.019*"book" + 0.015*"week" + 0.014*"one" + 0.013*"basketball" + 0.013*"food" + 0.012*"Post" + 0.012*"WTF"
topic #27 (0.033): 0.073*"blog" + 0.036*"Blogger" + 0.031*"post" + 0.021*"Amazon" + 0.021*"Win" + 0.020*"iOS" + 0.018*"DVD" + 0.015*"release" + 0.014*"Oprah" + 0.012*"web"
topic diff=0.113423, rho=0.116346
PROGRESS: pass 1, at document #48000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.099*"WikiLeaks" + 0.084*"Wikileaks" + 0.033*"NYT" + 0.030*"wikileaks" + 0.026*"US" + 0.024*"cablegate" + 0.018*"Wiki Leaks" + 0.016*"Opinion" + 0.015*"Guardian" + 0.014*"documents"
topic #25 (0.033): 0.064*"lol" + 0.045*"today" + 0.027*"people" + 0.025*"out" + 0.023*"you" + 0.022*"time" + 0.019*"tweet" + 0.018*"can" + 0.017*"us" + 0.017*"who"
topic #24 (0.033): 0.081*"who" + 0.044*"twit" + 0.022*"Arizona" + 0.019*"Nice" + 0.019*"Oh" + 0.019*"Yes" + 0.019*"me a" + 0.016*"a man" + 0.015*"I'm going" + 0.015*"fat"
topic #3 (0.033): 0.058*"UK" + 0.039*"London" + 0.033*"BBC" + 0.027*"police" + 0.017*"students" + 0.015*"British" + 0.015*"demo 2010" + 0.014*"Police" + 0.013*"protest" + 0.011*"Britain"
topic #8 (0.033): 0.086*"BBC" + 0.057*"Iran" + 0.042*"US" + 0.028*"Israel" + 0.025*"to win" + 0.021*"Afghanistan" + 0.014*"American" + 0.012*"Forest" + 0.010*"Egypt" + 0.009*"Iraq"
topic diff=0.022476, rho=0.116346
PROGRESS: pass 1, at document #50000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.081*"who" + 0.039*"twit" + 0.021*"Arizona" + 0.020*"me a" + 0.020*"Oh" + 0.019*"Nice" + 0.018*"Yes" + 0.017*"Bears" + 0.016*"a man" + 0.015*"right"
topic #3 (0.033): 0.059*"UK" + 0.038*"London" + 0.031*"BBC" + 0.027*"police" + 0.016*"students" + 0.016*"British" + 0.015*"demo 2010" + 0.014*"Police" + 0.012*"protest" + 0.010*"Britain"
topic #14 (0.033): 0.071*"tcot" + 0.022*"Bush" + 0.019*"sgp" + 0.018*"Obama" + 0.013*"teaparty" + 0.009*"woman" + 0.009*"GM" + 0.009*"Alaska" + 0.008*"Ohio" + 0.008*"FBI"
topic #26 (0.033): 0.036*"India" + 0.028*"news" + 0.022*"New Zealand" + 0.014*"mine" + 0.013*"dead" + 0.013*"business" + 0.012*"killed" + 0.011*"energy" + 0.010*"NZ" + 0.010*"Africa"
topic #29 (0.033): 0.207*"Thanksgiving" + 0.059*"Happy Thanksgiving" + 0.058*"LOL" + 0.030*"family" + 0.029*"holiday" + 0.027*"Turkey" + 0.027*"friends" + 0.025*"tonight" + 0.024*"thanksgiving" + 0.013*"Black Friday"
topic diff=0.028367, rho=0.116346
PROGRESS: pass 1, at document #52000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.057*"via" + 0.048*"iPad" + 0.042*"iPhone" + 0.037*"Apple" + 0.036*"Google" + 0.025*"Facebook" + 0.020*"iTunes" + 0.020*"Christmas" + 0.018*"Beatles" + 0.017*"app"
topic #4 (0.033): 0.034*"U.S" + 0.023*"President" + 0.023*"Obama" + 0.018*"White House" + 0.015*"Korea" + 0.014*"TIME" + 0.014*"President Obama" + 0.011*"military" + 0.011*"Sen" + 0.010*"Clinton"
topic #2 (0.033): 0.030*"News" + 0.023*"Texas" + 0.023*"NFL" + 0.021*"LMAO" + 0.016*"Tom DeLay" + 0.014*"English" + 0.013*"songs" + 0.012*"You" + 0.011*"Report" + 0.010*"bit.ly"
topic #27 (0.033): 0.069*"blog" + 0.030*"post" + 0.028*"Blogger" + 0.023*"Amazon" + 0.021*"iOS" + 0.019*"DVD" + 0.017*"Win" + 0.016*"release" + 0.014*"Oprah" + 0.013*"web"
topic #24 (0.033): 0.080*"who" + 0.035*"twit" + 0.022*"Oh" + 0.021*"me a" + 0.020*"Arizona" + 0.019*"Nice" + 0.017*"Yes" + 0.017*"a man" + 0.015*"Bears" + 0.015*"right"
topic diff=0.043076, rho=0.116346
PROGRESS: pass 1, at document #54000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.015*"U.S" + 0.013*"NYT" + 0.012*"World" + 0.011*"Will" + 0.011*"Study" + 0.010*"China" + 0.009*"News" + 0.009*"Tea Party" + 0.009*"Women" + 0.009*"Green"
topic #10 (0.033): 0.074*"para" + 0.053*"un" + 0.033*"como" + 0.026*"dos" + 0.024*"en" + 0.022*"Como" + 0.021*"la" + 0.017*"ll" + 0.017*"es" + 0.017*"pour"
topic #13 (0.033): 0.036*"AP" + 0.024*"DADT" + 0.012*"Pentagon" + 0.010*"risk" + 0.010*"child" + 0.010*"report" + 0.009*"friend" + 0.009*"debate" + 0.009*"Portland" + 0.009*"weather"
topic #7 (0.033): 0.149*"Twitter" + 0.076*"Facebook" + 0.034*"NYC" + 0.024*"Tweet" + 0.017*"Lady Gaga" + 0.013*"photos" + 0.012*"People" + 0.011*"Chinese" + 0.011*"Internet" + 0.009*"website"
topic #20 (0.033): 0.041*"TV" + 0.039*"live" + 0.028*"world" + 0.026*"Watch" + 0.026*"interview" + 0.021*"Hope" + 0.020*"families" + 0.019*"Check it" + 0.019*"climate" + 0.017*"Radio"
topic diff=0.018400, rho=0.116346
PROGRESS: pass 1, at document #56000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.040*"live" + 0.039*"TV" + 0.028*"world" + 0.025*"Watch" + 0.025*"interview" + 0.020*"Hope" + 0.020*"climate" + 0.020*"families" + 0.019*"Check it" + 0.017*"Radio"
topic #28 (0.033): 0.198*"YouTube" + 0.180*"video" + 0.046*"Video" + 0.022*"Holiday" + 0.020*"music" + 0.018*"Cyber Monday" + 0.013*"Live" + 0.010*"Check" + 0.010*"Tonight" + 0.007*"Photography"
topic #17 (0.033): 0.041*"North Korea" + 0.040*"China" + 0.035*"Ireland" + 0.025*"South Korea" + 0.020*"Irish" + 0.016*"Japan" + 0.016*"attack" + 0.015*"US" + 0.014*"Korea" + 0.014*"artillery"
topic #9 (0.033): 0.026*"will" + 0.019*"work" + 0.016*"do" + 0.015*"em" + 0.014*"don" + 0.013*"MSNBC" + 0.013*"talk" + 0.013*"play" + 0.012*"Americans" + 0.012*"we"
topic #3 (0.033): 0.058*"UK" + 0.037*"London" + 0.028*"police" + 0.025*"BBC" + 0.018*"students" + 0.016*"British" + 0.016*"demo 2010" + 0.013*"Police" + 0.013*"protest" + 0.011*"Britain"
topic diff=0.032269, rho=0.116346
PROGRESS: pass 1, at document #58000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.061*"via" + 0.046*"iPad" + 0.044*"iPhone" + 0.034*"Google" + 0.034*"Apple" + 0.023*"Facebook" + 0.020*"Christmas" + 0.019*"iTunes" + 0.017*"Beatles" + 0.017*"app"
topic #25 (0.033): 0.071*"lol" + 0.040*"today" + 0.027*"people" + 0.023*"you" + 0.022*"time" + 0.022*"ur" + 0.021*"out" + 0.019*"can" + 0.017*"love" + 0.017*"tweet"
topic #15 (0.033): 0.030*"snow" + 0.023*"USA" + 0.023*"here" + 0.017*"travel" + 0.013*"film" + 0.013*"Paris" + 0.012*"sex" + 0.012*"women" + 0.011*"England" + 0.009*"Thailand"
topic #14 (0.033): 0.064*"tcot" + 0.018*"Bush" + 0.016*"Obama" + 0.016*"sgp" + 0.012*"teaparty" + 0.010*"woman" + 0.009*"Alaska" + 0.009*"Ohio" + 0.008*"job" + 0.008*"GM"
topic #1 (0.033): 0.071*"TSA" + 0.055*"GOP" + 0.049*"Obama" + 0.026*"ppl" + 0.021*"Senate" + 0.017*"Palin" + 0.016*"vote" + 0.015*"Republicans" + 0.014*"House" + 0.013*"Congress"
topic diff=0.020970, rho=0.116346
bound: at document #0
-24.506 per-word bound, 23829619.6 perplexity estimate based on a held-out corpus of 2000 documents with 40495 words
PROGRESS: pass 1, at document #60000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.036*"haha" + 0.035*"football" + 0.032*"game" + 0.029*"show" + 0.015*"OK" + 0.015*"Kanye West" + 0.014*"hair" + 0.014*"team" + 0.013*"net" + 0.013*"beautiful"
topic #9 (0.033): 0.026*"will" + 0.020*"work" + 0.014*"do" + 0.013*"em" + 0.013*"don" + 0.012*"play" + 0.012*"Americans" + 0.012*"we" + 0.011*"MSNBC" + 0.011*"talk"
topic #2 (0.033): 0.030*"News" + 0.023*"NFL" + 0.022*"LMAO" + 0.020*"Texas" + 0.017*"Tom DeLay" + 0.014*"songs" + 0.013*"Report" + 0.013*"Boston" + 0.012*"bit.ly" + 0.012*"EUA"
topic #27 (0.033): 0.073*"blog" + 0.033*"post" + 0.023*"Amazon" + 0.021*"DVD" + 0.019*"Blogger" + 0.018*"iOS" + 0.016*"release" + 0.015*"Win" + 0.015*"Oprah" + 0.014*"web"
topic #11 (0.033): 0.056*"via" + 0.047*"iPad" + 0.042*"iPhone" + 0.034*"Apple" + 0.033*"Google" + 0.024*"Facebook" + 0.021*"Beatles" + 0.020*"iTunes" + 0.019*"Christmas" + 0.017*"app"
topic diff=0.023584, rho=0.116346
PROGRESS: pass 1, at document #62000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.049*"Blog" + 0.024*"We" + 0.021*"re" + 0.018*"RTs" + 0.016*"book" + 0.015*"food" + 0.015*"week" + 0.014*"WTF" + 0.013*"one" + 0.013*"basketball"
topic #28 (0.033): 0.165*"video" + 0.159*"YouTube" + 0.046*"Video" + 0.026*"Holiday" + 0.021*"Cyber Monday" + 0.020*"music" + 0.014*"Live" + 0.011*"Check" + 0.010*"Tonight" + 0.008*"Photos"
topic #12 (0.033): 0.126*"God" + 0.043*"Who" + 0.019*"GOD" + 0.017*"NPR" + 0.015*"Who's" + 0.014*"left" + 0.013*"help" + 0.012*"America" + 0.011*"guy" + 0.011*"local"
topic #10 (0.033): 0.089*"para" + 0.045*"un" + 0.044*"como" + 0.037*"dos" + 0.023*"en" + 0.019*"pour" + 0.019*"la" + 0.017*"ll" + 0.017*"Como" + 0.016*"se"
topic #16 (0.033): 0.048*"Black Friday" + 0.028*"Sarah Palin" + 0.022*"HIV" + 0.020*"Canada" + 0.018*"gay" + 0.018*"DWTS" + 0.015*"Here" + 0.012*"Bristol" + 0.012*"If" + 0.011*"Dec"
topic diff=0.035358, rho=0.116346
PROGRESS: pass 1, at document #64000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.042*"CNN" + 0.039*"FARC" + 0.039*"Chicago" + 0.025*"Retweet" + 0.024*"NATO" + 0.023*"cancer" + 0.020*"Pope" + 0.020*"Friday" + 0.015*"Toronto" + 0.015*"up"
topic #5 (0.033): 0.024*"season" + 0.021*"student" + 0.017*"NBC" + 0.017*"high school" + 0.015*"Good" + 0.014*"Wisconsin" + 0.013*"smile" + 0.013*"record" + 0.013*"Los Angeles" + 0.013*"quote"
topic #6 (0.033): 0.016*"U.S" + 0.012*"World" + 0.011*"China" + 0.010*"Will" + 0.010*"Study" + 0.010*"Green" + 0.010*"News" + 0.010*"NYT" + 0.009*"Women" + 0.008*"BP"
topic #3 (0.033): 0.063*"photography" + 0.052*"UK" + 0.038*"London" + 0.026*"police" + 0.017*"BBC" + 0.017*"students" + 0.014*"British" + 0.013*"Police" + 0.013*"demo 2010" + 0.012*"protest"
topic #26 (0.033): 0.036*"India" + 0.030*"news" + 0.019*"New Zealand" + 0.015*"business" + 0.015*"dead" + 0.014*"mine" + 0.013*"energy" + 0.012*"killed" + 0.011*"Cambodia" + 0.010*"Australia"
topic diff=0.029163, rho=0.116346
PROGRESS: pass 1, at document #66000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.078*"blog" + 0.031*"post" + 0.022*"Amazon" + 0.021*"DVD" + 0.020*"iOS" + 0.016*"release" + 0.015*"Blogger" + 0.015*"web" + 0.014*"Oprah" + 0.012*"Win"
topic #4 (0.033): 0.034*"U.S" + 0.022*"President" + 0.021*"Obama" + 0.018*"S. Korea" + 0.015*"White House" + 0.015*"military" + 0.015*"Korea" + 0.012*"President Obama" + 0.012*"Cancun" + 0.012*"N. Korea"
topic #8 (0.033): 0.058*"Iran" + 0.043*"BBC" + 0.039*"US" + 0.026*"to win" + 0.025*"Israel" + 0.023*"Afghanistan" + 0.016*"Brasil" + 0.015*"American" + 0.010*"Egypt" + 0.009*"Afghan"
topic #22 (0.033): 0.040*"Blog" + 0.027*"see" + 0.025*"artist" + 0.023*"birth" + 0.022*"We" + 0.021*"heroes" + 0.020*"re" + 0.014*"book" + 0.014*"RTs" + 0.014*"food"
topic #7 (0.033): 0.139*"Twitter" + 0.081*"Facebook" + 0.029*"NYC" + 0.024*"Tweet" + 0.021*"website" + 0.014*"Chinese" + 0.014*"photos" + 0.013*"People" + 0.012*"Lady Gaga" + 0.011*"Internet"
topic diff=0.032817, rho=0.116346
PROGRESS: pass 1, at document #68000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.048*"haha" + 0.033*"football" + 0.033*"game" + 0.029*"show" + 0.015*"Kanye West" + 0.014*"beautiful" + 0.013*"hair" + 0.013*"team" + 0.012*"New Orleans" + 0.011*"NBA"
topic #4 (0.033): 0.033*"U.S" + 0.023*"President" + 0.021*"Obama" + 0.019*"S. Korea" + 0.015*"White House" + 0.015*"military" + 0.014*"Korea" + 0.014*"President Obama" + 0.012*"Cancun" + 0.011*"N. Korea"
topic #25 (0.033): 0.057*"lol" + 0.040*"today" + 0.029*"people" + 0.026*"you" + 0.024*"ur" + 0.023*"time" + 0.022*"can" + 0.022*"out" + 0.019*"love" + 0.018*"who"
topic #15 (0.033): 0.025*"snow" + 0.021*"here" + 0.021*"travel" + 0.020*"USA" + 0.014*"sex" + 0.014*"Paris" + 0.013*"film" + 0.013*"women" + 0.011*"England" + 0.011*"baby"
topic #21 (0.033): 0.044*"CNN" + 0.037*"Chicago" + 0.031*"FARC" + 0.030*"cancer" + 0.028*"NATO" + 0.024*"Retweet" + 0.021*"Friday" + 0.019*"Pope" + 0.016*"up" + 0.014*"Toronto"
topic diff=0.018404, rho=0.116346
PROGRESS: pass 1, at document #70000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.039*"live" + 0.036*"TV" + 0.029*"world" + 0.027*"Watch" + 0.026*"Radio" + 0.024*"interview" + 0.021*"climate" + 0.021*"climate change" + 0.019*"Hope" + 0.017*"Climate Change"
topic #8 (0.033): 0.057*"Iran" + 0.045*"US" + 0.036*"BBC" + 0.026*"Israel" + 0.024*"to win" + 0.023*"Afghanistan" + 0.016*"American" + 0.015*"Brasil" + 0.010*"Egypt" + 0.010*"Afghan"
topic #24 (0.033): 0.089*"who" + 0.024*"me a" + 0.021*"a man" + 0.019*"I'm going" + 0.018*"Arizona" + 0.017*"Oh" + 0.017*"Nice" + 0.016*"Yes" + 0.016*"right" + 0.016*"always"
topic #14 (0.033): 0.070*"tcot" + 0.016*"Bush" + 0.016*"Obama" + 0.012*"teaparty" + 0.012*"woman" + 0.011*"sgp" + 0.011*"GM" + 0.009*"Alaska" + 0.008*"arrested" + 0.008*"single"
topic #11 (0.033): 0.063*"via" + 0.039*"iPad" + 0.034*"iPhone" + 0.032*"Google" + 0.031*"Apple" + 0.025*"Facebook" + 0.025*"iTunes" + 0.021*"Christmas" + 0.017*"app" + 0.016*"Beatles"
topic diff=0.020853, rho=0.116346
PROGRESS: pass 1, at document #72000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.036*"AP" + 0.020*"DADT" + 0.013*"risk" + 0.013*"child" + 0.012*"friend" + 0.011*"study" + 0.010*"weather" + 0.010*"Pentagon" + 0.009*"report" + 0.008*"network"
topic #23 (0.033): 0.048*"haha" + 0.033*"football" + 0.032*"game" + 0.030*"show" + 0.017*"beautiful" + 0.015*"Kanye West" + 0.014*"hair" + 0.014*"thoughts" + 0.013*"NBA" + 0.013*"team"
topic #26 (0.033): 0.040*"India" + 0.030*"news" + 0.025*"business" + 0.019*"New Zealand" + 0.015*"dead" + 0.015*"mine" + 0.014*"energy" + 0.011*"killed" + 0.010*"Australia" + 0.009*"Africa"
topic #24 (0.033): 0.088*"who" + 0.024*"me a" + 0.022*"a man" + 0.020*"I'm going" + 0.019*"Nice" + 0.018*"Yes" + 0.018*"Oh" + 0.017*"right" + 0.017*"always" + 0.016*"man"
topic #9 (0.033): 0.028*"will" + 0.019*"work" + 0.016*"do" + 0.014*"em" + 0.014*"MSNBC" + 0.013*"don" + 0.012*"mind" + 0.012*"play" + 0.012*"something" + 0.012*"VIDEO"
topic diff=0.015193, rho=0.116346
PROGRESS: pass 1, at document #74000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.023*"snow" + 0.022*"here" + 0.021*"USA" + 0.018*"travel" + 0.017*"England" + 0.014*"film" + 0.014*"women" + 0.013*"sex" + 0.011*"Paris" + 0.010*"hit"
topic #4 (0.033): 0.032*"U.S" + 0.022*"President" + 0.020*"Obama" + 0.016*"S. Korea" + 0.015*"military" + 0.014*"Korea" + 0.013*"White House" + 0.013*"President Obama" + 0.011*"Cancun" + 0.011*"Sen"
topic #2 (0.033): 0.044*"News" + 0.025*"son" + 0.021*"NFL" + 0.020*"Texas" + 0.018*"LMAO" + 0.014*"Houston" + 0.013*"Boston" + 0.013*"Tom DeLay" + 0.012*"Las Vegas" + 0.011*"Report"
topic #0 (0.033): 0.097*"Haiti" + 0.031*"cholera" + 0.023*"UN" + 0.018*"Cholera" + 0.018*"Mexico" + 0.014*"election" + 0.013*"death" + 0.012*"tweeted" + 0.012*"Asian" + 0.011*"Elections"
topic #29 (0.033): 0.194*"Thanksgiving" + 0.066*"Happy Thanksgiving" + 0.047*"LOL" + 0.038*"thanksgiving" + 0.033*"family" + 0.028*"friends" + 0.026*"holiday" + 0.025*"Turkey" + 0.025*"tonight" + 0.012*"Black Friday"
topic diff=0.033642, rho=0.116346
PROGRESS: pass 1, at document #76000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.054*"Iran" + 0.043*"US" + 0.032*"Israel" + 0.030*"to win" + 0.028*"BBC" + 0.021*"Afghanistan" + 0.016*"American" + 0.015*"Brasil" + 0.012*"Egypt" + 0.011*"Afghan"
topic #20 (0.033): 0.039*"live" + 0.035*"TV" + 0.031*"Watch" + 0.031*"world" + 0.027*"Hope" + 0.024*"interview" + 0.021*"Radio" + 0.020*"climate" + 0.019*"homeless" + 0.018*"climate change"
topic #7 (0.033): 0.138*"Twitter" + 0.081*"Facebook" + 0.027*"Tweet" + 0.027*"NYC" + 0.018*"website" + 0.014*"People" + 0.013*"photos" + 0.013*"Chinese" + 0.011*"Internet" + 0.009*"Lady Gaga"
topic #14 (0.033): 0.065*"tcot" + 0.016*"Obama" + 0.014*"Bush" + 0.012*"woman" + 0.011*"teaparty" + 0.010*"GM" + 0.009*"Alaska" + 0.008*"sgp" + 0.008*"arrested" + 0.008*"job"
topic #29 (0.033): 0.196*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.044*"LOL" + 0.038*"thanksgiving" + 0.033*"family" + 0.028*"friends" + 0.026*"holiday" + 0.025*"tonight" + 0.025*"Turkey" + 0.012*"movie"
topic diff=0.029227, rho=0.116346
PROGRESS: pass 1, at document #78000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.096*"para" + 0.055*"como" + 0.052*"un" + 0.036*"dos" + 0.024*"la" + 0.024*"en" + 0.022*"es" + 0.018*"se" + 0.018*"este" + 0.018*"si"
topic #17 (0.033): 0.052*"China" + 0.044*"North Korea" + 0.028*"South Korea" + 0.022*"Ireland" + 0.018*"Japan" + 0.017*"US" + 0.017*"attack" + 0.017*"Korea" + 0.014*"Europe" + 0.013*"Reuters"
topic #21 (0.033): 0.039*"CNN" + 0.033*"NATO" + 0.032*"Chicago" + 0.030*"cancer" + 0.023*"Retweet" + 0.021*"Friday" + 0.020*"Pope" + 0.019*"FARC" + 0.016*"up" + 0.015*"Photo"
topic #29 (0.033): 0.196*"Thanksgiving" + 0.069*"Happy Thanksgiving" + 0.042*"LOL" + 0.037*"thanksgiving" + 0.033*"family" + 0.029*"friends" + 0.027*"holiday" + 0.025*"tonight" + 0.025*"Turkey" + 0.012*"Black Friday"
topic #19 (0.033): 0.046*"Today" + 0.045*"Leslie Nielsen" + 0.039*"Love" + 0.023*"Time" + 0.023*"nice" + 0.021*"Life" + 0.021*"right now" + 0.020*"white" + 0.017*"Music" + 0.015*"Actor"
topic diff=0.029877, rho=0.116346
bound: at document #0
-22.647 per-word bound, 6567498.8 perplexity estimate based on a held-out corpus of 2000 documents with 49203 words
PROGRESS: pass 1, at document #80000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.047*"UK" + 0.035*"London" + 0.032*"photography" + 0.024*"police" + 0.018*"Colombia" + 0.017*"British" + 0.016*"students" + 0.014*"protest" + 0.013*"Event" + 0.013*"Police"
topic #28 (0.033): 0.147*"video" + 0.093*"YouTube" + 0.053*"Video" + 0.025*"Holiday" + 0.024*"Cyber Monday" + 0.023*"music" + 0.016*"Check" + 0.014*"Photography" + 0.014*"Live" + 0.012*"Tonight"
topic #18 (0.033): 0.092*"Wikileaks" + 0.086*"WikiLeaks" + 0.036*"wikileaks" + 0.035*"cablegate" + 0.023*"US" + 0.020*"Wiki Leaks" + 0.016*"NYT" + 0.015*"Guardian" + 0.014*"documents" + 0.014*"cables"
topic #17 (0.033): 0.053*"China" + 0.044*"North Korea" + 0.027*"South Korea" + 0.021*"Ireland" + 0.017*"US" + 0.017*"attack" + 0.017*"Japan" + 0.017*"Reuters" + 0.016*"Korea" + 0.014*"Europe"
topic #7 (0.033): 0.149*"Twitter" + 0.086*"Facebook" + 0.030*"Tweet" + 0.026*"NYC" + 0.018*"website" + 0.014*"photos" + 0.014*"Chinese" + 0.013*"People" + 0.011*"Internet" + 0.008*"social media"
topic diff=0.061435, rho=0.116346
PROGRESS: pass 1, at document #82000/143749
performing inference on a chunk of 2000 documents
1997/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.091*"who" + 0.023*"me a" + 0.022*"Nice" + 0.020*"right" + 0.020*"a man" + 0.018*"man" + 0.017*"Yes" + 0.016*"I'm going" + 0.016*"Oh" + 0.015*"always"
topic #2 (0.033): 0.043*"News" + 0.020*"Texas" + 0.019*"son" + 0.018*"Uruguay" + 0.017*"NFL" + 0.015*"LMAO" + 0.015*"Tom DeLay" + 0.013*"Jewish" + 0.013*"English" + 0.012*"Report"
topic #18 (0.033): 0.091*"WikiLeaks" + 0.089*"Wikileaks" + 0.035*"wikileaks" + 0.034*"cablegate" + 0.030*"Guardian" + 0.023*"US" + 0.019*"Wiki Leaks" + 0.016*"NYT" + 0.015*"documents" + 0.015*"cables"
topic #12 (0.033): 0.098*"God" + 0.039*"Who" + 0.022*"al" + 0.018*"left" + 0.016*"guy" + 0.015*"green" + 0.014*"NPR" + 0.014*"GOD" + 0.012*"help" + 0.012*"Chile"
topic #13 (0.033): 0.036*"AP" + 0.019*"DADT" + 0.013*"risk" + 0.012*"child" + 0.011*"friend" + 0.011*"study" + 0.010*"Pentagon" + 0.010*"Debt" + 0.009*"report" + 0.009*"state"
topic diff=0.125957, rho=0.116346
PROGRESS: pass 1, at document #84000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.042*"AP" + 0.019*"DADT" + 0.013*"risk" + 0.012*"child" + 0.011*"Pentagon" + 0.011*"friend" + 0.010*"study" + 0.009*"report" + 0.009*"Debt" + 0.009*"law"
topic #1 (0.033): 0.083*"TSA" + 0.057*"Obama" + 0.048*"GOP" + 0.029*"ppl" + 0.019*"Senate" + 0.016*"Palin" + 0.015*"Congress" + 0.014*"politics" + 0.013*"House" + 0.013*"vote"
topic #8 (0.033): 0.091*"Iran" + 0.049*"Israel" + 0.043*"US" + 0.032*"Afghanistan" + 0.022*"Iraq" + 0.018*"to win" + 0.016*"American" + 0.015*"Egypt" + 0.015*"BBC" + 0.014*"Brasil"
topic #21 (0.033): 0.044*"NATO" + 0.042*"CNN" + 0.031*"education" + 0.025*"Chicago" + 0.025*"dans" + 0.023*"cancer" + 0.021*"Pope" + 0.020*"Toronto" + 0.018*"Friday" + 0.017*"Retweet"
topic #7 (0.033): 0.151*"Twitter" + 0.083*"Facebook" + 0.027*"Tweet" + 0.025*"NYC" + 0.017*"website" + 0.015*"COICA" + 0.013*"Tweets" + 0.013*"People" + 0.013*"photos" + 0.013*"Chinese"
topic diff=0.018520, rho=0.116346
PROGRESS: pass 1, at document #86000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.051*"lol" + 0.039*"today" + 0.029*"people" + 0.026*"you" + 0.025*"time" + 0.021*"out" + 0.020*"can" + 0.020*"ur" + 0.019*"who" + 0.018*"Lol"
topic #3 (0.033): 0.050*"UK" + 0.038*"photography" + 0.030*"London" + 0.022*"police" + 0.020*"students" + 0.017*"British" + 0.015*"Colombia" + 0.015*"school" + 0.015*"protest" + 0.012*"Police"
topic #26 (0.033): 0.036*"India" + 0.029*"news" + 0.023*"business" + 0.020*"New Zealand" + 0.015*"dead" + 0.013*"mine" + 0.013*"Africa" + 0.012*"energy" + 0.011*"killed" + 0.011*"Australia"
topic #23 (0.033): 0.051*"haha" + 0.034*"game" + 0.029*"show" + 0.027*"football" + 0.017*"beautiful" + 0.016*"Kanye West" + 0.015*"hair" + 0.015*"net" + 0.014*"team" + 0.012*"thoughts"
topic #19 (0.033): 0.046*"Leslie Nielsen" + 0.045*"Today" + 0.037*"Love" + 0.024*"Jews" + 0.024*"nice" + 0.022*"Life" + 0.021*"right now" + 0.021*"Time" + 0.019*"white" + 0.014*"need"
topic diff=0.032981, rho=0.116346
PROGRESS: pass 1, at document #88000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.024*"season" + 0.022*"NBC" + 0.020*"high school" + 0.020*"student" + 0.016*"Wisconsin" + 0.016*"smile" + 0.015*"record" + 0.014*"EEUU" + 0.013*"Football" + 0.012*"Gold"
topic #25 (0.033): 0.048*"lol" + 0.038*"today" + 0.029*"people" + 0.027*"you" + 0.025*"time" + 0.020*"can" + 0.020*"out" + 0.020*"ur" + 0.020*"who" + 0.018*"love"
topic #2 (0.033): 0.037*"News" + 0.027*"bit.ly" + 0.021*"Texas" + 0.020*"NFL" + 0.020*"EUA" + 0.015*"son" + 0.015*"LMAO" + 0.014*"English" + 0.014*"Uruguay" + 0.014*"Report"
topic #6 (0.033): 0.015*"U.S" + 0.013*"World" + 0.010*"News" + 0.010*"China" + 0.010*"Will" + 0.008*"France" + 0.008*"Women" + 0.008*"Study" + 0.008*"NYT" + 0.007*"the Internet"
topic #27 (0.033): 0.094*"blog" + 0.044*"post" + 0.021*"Education" + 0.020*"DVD" + 0.020*"Complexo do Alemão" + 0.020*"recipes" + 0.016*"Amazon" + 0.015*"após" + 0.015*"iOS" + 0.014*"web"
topic diff=0.032594, rho=0.116346
PROGRESS: pass 1, at document #90000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.015*"U.S" + 0.012*"World" + 0.011*"News" + 0.010*"Will" + 0.009*"China" + 0.009*"Study" + 0.008*"Women" + 0.008*"France" + 0.008*"NYT" + 0.007*"BP"
topic #25 (0.033): 0.074*"lol" + 0.036*"today" + 0.028*"people" + 0.027*"you" + 0.025*"time" + 0.019*"can" + 0.019*"ur" + 0.018*"who" + 0.018*"out" + 0.018*"love"
topic #22 (0.033): 0.036*"Blog" + 0.024*"re" + 0.023*"We" + 0.021*"see" + 0.017*"believe" + 0.015*"food" + 0.014*"WTF" + 0.014*"book" + 0.013*"health" + 0.012*"words"
topic #0 (0.033): 0.076*"Haiti" + 0.027*"UN" + 0.023*"cholera" + 0.019*"Gaza" + 0.018*"Mexico" + 0.017*"Israeli" + 0.016*"Lebanon" + 0.016*"Yahoo" + 0.016*"tweeted" + 0.015*"death"
topic #20 (0.033): 0.042*"live" + 0.035*"TV" + 0.030*"Watch" + 0.030*"world" + 0.028*"Hope" + 0.019*"interview" + 0.018*"climate" + 0.018*"COP16" + 0.017*"dia" + 0.016*"climate change"
topic diff=0.025617, rho=0.116346
PROGRESS: pass 1, at document #92000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.069*"via" + 0.038*"Google" + 0.036*"iPad" + 0.032*"iPhone" + 0.030*"Facebook" + 0.026*"Apple" + 0.021*"Christmas" + 0.020*"online" + 0.017*"iTunes" + 0.017*"Free"
topic #27 (0.033): 0.091*"blog" + 0.040*"post" + 0.020*"DVD" + 0.019*"Education" + 0.017*"Complexo do Alemão" + 0.017*"recipes" + 0.017*"release" + 0.016*"Amazon" + 0.015*"iOS" + 0.014*"PC"
topic #24 (0.033): 0.094*"who" + 0.024*"me a" + 0.021*"Nice" + 0.019*"man" + 0.018*"a man" + 0.018*"right" + 0.018*"Yes" + 0.017*"Oh" + 0.017*"Arizona" + 0.016*"always"
topic #25 (0.033): 0.072*"lol" + 0.036*"today" + 0.028*"people" + 0.027*"you" + 0.025*"time" + 0.020*"can" + 0.019*"who" + 0.018*"ur" + 0.018*"out" + 0.018*"love"
topic #18 (0.033): 0.098*"WikiLeaks" + 0.089*"Wikileaks" + 0.032*"cablegate" + 0.031*"wikileaks" + 0.025*"US" + 0.022*"Guardian" + 0.020*"Wiki Leaks" + 0.017*"documents" + 0.015*"NYT" + 0.013*"cables"
topic diff=0.023687, rho=0.116346
PROGRESS: pass 1, at document #94000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.015*"U.S" + 0.012*"World" + 0.010*"Will" + 0.010*"China" + 0.010*"News" + 0.009*"Women" + 0.008*"Study" + 0.008*"Tea Party" + 0.008*"France" + 0.008*"New York"
topic #11 (0.033): 0.071*"via" + 0.038*"iPad" + 0.037*"Google" + 0.034*"iPhone" + 0.030*"Facebook" + 0.028*"Apple" + 0.020*"Christmas" + 0.019*"online" + 0.016*"iTunes" + 0.016*"Free"
topic #7 (0.033): 0.148*"Twitter" + 0.083*"Facebook" + 0.027*"NYC" + 0.027*"Tweet" + 0.013*"photos" + 0.012*"website" + 0.012*"People" + 0.012*"Chinese" + 0.012*"Internet" + 0.011*"Tweets"
topic #19 (0.033): 0.043*"Today" + 0.039*"Love" + 0.034*"Leslie Nielsen" + 0.029*"Time" + 0.022*"nice" + 0.021*"Life" + 0.021*"right now" + 0.018*"Jews" + 0.017*"Oregon" + 0.015*"Actor"
topic #26 (0.033): 0.042*"news" + 0.036*"India" + 0.022*"New Zealand" + 0.021*"business" + 0.015*"dead" + 0.015*"mine" + 0.013*"energy" + 0.012*"killed" + 0.011*"police" + 0.010*"Africa"
topic diff=0.018824, rho=0.116346
PROGRESS: pass 1, at document #96000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #26 (0.033): 0.040*"news" + 0.038*"India" + 0.023*"New Zealand" + 0.019*"business" + 0.015*"mine" + 0.015*"dead" + 0.012*"killed" + 0.012*"police" + 0.011*"energy" + 0.010*"Australia"
topic #28 (0.033): 0.116*"video" + 0.065*"YouTube" + 0.060*"Video" + 0.031*"Cyber Monday" + 0.027*"Holiday" + 0.024*"music" + 0.016*"Live" + 0.015*"Check" + 0.012*"ad" + 0.012*"Photography"
topic #18 (0.033): 0.097*"WikiLeaks" + 0.087*"Wikileaks" + 0.031*"cablegate" + 0.027*"wikileaks" + 0.025*"US" + 0.021*"Guardian" + 0.020*"Wiki Leaks" + 0.019*"Pakistan" + 0.017*"documents" + 0.015*"NYT"
topic #11 (0.033): 0.073*"via" + 0.038*"iPad" + 0.037*"Google" + 0.033*"iPhone" + 0.030*"Facebook" + 0.027*"Apple" + 0.021*"Christmas" + 0.020*"online" + 0.017*"iTunes" + 0.016*"Free"
topic #17 (0.033): 0.050*"China" + 0.044*"North Korea" + 0.024*"South Korea" + 0.021*"Ireland" + 0.018*"Reuters" + 0.018*"Korea" + 0.017*"US" + 0.017*"Japan" + 0.017*"Europe" + 0.016*"attack"
topic diff=0.019212, rho=0.116346
PROGRESS: pass 1, at document #98000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.032*"U.S" + 0.021*"President" + 0.018*"Obama" + 0.016*"military" + 0.015*"White House" + 0.015*"Seoul" + 0.015*"S. Korea" + 0.015*"Cancun" + 0.013*"cooking" + 0.013*"Korea"
topic #23 (0.033): 0.039*"haha" + 0.033*"game" + 0.028*"show" + 0.027*"football" + 0.023*"net" + 0.017*"beautiful" + 0.016*"team" + 0.014*"hair" + 0.012*"thoughts" + 0.012*"Kanye West"
topic #10 (0.033): 0.118*"para" + 0.064*"un" + 0.044*"como" + 0.038*"dos" + 0.029*"en" + 0.025*"es" + 0.022*"la" + 0.020*"se" + 0.017*"pour" + 0.016*"si"
topic #20 (0.033): 0.041*"live" + 0.033*"TV" + 0.032*"world" + 0.031*"Watch" + 0.025*"Hope" + 0.020*"interview" + 0.020*"Check it" + 0.019*"families" + 0.019*"D RT" + 0.018*"climate"
topic #5 (0.033): 0.023*"student" + 0.021*"high school" + 0.020*"season" + 0.020*"NBC" + 0.019*"smile" + 0.016*"Wisconsin" + 0.016*"Football" + 0.015*"record" + 0.014*"NY" + 0.013*"EEUU"
topic diff=0.036328, rho=0.116346
bound: at document #0
-24.160 per-word bound, 18742538.9 perplexity estimate based on a held-out corpus of 2000 documents with 44486 words
PROGRESS: pass 1, at document #100000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.021*"student" + 0.020*"high school" + 0.019*"season" + 0.019*"NBC" + 0.018*"smile" + 0.015*"Wisconsin" + 0.014*"record" + 0.014*"Football" + 0.014*"NY" + 0.012*"EEUU"
topic #19 (0.033): 0.044*"Love" + 0.043*"Today" + 0.038*"Leslie Nielsen" + 0.028*"Time" + 0.024*"Life" + 0.021*"nice" + 0.020*"right now" + 0.018*"Actor" + 0.016*"Oregon" + 0.016*"Jews"
topic #26 (0.033): 0.039*"India" + 0.036*"news" + 0.021*"New Zealand" + 0.018*"business" + 0.014*"dead" + 0.014*"Australia" + 0.013*"mine" + 0.012*"killed" + 0.011*"police" + 0.011*"energy"
topic #3 (0.033): 0.061*"UK" + 0.036*"London" + 0.023*"students" + 0.021*"photography" + 0.021*"police" + 0.016*"British" + 0.015*"protest" + 0.015*"Colombia" + 0.013*"Police" + 0.011*"Read"
topic #23 (0.033): 0.036*"haha" + 0.031*"game" + 0.028*"show" + 0.026*"football" + 0.021*"net" + 0.016*"beautiful" + 0.016*"team" + 0.013*"hair" + 0.012*"thoughts" + 0.011*"Kanye West"
topic diff=0.021607, rho=0.116346
PROGRESS: pass 1, at document #102000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.039*"NATO" + 0.037*"CNN" + 0.029*"Venezuela" + 0.028*"cancer" + 0.027*"dans" + 0.026*"Chicago" + 0.022*"Pope" + 0.021*"education" + 0.018*"Retweet" + 0.015*"Friday"
topic #29 (0.033): 0.196*"Thanksgiving" + 0.069*"Happy Thanksgiving" + 0.039*"LOL" + 0.035*"friends" + 0.032*"family" + 0.031*"holiday" + 0.029*"thanksgiving" + 0.029*"Turkey" + 0.020*"tonight" + 0.014*"Harry Potter"
topic #18 (0.033): 0.097*"WikiLeaks" + 0.089*"Wikileaks" + 0.039*"wikileaks" + 0.032*"cablegate" + 0.025*"US" + 0.020*"Pakistan" + 0.020*"Guardian" + 0.018*"Wiki Leaks" + 0.015*"documents" + 0.015*"NYT"
topic #26 (0.033): 0.039*"India" + 0.033*"news" + 0.022*"New Zealand" + 0.021*"business" + 0.014*"dead" + 0.014*"mine" + 0.013*"Australia" + 0.013*"killed" + 0.011*"police" + 0.011*"murder"
topic #28 (0.033): 0.108*"video" + 0.068*"Video" + 0.057*"YouTube" + 0.028*"Cyber Monday" + 0.026*"Holiday" + 0.022*"music" + 0.014*"Live" + 0.013*"Check" + 0.012*"Photography" + 0.011*"ad"
topic diff=0.089321, rho=0.116346
PROGRESS: pass 1, at document #104000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.068*"Haiti" + 0.028*"UN" + 0.024*"cholera" + 0.022*"Yahoo" + 0.022*"Mexico" + 0.020*"earthquake" + 0.018*"gold" + 0.016*"election" + 0.015*"death" + 0.014*"Asian"
topic #18 (0.033): 0.089*"Wikileaks" + 0.087*"WikiLeaks" + 0.045*"cablegate" + 0.044*"wikileaks" + 0.027*"US" + 0.022*"Guardian" + 0.018*"Wiki Leaks" + 0.017*"Pakistan" + 0.014*"documents" + 0.013*"cables"
topic #5 (0.033): 0.024*"NBC" + 0.023*"student" + 0.020*"smile" + 0.019*"season" + 0.018*"high school" + 0.015*"Wisconsin" + 0.015*"Football" + 0.014*"record" + 0.013*"NY" + 0.013*"EEUU"
topic #25 (0.033): 0.060*"lol" + 0.042*"today" + 0.027*"people" + 0.025*"time" + 0.025*"you" + 0.023*"out" + 0.021*"ur" + 0.020*"who" + 0.019*"am" + 0.018*"can"
topic #27 (0.033): 0.091*"blog" + 0.038*"post" + 0.020*"DVD" + 0.018*"web" + 0.016*"release" + 0.016*"Amazon" + 0.015*"PC" + 0.015*"Oprah" + 0.013*"Education" + 0.013*"iOS"
topic diff=0.092619, rho=0.116346
PROGRESS: pass 1, at document #106000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.056*"AP" + 0.016*"Amanda Knox" + 0.015*"DADT" + 0.015*"child" + 0.012*"risk" + 0.010*"law" + 0.010*"study" + 0.010*"report" + 0.009*"prison" + 0.009*"Pentagon"
topic #24 (0.033): 0.090*"who" + 0.022*"me a" + 0.021*"Nice" + 0.018*"a man" + 0.017*"actor" + 0.016*"Arizona" + 0.016*"Oh" + 0.016*"Yes" + 0.015*"always" + 0.015*"right"
topic #10 (0.033): 0.097*"para" + 0.076*"un" + 0.042*"como" + 0.031*"es" + 0.029*"en" + 0.027*"pour" + 0.027*"dos" + 0.023*"la" + 0.018*"México" + 0.018*"se"
topic #1 (0.033): 0.070*"TSA" + 0.057*"Obama" + 0.053*"GOP" + 0.022*"Senate" + 0.020*"ppl" + 0.018*"Congress" + 0.018*"Palin" + 0.015*"House" + 0.015*"vote" + 0.015*"Republicans"
topic #14 (0.033): 0.080*"tcot" + 0.024*"Bush" + 0.015*"Obama" + 0.014*"teaparty" + 0.012*"Muslim" + 0.010*"GM" + 0.010*"woman" + 0.010*"Islam" + 0.009*"arrested" + 0.009*"Alaska"
topic diff=0.025109, rho=0.116346
PROGRESS: pass 1, at document #108000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.206*"Thanksgiving" + 0.068*"Happy Thanksgiving" + 0.036*"LOL" + 0.034*"holiday" + 0.032*"friends" + 0.031*"family" + 0.028*"Turkey" + 0.025*"thanksgiving" + 0.021*"tonight" + 0.016*"Black Friday"
topic #17 (0.033): 0.050*"China" + 0.041*"North Korea" + 0.026*"Ireland" + 0.025*"Japan" + 0.024*"South Korea" + 0.019*"Reuters" + 0.018*"Korea" + 0.016*"US" + 0.016*"BBC News" + 0.015*"Irish"
topic #20 (0.033): 0.055*"TV" + 0.043*"live" + 0.034*"Watch" + 0.026*"world" + 0.023*"interview" + 0.021*"Hope" + 0.021*"climate" + 0.020*"Radio" + 0.020*"families" + 0.020*"COP16"
topic #3 (0.033): 0.083*"UK" + 0.035*"London" + 0.026*"police" + 0.022*"students" + 0.016*"British" + 0.016*"protest" + 0.016*"Labour" + 0.013*"Police" + 0.013*"photography" + 0.013*"demo 2010"
topic #7 (0.033): 0.154*"Twitter" + 0.080*"Facebook" + 0.030*"Tweet" + 0.024*"NYC" + 0.017*"Lmao" + 0.013*"Tweets" + 0.012*"photos" + 0.012*"Chinese" + 0.012*"People" + 0.011*"website"
topic diff=0.024565, rho=0.116346
PROGRESS: pass 1, at document #110000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.044*"Black Friday" + 0.023*"Sarah Palin" + 0.023*"read" + 0.022*"newspaper" + 0.021*"HIV" + 0.020*"If" + 0.019*"Canada" + 0.018*"DWTS" + 0.016*"Here" + 0.014*"gay"
topic #23 (0.033): 0.034*"football" + 0.031*"haha" + 0.028*"game" + 0.026*"show" + 0.022*"Spanish" + 0.017*"team" + 0.017*"Italian" + 0.016*"reply" + 0.015*"net" + 0.013*"Kanye West"
topic #5 (0.033): 0.089*"fb" + 0.025*"season" + 0.023*"NBC" + 0.019*"student" + 0.019*"Sports" + 0.017*"Football" + 0.016*"smile" + 0.015*"high school" + 0.015*"Wisconsin" + 0.011*"Los Angeles"
topic #15 (0.033): 0.039*"French" + 0.032*"USA" + 0.020*"here" + 0.018*"Indonesian" + 0.018*"travel" + 0.017*"women" + 0.016*"England" + 0.015*"snow" + 0.013*"sex" + 0.012*"Hollywood"
topic #27 (0.033): 0.080*"blog" + 0.074*"release" + 0.035*"post" + 0.018*"DVD" + 0.017*"Amazon" + 0.017*"book" + 0.017*"iOS" + 0.015*"web" + 0.015*"PC" + 0.013*"Oprah"
topic diff=0.079491, rho=0.116346
PROGRESS: pass 1, at document #112000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.044*"Black Friday" + 0.024*"read" + 0.023*"Sarah Palin" + 0.022*"newspaper" + 0.021*"HIV" + 0.019*"If" + 0.019*"Canada" + 0.018*"DWTS" + 0.016*"Here" + 0.013*"gay"
topic #8 (0.033): 0.057*"Iran" + 0.045*"US" + 0.035*"Israel" + 0.025*"Afghanistan" + 0.019*"Brasil" + 0.018*"to win" + 0.016*"American" + 0.015*"Iraq" + 0.012*"Election" + 0.012*"BBC"
topic #5 (0.033): 0.081*"fb" + 0.024*"season" + 0.022*"NBC" + 0.019*"student" + 0.017*"Sports" + 0.017*"Football" + 0.016*"smile" + 0.015*"high school" + 0.015*"Wisconsin" + 0.011*"Los Angeles"
topic #21 (0.033): 0.036*"NATO" + 0.033*"CNN" + 0.025*"Chicago" + 0.024*"Venezuela" + 0.023*"cancer" + 0.022*"Pope" + 0.020*"Arsenal" + 0.018*"dans" + 0.018*"education" + 0.018*"Retweet"
topic #12 (0.033): 0.120*"Celebrity" + 0.078*"God" + 0.040*"Who" + 0.022*"al" + 0.014*"left" + 0.013*"NPR" + 0.012*"Who's" + 0.012*"green" + 0.011*"poor" + 0.011*"America"
topic diff=0.031311, rho=0.116346
PROGRESS: pass 1, at document #114000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.041*"News" + 0.024*"LMAO" + 0.021*"NFL" + 0.016*"Texas" + 0.016*"English" + 0.015*"MUSIC" + 0.014*"Boston" + 0.014*"You" + 0.013*"bit.ly" + 0.013*"Portuguese"
topic #5 (0.033): 0.073*"fb" + 0.024*"season" + 0.021*"NBC" + 0.019*"student" + 0.016*"smile" + 0.016*"Football" + 0.016*"Sports" + 0.015*"high school" + 0.014*"Wisconsin" + 0.012*"Good"
topic #29 (0.033): 0.199*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.040*"LOL" + 0.033*"family" + 0.033*"friends" + 0.031*"holiday" + 0.027*"Turkey" + 0.023*"thanksgiving" + 0.019*"tonight" + 0.016*"Black Friday"
topic #13 (0.033): 0.099*"Breaking News" + 0.042*"AP" + 0.014*"DADT" + 0.011*"child" + 0.010*"risk" + 0.010*"study" + 0.009*"Amanda Knox" + 0.009*"Pentagon" + 0.009*"report" + 0.009*"Belgium"
topic #21 (0.033): 0.039*"NATO" + 0.035*"CNN" + 0.026*"Chicago" + 0.025*"Venezuela" + 0.022*"cancer" + 0.022*"Pope" + 0.021*"Retweet" + 0.019*"Arsenal" + 0.018*"education" + 0.017*"dans"
topic diff=0.015970, rho=0.116346
PROGRESS: pass 1, at document #116000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.092*"Celebrity" + 0.078*"God" + 0.039*"Who" + 0.025*"al" + 0.014*"NPR" + 0.013*"left" + 0.012*"DREAM Act" + 0.012*"America" + 0.011*"green" + 0.011*"Who's"
topic #26 (0.033): 0.040*"India" + 0.033*"news" + 0.020*"New Zealand" + 0.019*"NZ" + 0.015*"business" + 0.015*"killed" + 0.014*"dead" + 0.014*"Australia" + 0.013*"mine" + 0.011*"energy"
topic #4 (0.033): 0.029*"U.S" + 0.029*"Mc" + 0.023*"President" + 0.016*"Obama" + 0.016*"White House" + 0.015*"Hoy" + 0.014*"Cancun" + 0.014*"military" + 0.013*"S. Korea" + 0.012*"Yahoo! News"
topic #7 (0.033): 0.200*"Twitter" + 0.078*"Facebook" + 0.023*"Tweet" + 0.019*"NYC" + 0.013*"uploaded" + 0.012*"Chinese" + 0.011*"Internet" + 0.011*"Thx" + 0.011*"Lmao" + 0.010*"photos"
topic #19 (0.033): 0.047*"Today" + 0.045*"Love" + 0.043*"Leslie Nielsen" + 0.026*"white" + 0.026*"Time" + 0.022*"Life" + 0.018*"nice" + 0.017*"Actor" + 0.015*"right now" + 0.014*"Palestinian"
topic diff=0.030974, rho=0.116346
PROGRESS: pass 1, at document #118000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.022*"jajaja" + 0.016*"U.S" + 0.016*"News" + 0.012*"World" + 0.010*"Women" + 0.010*"Will" + 0.009*"China" + 0.009*"Billion" + 0.008*"New York" + 0.008*"Tea Party"
topic #7 (0.033): 0.192*"Twitter" + 0.080*"Facebook" + 0.035*"NYC" + 0.023*"Tweet" + 0.013*"Thx" + 0.012*"uploaded" + 0.011*"Chinese" + 0.011*"Internet" + 0.010*"Lmao" + 0.010*"People"
topic #28 (0.033): 0.111*"video" + 0.087*"YouTube" + 0.068*"Video" + 0.029*"Cyber Monday" + 0.028*"Holiday" + 0.023*"music" + 0.014*"Check" + 0.013*"Live" + 0.010*"Photography" + 0.010*"Page"
topic #9 (0.033): 0.029*"will" + 0.028*"em" + 0.023*"do" + 0.020*"work" + 0.014*"Thank you" + 0.013*"mind" + 0.013*"soul" + 0.013*"talk" + 0.012*"VIDEO" + 0.012*"don"
topic #24 (0.033): 0.089*"who" + 0.021*"Nice" + 0.021*"a man" + 0.019*"me a" + 0.018*"Madrid" + 0.018*"Yes" + 0.016*"always" + 0.016*"actor" + 0.016*"Arizona" + 0.015*"Oh"
topic diff=0.017218, rho=0.116346
bound: at document #0
-23.298 per-word bound, 10313935.2 perplexity estimate based on a held-out corpus of 2000 documents with 33817 words
PROGRESS: pass 1, at document #120000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.046*"live" + 0.045*"TV" + 0.035*"Watch" + 0.030*"world" + 0.028*"Hope" + 0.022*"climate" + 0.021*"interview" + 0.019*"families" + 0.018*"Check it" + 0.017*"climate change"
topic #15 (0.033): 0.038*"USA" + 0.024*"French" + 0.021*"women" + 0.018*"Thailand" + 0.018*"England" + 0.018*"snow" + 0.017*"here" + 0.017*"travel" + 0.015*"sex" + 0.013*"Thai"
topic #11 (0.033): 0.060*"via" + 0.045*"Google" + 0.044*"iPad" + 0.041*"iPhone" + 0.038*"Apple" + 0.037*"Facebook" + 0.021*"Android" + 0.017*"Christmas" + 0.016*"iTunes" + 0.016*"Microsoft"
topic #9 (0.033): 0.029*"will" + 0.026*"em" + 0.023*"do" + 0.019*"work" + 0.015*"VIDEO" + 0.013*"Thank you" + 0.013*"mind" + 0.013*"talk" + 0.012*"soul" + 0.012*"first"
topic #2 (0.033): 0.043*"News" + 0.034*"Boston" + 0.023*"EUA" + 0.022*"NFL" + 0.021*"LMAO" + 0.019*"bit.ly" + 0.016*"USD" + 0.015*"Texas" + 0.014*"English" + 0.013*"Las Vegas"
topic diff=0.032222, rho=0.116346
PROGRESS: pass 1, at document #122000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.050*"lol" + 0.037*"today" + 0.030*"people" + 0.024*"time" + 0.023*"you" + 0.021*"ur" + 0.021*"out" + 0.020*"can" + 0.020*"who" + 0.018*"love"
topic #14 (0.033): 0.057*"tcot" + 0.020*"Bush" + 0.014*"Obama" + 0.011*"Colombian" + 0.011*"GM" + 0.010*"woman" + 0.009*"teaparty" + 0.009*"FBI" + 0.009*"Muslim" + 0.009*"arrested"
topic #2 (0.033): 0.042*"News" + 0.032*"Boston" + 0.025*"EUA" + 0.022*"NFL" + 0.020*"LMAO" + 0.018*"bit.ly" + 0.016*"Texas" + 0.015*"English" + 0.015*"USD" + 0.014*"Report"
topic #21 (0.033): 0.036*"NATO" + 0.033*"CNN" + 0.029*"Chicago" + 0.028*"Retweet" + 0.025*"Venezuela" + 0.022*"dans" + 0.022*"Pope" + 0.020*"cancer" + 0.017*"Friday" + 0.017*"Arsenal"
topic #7 (0.033): 0.182*"Twitter" + 0.081*"Facebook" + 0.036*"NYC" + 0.022*"Tweet" + 0.013*"Lmao" + 0.012*"Thx" + 0.012*"People" + 0.012*"uploaded" + 0.011*"Chinese" + 0.010*"photos"
topic diff=0.017564, rho=0.116346
PROGRESS: pass 1, at document #124000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.112*"God" + 0.058*"Celebrity" + 0.040*"Who" + 0.024*"al" + 0.013*"left" + 0.013*"NPR" + 0.013*"GOD" + 0.011*"DREAM Act" + 0.011*"evil" + 0.011*"poor"
topic #26 (0.033): 0.053*"India" + 0.033*"news" + 0.020*"New Zealand" + 0.019*"NZ" + 0.015*"business" + 0.014*"Australia" + 0.014*"killed" + 0.013*"dead" + 0.013*"energy" + 0.012*"mine"
topic #27 (0.033): 0.081*"blog" + 0.042*"release" + 0.033*"post" + 0.020*"Amazon" + 0.018*"DVD" + 0.018*"Complexo do Alemão" + 0.017*"Oprah" + 0.017*"PC" + 0.016*"recipes" + 0.014*"web"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.074*"Happy Thanksgiving" + 0.044*"LOL" + 0.041*"family" + 0.032*"friends" + 0.027*"holiday" + 0.026*"Turkey" + 0.023*"thanksgiving" + 0.019*"tonight" + 0.015*"Black Friday"
topic #1 (0.033): 0.052*"GOP" + 0.051*"TSA" + 0.050*"Obama" + 0.024*"ppl" + 0.022*"Congress" + 0.019*"Republicans" + 0.018*"Senate" + 0.016*"Palin" + 0.015*"House" + 0.014*"vote"
topic diff=0.037736, rho=0.116346
PROGRESS: pass 1, at document #126000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.114*"God" + 0.053*"Celebrity" + 0.041*"Who" + 0.023*"al" + 0.013*"left" + 0.013*"NPR" + 0.012*"GOD" + 0.012*"Who's" + 0.012*"guy" + 0.011*"evil"
topic #22 (0.033): 0.097*"week" + 0.044*"Blog" + 0.026*"book" + 0.019*"Vintage" + 0.019*"We" + 0.019*"re" + 0.013*"see" + 0.013*"reason" + 0.013*"Post" + 0.012*"Gracias"
topic #3 (0.033): 0.080*"UK" + 0.037*"London" + 0.024*"police" + 0.020*"students" + 0.018*"British" + 0.017*"protest" + 0.015*"Cuba" + 0.014*"Colombia" + 0.014*"Police" + 0.011*"demo 2010"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.076*"Happy Thanksgiving" + 0.042*"LOL" + 0.041*"family" + 0.032*"friends" + 0.028*"holiday" + 0.026*"Turkey" + 0.025*"thanksgiving" + 0.018*"tonight" + 0.015*"Black Friday"
topic #18 (0.033): 0.091*"WikiLeaks" + 0.079*"Wikileaks" + 0.031*"wikileaks" + 0.027*"cablegate" + 0.026*"US" + 0.018*"Guardian" + 0.018*"Pakistan" + 0.017*"Wiki Leaks" + 0.014*"documents" + 0.012*"ha"
topic diff=0.051456, rho=0.116346
PROGRESS: pass 1, at document #128000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.037*"fb" + 0.025*"season" + 0.022*"Football" + 0.022*"student" + 0.017*"NBC" + 0.017*"high school" + 0.016*"smile" + 0.014*"Sports" + 0.014*"Wisconsin" + 0.013*"Gold"
topic #0 (0.033): 0.076*"Haiti" + 0.040*"Amazon.com" + 0.027*"cholera" + 0.022*"UN" + 0.018*"Dutch" + 0.017*"Asian" + 0.017*"Israeli" + 0.017*"Gaza" + 0.015*"gold" + 0.015*"Cholera"
topic #18 (0.033): 0.090*"WikiLeaks" + 0.077*"Wikileaks" + 0.031*"wikileaks" + 0.026*"cablegate" + 0.025*"US" + 0.019*"Pakistan" + 0.017*"Wiki Leaks" + 0.017*"Guardian" + 0.014*"documents" + 0.013*"ha"
topic #8 (0.033): 0.046*"Iran" + 0.046*"US" + 0.036*"Israel" + 0.026*"Afghanistan" + 0.020*"to win" + 0.020*"Brasil" + 0.015*"American" + 0.014*"Iraq" + 0.012*"pode" + 0.011*"BBC"
topic #29 (0.033): 0.206*"Thanksgiving" + 0.077*"Happy Thanksgiving" + 0.042*"LOL" + 0.040*"family" + 0.032*"friends" + 0.028*"holiday" + 0.026*"Turkey" + 0.025*"thanksgiving" + 0.018*"tonight" + 0.015*"Black Friday"
topic diff=0.033027, rho=0.116346
PROGRESS: pass 1, at document #130000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.080*"blog" + 0.033*"release" + 0.032*"post" + 0.021*"DVD" + 0.020*"Oprah" + 0.020*"Amazon" + 0.017*"recipes" + 0.017*"PC" + 0.014*"iOS" + 0.013*"web"
topic #16 (0.033): 0.045*"Black Friday" + 0.023*"Sarah Palin" + 0.021*"HIV" + 0.018*"read" + 0.016*"If" + 0.015*"Here" + 0.015*"Canada" + 0.015*"DWTS" + 0.012*"to show" + 0.012*"newspaper"
topic #8 (0.033): 0.047*"US" + 0.044*"Iran" + 0.035*"Israel" + 0.026*"Afghanistan" + 0.020*"to win" + 0.019*"Brasil" + 0.015*"American" + 0.013*"Iraq" + 0.011*"pode" + 0.010*"BBC"
topic #24 (0.033): 0.087*"who" + 0.024*"a man" + 0.021*"Nice" + 0.021*"always" + 0.020*"me a" + 0.018*"Oh" + 0.016*"Yes" + 0.016*"Arizona" + 0.015*"actor" + 0.015*"media"
topic #6 (0.033): 0.015*"U.S" + 0.015*"News" + 0.013*"BP" + 0.013*"World" + 0.012*"jajaja" + 0.010*"China" + 0.009*"Will" + 0.009*"New York" + 0.009*"Women" + 0.008*"Earth"
topic diff=0.020898, rho=0.116346
PROGRESS: pass 1, at document #132000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.057*"CNN" + 0.040*"Retweet" + 0.031*"NATO" + 0.027*"Chicago" + 0.020*"Venezuela" + 0.020*"Pope" + 0.020*"cancer" + 0.019*"Friday" + 0.015*"education" + 0.014*"dans"
topic #6 (0.033): 0.015*"U.S" + 0.015*"News" + 0.013*"World" + 0.013*"BP" + 0.011*"jajaja" + 0.010*"China" + 0.010*"Will" + 0.009*"Women" + 0.009*"New York" + 0.008*"Earth"
topic #1 (0.033): 0.055*"TSA" + 0.050*"Obama" + 0.043*"GOP" + 0.024*"ppl" + 0.020*"Congress" + 0.019*"Senate" + 0.018*"vote" + 0.017*"Republicans" + 0.015*"Palin" + 0.015*"House"
topic #12 (0.033): 0.109*"God" + 0.039*"Who" + 0.038*"Celebrity" + 0.022*"al" + 0.014*"Who's" + 0.013*"left" + 0.012*"guy" + 0.012*"help" + 0.011*"GOD" + 0.011*"NPR"
topic #4 (0.033): 0.027*"U.S" + 0.021*"Seoul" + 0.021*"President" + 0.020*"White House" + 0.020*"Hoy" + 0.018*"Obama" + 0.016*"cooking" + 0.013*"Cancun" + 0.013*"Yahoo! News" + 0.013*"Mc"
topic diff=0.026021, rho=0.116346
PROGRESS: pass 1, at document #134000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.071*"tweet" + 0.042*"lol" + 0.034*"now" + 0.032*"today" + 0.027*"people" + 0.024*"you" + 0.024*"ur" + 0.023*"time" + 0.020*"can" + 0.019*"who"
topic #10 (0.033): 0.105*"Como" + 0.081*"para" + 0.064*"un" + 0.038*"como" + 0.029*"es" + 0.026*"la" + 0.025*"dos" + 0.024*"en" + 0.017*"este" + 0.017*"se"
topic #23 (0.033): 0.034*"game" + 0.030*"show" + 0.026*"football" + 0.024*"haha" + 0.020*"RETWEET" + 0.019*"team" + 0.018*"net" + 0.018*"Dallas" + 0.018*"beautiful" + 0.016*"NBA"
topic #0 (0.033): 0.074*"Haiti" + 0.030*"Amazon.com" + 0.027*"cholera" + 0.023*"UN" + 0.018*"Asian" + 0.015*"death" + 0.015*"Cholera" + 0.014*"election" + 0.014*"Israeli" + 0.014*"Mexico"
topic #16 (0.033): 0.043*"Black Friday" + 0.037*"chat" + 0.031*"cash" + 0.020*"Sarah Palin" + 0.018*"DWTS" + 0.018*"read" + 0.017*"HIV" + 0.015*"If" + 0.014*"Here" + 0.014*"Canada"
topic diff=0.021677, rho=0.116346
PROGRESS: pass 1, at document #136000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.026*"U.S" + 0.022*"President" + 0.021*"White House" + 0.018*"Seoul" + 0.018*"Obama" + 0.017*"Hoy" + 0.015*"cooking" + 0.013*"S. Korea" + 0.013*"military" + 0.013*"President Obama"
topic #13 (0.033): 0.041*"AP" + 0.037*"add" + 0.035*"Breaking News" + 0.014*"Rome" + 0.013*"DADT" + 0.011*"child" + 0.010*"friend" + 0.010*"study" + 0.009*"risk" + 0.009*"founder"
topic #3 (0.033): 0.071*"UK" + 0.036*"London" + 0.021*"police" + 0.018*"students" + 0.018*"photography" + 0.017*"British" + 0.016*"protest" + 0.013*"Cuba" + 0.012*"Colombia" + 0.011*"Police"
topic #29 (0.033): 0.205*"Thanksgiving" + 0.073*"Happy Thanksgiving" + 0.039*"LOL" + 0.037*"family" + 0.034*"friends" + 0.032*"holiday" + 0.025*"Turkey" + 0.023*"thanksgiving" + 0.021*"tonight" + 0.018*"Black Friday"
topic #22 (0.033): 0.073*"Blog" + 0.055*"week" + 0.021*"re" + 0.021*"book" + 0.021*"We" + 0.014*"believe" + 0.013*"basketball" + 0.013*"words" + 0.013*"see" + 0.012*"learn"
topic diff=0.015620, rho=0.116346
PROGRESS: pass 1, at document #138000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.144*"video" + 0.061*"Video" + 0.060*"YouTube" + 0.028*"Holiday" + 0.026*"music" + 0.022*"Cyber Monday" + 0.017*"Check" + 0.014*"Photography" + 0.013*"Live" + 0.011*"Tonight"
topic #24 (0.033): 0.089*"who" + 0.023*"a man" + 0.021*"Nice" + 0.020*"me a" + 0.020*"always" + 0.020*"Oh" + 0.016*"Arizona" + 0.016*"Yes" + 0.015*"media" + 0.014*"man"
topic #8 (0.033): 0.046*"US" + 0.042*"Iran" + 0.039*"Israel" + 0.026*"Afghanistan" + 0.019*"Egypt" + 0.019*"to win" + 0.014*"American" + 0.013*"Brasil" + 0.013*"Iraq" + 0.011*"Afghan"
topic #21 (0.033): 0.054*"CNN" + 0.040*"Retweet" + 0.033*"NATO" + 0.025*"Chicago" + 0.022*"cancer" + 0.021*"Friday" + 0.021*"Venezuela" + 0.020*"Pope" + 0.015*"condoms" + 0.015*"Toronto"
topic #5 (0.033): 0.042*"IM" + 0.030*"fb" + 0.022*"student" + 0.022*"season" + 0.021*"Bingo" + 0.020*"Football" + 0.019*"NBC" + 0.019*"smile" + 0.018*"high school" + 0.015*"Wisconsin"
topic diff=0.021746, rho=0.116346
bound: at document #0
-22.278 per-word bound, 5086618.5 perplexity estimate based on a held-out corpus of 2000 documents with 78501 words
PROGRESS: pass 1, at document #140000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.152*"Como" + 0.070*"para" + 0.065*"un" + 0.034*"como" + 0.029*"es" + 0.027*"en" + 0.025*"la" + 0.020*"dos" + 0.017*"México" + 0.016*"este"
topic #0 (0.033): 0.072*"Haiti" + 0.025*"cholera" + 0.022*"UN" + 0.021*"Amazon.com" + 0.016*"Israeli" + 0.016*"election" + 0.016*"Asian" + 0.015*"death" + 0.015*"Gaza" + 0.014*"Cholera"
topic #24 (0.033): 0.087*"who" + 0.024*"fat" + 0.022*"a man" + 0.020*"me a" + 0.019*"Nice" + 0.019*"Oh" + 0.019*"always" + 0.016*"Yes" + 0.015*"Arizona" + 0.015*"media"
topic #2 (0.033): 0.043*"bit.ly" + 0.037*"News" + 0.022*"NFL" + 0.020*"Boston" + 0.020*"English" + 0.017*"LMAO" + 0.016*"Texas" + 0.014*"You" + 0.012*"songs" + 0.012*"Tom DeLay"
topic #28 (0.033): 0.140*"video" + 0.069*"Video" + 0.057*"YouTube" + 0.028*"Holiday" + 0.025*"music" + 0.023*"Cyber Monday" + 0.019*"WOW" + 0.018*"Live" + 0.018*"Check" + 0.012*"Photography"
topic diff=0.069876, rho=0.116346
PROGRESS: pass 1, at document #142000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.044*"Iran" + 0.044*"US" + 0.035*"Israel" + 0.027*"Afghanistan" + 0.021*"to win" + 0.018*"Egypt" + 0.016*"American" + 0.013*"Iraq" + 0.012*"Brasil" + 0.011*"Afghan"
topic #21 (0.033): 0.048*"CNN" + 0.041*"Retweet" + 0.031*"NATO" + 0.023*"Chicago" + 0.022*"Friday" + 0.021*"cancer" + 0.020*"Pope" + 0.019*"Venezuela" + 0.018*"Toronto" + 0.018*"speech"
topic #24 (0.033): 0.082*"who" + 0.036*"college" + 0.021*"fat" + 0.021*"a man" + 0.021*"me a" + 0.020*"Oh" + 0.019*"Nice" + 0.019*"always" + 0.018*"Yes" + 0.015*"Yeah"
topic #15 (0.033): 0.041*"Home" + 0.028*"sex" + 0.028*"USA" + 0.019*"here" + 0.017*"girl" + 0.017*"women" + 0.016*"snow" + 0.015*"School" + 0.015*"travel" + 0.014*"England"
topic #17 (0.033): 0.050*"China" + 0.043*"North Korea" + 0.029*"South Korea" + 0.024*"Ireland" + 0.021*"Korea" + 0.017*"Irish" + 0.017*"party" + 0.017*"Japan" + 0.016*"US" + 0.014*"BBC News"
topic diff=0.024925, rho=0.116346
bound: at document #0
-23.355 per-word bound, 10732134.7 perplexity estimate based on a held-out corpus of 1749 documents with 49577 words
PROGRESS: pass 1, at document #143749/143749
performing inference on a chunk of 1749 documents
1748/1749 documents converged within 50 iterations
updating topics
merging changes from 1749 documents into a model of 143749 documents
topic #29 (0.033): 0.199*"Thanksgiving" + 0.074*"Happy Thanksgiving" + 0.044*"LOL" + 0.038*"friends" + 0.037*"family" + 0.028*"holiday" + 0.024*"thanksgiving" + 0.022*"Turkey" + 0.018*"tonight" + 0.018*"go"
topic #7 (0.033): 0.283*"Twitter" + 0.065*"Facebook" + 0.060*"NYC" + 0.021*"Tweet" + 0.013*"Lady Gaga" + 0.011*"photos" + 0.010*"website" + 0.010*"Founder" + 0.009*"Chinese" + 0.008*"wind"
topic #26 (0.033): 0.036*"news" + 0.034*"India" + 0.020*"New Zealand" + 0.017*"business" + 0.015*"feeling" + 0.015*"dead" + 0.014*"mine" + 0.014*"banned" + 0.014*"killed" + 0.013*"Australia"
topic #19 (0.033): 0.047*"Love" + 0.041*"Leslie Nielsen" + 0.038*"Today" + 0.032*"need" + 0.028*"Music" + 0.026*"Time" + 0.021*"right now" + 0.020*"nice" + 0.020*"Jets" + 0.019*"Life"
topic #9 (0.033): 0.040*"kids" + 0.029*"will" + 0.018*"work" + 0.018*"don" + 0.015*"god" + 0.015*"down" + 0.014*"do" + 0.013*"talk" + 0.013*"soul" + 0.013*"mind"
topic diff=0.015221, rho=0.116346
PROGRESS: pass 2, at document #2000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.077*"Haiti" + 0.027*"cholera" + 0.021*"UN" + 0.016*"Amazon.com" + 0.015*"death" + 0.015*"election" + 0.014*"Mexico" + 0.014*"Cholera" + 0.013*"Israeli" + 0.013*"Asian"
topic #14 (0.033): 0.046*"hanging" + 0.042*"tcot" + 0.017*"Bush" + 0.013*"Obama" + 0.008*"jobs" + 0.008*"woman" + 0.008*"Alaska" + 0.008*"Ohio" + 0.008*"arrested" + 0.008*"teaparty"
topic #10 (0.033): 0.106*"Como" + 0.070*"un" + 0.068*"para" + 0.039*"como" + 0.034*"es" + 0.028*"en" + 0.025*"la" + 0.018*"dos" + 0.017*"este" + 0.016*"más"
topic #15 (0.033): 0.033*"Home" + 0.029*"sex" + 0.027*"USA" + 0.018*"here" + 0.016*"women" + 0.016*"England" + 0.015*"snow" + 0.014*"girl" + 0.013*"travel" + 0.013*"School"
topic #5 (0.033): 0.030*"NBC" + 0.024*"IM" + 0.021*"student" + 0.020*"fb" + 0.019*"season" + 0.018*"high school" + 0.017*"smile" + 0.015*"Football" + 0.015*"method" + 0.015*"Wisconsin"
topic diff=0.025805, rho=0.115567
PROGRESS: pass 2, at document #4000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.041*"CNN" + 0.035*"Retweet" + 0.032*"NATO" + 0.024*"cancer" + 0.021*"Chicago" + 0.021*"Pope" + 0.019*"Friday" + 0.019*"speech" + 0.016*"Venezuela" + 0.016*"condoms"
topic #15 (0.033): 0.031*"Home" + 0.027*"sex" + 0.026*"USA" + 0.019*"here" + 0.015*"women" + 0.015*"England" + 0.015*"snow" + 0.014*"travel" + 0.013*"girl" + 0.012*"School"
topic #23 (0.033): 0.033*"game" + 0.031*"haha" + 0.026*"football" + 0.025*"show" + 0.022*"RETWEET" + 0.017*"net" + 0.014*"team" + 0.013*"Kanye West" + 0.013*"beautiful" + 0.012*"Apple iPad"
topic #28 (0.033): 0.124*"video" + 0.062*"Video" + 0.056*"YouTube" + 0.027*"Holiday" + 0.024*"Cyber Monday" + 0.022*"music" + 0.017*"Check" + 0.016*"WOW" + 0.015*"Live" + 0.010*"Tonight"
topic #3 (0.033): 0.063*"UK" + 0.035*"London" + 0.020*"police" + 0.018*"students" + 0.017*"protest" + 0.016*"British" + 0.016*"school" + 0.013*"Colombia" + 0.010*"photography" + 0.010*"failure"
topic diff=0.042235, rho=0.115567
PROGRESS: pass 2, at document #6000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.045*"lol" + 0.038*"tweet" + 0.032*"today" + 0.030*"people" + 0.027*"give me" + 0.024*"time" + 0.023*"you" + 0.021*"now" + 0.019*"twitter" + 0.019*"out"
topic #13 (0.033): 0.048*"AP" + 0.022*"DADT" + 0.019*"Breaking News" + 0.018*"add" + 0.011*"Art" + 0.011*"prison" + 0.010*"risk" + 0.010*"study" + 0.010*"Pentagon" + 0.010*"child"
topic #16 (0.033): 0.046*"Black Friday" + 0.022*"Sarah Palin" + 0.018*"HIV" + 0.017*"cash" + 0.017*"read" + 0.017*"chat" + 0.015*"DWTS" + 0.015*"Here" + 0.015*"gay" + 0.013*"Canada"
topic #1 (0.033): 0.061*"TSA" + 0.053*"Obama" + 0.043*"GOP" + 0.025*"ppl" + 0.020*"Senate" + 0.019*"Palin" + 0.017*"Republicans" + 0.016*"Congress" + 0.015*"vote" + 0.014*"House"
topic #12 (0.033): 0.101*"God" + 0.032*"Who" + 0.026*"al" + 0.018*"Celebrity" + 0.016*"help" + 0.014*"TED" + 0.012*"dogs" + 0.012*"Who's" + 0.012*"guy" + 0.012*"fashion"
topic diff=0.055007, rho=0.115567
PROGRESS: pass 2, at document #8000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.091*"WikiLeaks" + 0.082*"Wikileaks" + 0.031*"wikileaks" + 0.028*"cablegate" + 0.022*"US" + 0.018*"Wikipedia" + 0.017*"Wiki Leaks" + 0.014*"Pakistan" + 0.014*"secret" + 0.014*"NYT"
topic #7 (0.033): 0.239*"Twitter" + 0.076*"Facebook" + 0.047*"NYC" + 0.017*"Tweet" + 0.011*"Lady Gaga" + 0.010*"Chinese" + 0.010*"photos" + 0.009*"Internet" + 0.009*"website" + 0.009*"Thx"
topic #21 (0.033): 0.035*"CNN" + 0.033*"NATO" + 0.028*"Retweet" + 0.024*"Pope" + 0.022*"Chicago" + 0.021*"cancer" + 0.018*"Venezuela" + 0.018*"Friday" + 0.017*"condoms" + 0.016*"speech"
topic #29 (0.033): 0.198*"Thanksgiving" + 0.065*"Happy Thanksgiving" + 0.036*"LOL" + 0.034*"family" + 0.033*"holiday" + 0.032*"friends" + 0.025*"Turkey" + 0.020*"thanksgiving" + 0.019*"tonight" + 0.017*"Black Friday"
topic #25 (0.033): 0.042*"lol" + 0.037*"tweet" + 0.035*"today" + 0.030*"people" + 0.026*"give me" + 0.024*"time" + 0.023*"you" + 0.021*"out" + 0.020*"now" + 0.019*"twitter"
topic diff=0.039448, rho=0.115567
PROGRESS: pass 2, at document #10000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.130*"video" + 0.076*"YouTube" + 0.063*"Video" + 0.027*"Holiday" + 0.025*"Cyber Monday" + 0.020*"music" + 0.016*"Live" + 0.014*"Check" + 0.012*"WOW" + 0.009*"Tonight"
topic #15 (0.033): 0.025*"Home" + 0.025*"sex" + 0.024*"USA" + 0.018*"here" + 0.016*"travel" + 0.016*"women" + 0.014*"snow" + 0.014*"England" + 0.013*"film" + 0.012*"girl"
topic #7 (0.033): 0.234*"Twitter" + 0.076*"Facebook" + 0.045*"NYC" + 0.017*"Tweet" + 0.011*"Internet" + 0.011*"Chinese" + 0.011*"Lady Gaga" + 0.010*"photos" + 0.010*"website" + 0.009*"Thx"
topic #9 (0.033): 0.033*"kids" + 0.028*"will" + 0.019*"work" + 0.014*"don" + 0.014*"do" + 0.012*"em" + 0.012*"VIDEO" + 0.011*"talk" + 0.011*"down" + 0.011*"MSNBC"
topic #18 (0.033): 0.091*"WikiLeaks" + 0.083*"Wikileaks" + 0.034*"wikileaks" + 0.027*"cablegate" + 0.022*"US" + 0.020*"Wiki Leaks" + 0.017*"Wikipedia" + 0.015*"Pakistan" + 0.015*"NYT" + 0.014*"Guardian"
topic diff=0.040872, rho=0.115567
PROGRESS: pass 2, at document #12000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.028*"News" + 0.022*"bit.ly" + 0.019*"NFL" + 0.017*"Texas" + 0.015*"Tom DeLay" + 0.015*"English" + 0.015*"Report" + 0.014*"Boston" + 0.014*"You" + 0.012*"songs"
topic #19 (0.033): 0.046*"Leslie Nielsen" + 0.038*"Love" + 0.036*"Today" + 0.025*"Music" + 0.024*"Time" + 0.023*"need" + 0.018*"Life" + 0.017*"nice" + 0.017*"right now" + 0.017*"Actor"
topic #27 (0.033): 0.082*"blog" + 0.039*"Back" + 0.034*"post" + 0.021*"Amazon" + 0.018*"release" + 0.017*"iOS" + 0.016*"DVD" + 0.016*"web" + 0.015*"Education" + 0.013*"Oprah"
topic #14 (0.033): 0.053*"tcot" + 0.030*"hanging" + 0.013*"Bush" + 0.013*"Obama" + 0.009*"woman" + 0.009*"Police" + 0.009*"teaparty" + 0.008*"Alaska" + 0.008*"arrested" + 0.008*"Ohio"
topic #28 (0.033): 0.128*"video" + 0.073*"YouTube" + 0.064*"Video" + 0.026*"Holiday" + 0.025*"Cyber Monday" + 0.020*"music" + 0.016*"Live" + 0.014*"Check" + 0.011*"WOW" + 0.009*"Tonight"
topic diff=0.042666, rho=0.115567
PROGRESS: pass 2, at document #14000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.041*"lol" + 0.036*"today" + 0.034*"tweet" + 0.031*"people" + 0.024*"time" + 0.023*"you" + 0.021*"give me" + 0.021*"out" + 0.019*"twitter" + 0.019*"now"
topic #6 (0.033): 0.016*"U.S" + 0.012*"jajaja" + 0.012*"World" + 0.009*"Will" + 0.009*"Health" + 0.009*"News" + 0.009*"China" + 0.009*"New York" + 0.009*"Business" + 0.008*"Green"
topic #12 (0.033): 0.084*"God" + 0.033*"Who" + 0.028*"al" + 0.015*"NPR" + 0.015*"help" + 0.014*"Celebrity" + 0.012*"fashion" + 0.011*"local" + 0.011*"Who's" + 0.011*"left"
topic #9 (0.033): 0.031*"kids" + 0.028*"will" + 0.020*"work" + 0.014*"don" + 0.013*"do" + 0.012*"talk" + 0.012*"MSNBC" + 0.011*"VIDEO" + 0.011*"play" + 0.011*"em"
topic #20 (0.033): 0.044*"live" + 0.036*"TV" + 0.032*"Watch" + 0.031*"world" + 0.027*"Check it" + 0.024*"Check it out" + 0.022*"climate" + 0.021*"Hope" + 0.021*"interview" + 0.020*"COP16"
topic diff=0.048042, rho=0.115567
PROGRESS: pass 2, at document #16000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.040*"AP" + 0.021*"DADT" + 0.014*"Breaking News" + 0.012*"add" + 0.011*"study" + 0.011*"risk" + 0.010*"child" + 0.010*"Art" + 0.010*"report" + 0.010*"Pentagon"
topic #2 (0.033): 0.028*"News" + 0.020*"bit.ly" + 0.017*"Texas" + 0.017*"NFL" + 0.016*"Tom DeLay" + 0.014*"Report" + 0.014*"Tory" + 0.014*"English" + 0.013*"You" + 0.013*"Boston"
topic #5 (0.033): 0.026*"student" + 0.025*"NBC" + 0.019*"season" + 0.019*"fb" + 0.019*"high school" + 0.017*"Wisconsin" + 0.013*"IM" + 0.013*"smile" + 0.013*"NY" + 0.012*"Willie Nelson"
topic #16 (0.033): 0.049*"Black Friday" + 0.024*"Sarah Palin" + 0.022*"read" + 0.022*"newspaper" + 0.020*"HIV" + 0.015*"gay" + 0.015*"Here" + 0.015*"Canada" + 0.014*"DWTS" + 0.013*"cash"
topic #21 (0.033): 0.035*"CNN" + 0.031*"NATO" + 0.029*"Pope" + 0.026*"Chicago" + 0.021*"Retweet" + 0.020*"cancer" + 0.019*"Venezuela" + 0.018*"Friday" + 0.017*"condoms" + 0.017*"education"
topic diff=0.025520, rho=0.115567
PROGRESS: pass 2, at document #18000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.201*"Thanksgiving" + 0.061*"Happy Thanksgiving" + 0.040*"LOL" + 0.033*"holiday" + 0.033*"family" + 0.028*"friends" + 0.027*"Turkey" + 0.019*"tonight" + 0.019*"thanksgiving" + 0.016*"Black Friday"
topic #28 (0.033): 0.124*"video" + 0.071*"YouTube" + 0.064*"Video" + 0.028*"Holiday" + 0.026*"Cyber Monday" + 0.019*"music" + 0.017*"Check" + 0.014*"Live" + 0.009*"WOW" + 0.008*"Tonight"
topic #22 (0.033): 0.055*"Blog" + 0.024*"week" + 0.024*"We" + 0.023*"re" + 0.019*"book" + 0.016*"basketball" + 0.015*"believe" + 0.014*"change" + 0.013*"WTF" + 0.012*"Post"
topic #2 (0.033): 0.028*"News" + 0.019*"bit.ly" + 0.018*"Texas" + 0.016*"Tom DeLay" + 0.016*"NFL" + 0.014*"Report" + 0.014*"English" + 0.014*"Tory" + 0.013*"You" + 0.013*"Boston"
topic #26 (0.033): 0.033*"news" + 0.029*"India" + 0.022*"New Zealand" + 0.015*"business" + 0.015*"mine" + 0.013*"dead" + 0.013*"killed" + 0.011*"Australia" + 0.011*"police" + 0.010*"NASA"
topic diff=0.041954, rho=0.115567
bound: at document #0
-23.062 per-word bound, 8757300.7 perplexity estimate based on a held-out corpus of 2000 documents with 20435 words
PROGRESS: pass 2, at document #20000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.084*"Haiti" + 0.027*"UN" + 0.027*"cholera" + 0.021*"election" + 0.015*"Cholera" + 0.014*"Mexico" + 0.013*"Gaza" + 0.012*"Israeli" + 0.011*"earthquake" + 0.011*"death"
topic #10 (0.033): 0.074*"para" + 0.069*"un" + 0.056*"Como" + 0.046*"como" + 0.027*"es" + 0.026*"en" + 0.025*"la" + 0.017*"eso" + 0.017*"hoy" + 0.017*"dos"
topic #1 (0.033): 0.077*"TSA" + 0.054*"Obama" + 0.040*"GOP" + 0.023*"Senate" + 0.019*"Palin" + 0.017*"ppl" + 0.017*"House" + 0.015*"Congress" + 0.015*"Republicans" + 0.014*"vote"
topic #28 (0.033): 0.122*"video" + 0.068*"YouTube" + 0.062*"Video" + 0.030*"Holiday" + 0.027*"Cyber Monday" + 0.019*"music" + 0.018*"Check" + 0.013*"Live" + 0.009*"Windows" + 0.009*"WOW"
topic #26 (0.033): 0.037*"news" + 0.029*"India" + 0.022*"New Zealand" + 0.015*"mine" + 0.014*"business" + 0.014*"dead" + 0.013*"killed" + 0.011*"police" + 0.011*"energy" + 0.011*"Australia"
topic diff=0.048947, rho=0.115567
PROGRESS: pass 2, at document #22000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.053*"UK" + 0.032*"London" + 0.025*"police" + 0.019*"students" + 0.017*"protest" + 0.016*"British" + 0.013*"school" + 0.013*"Police" + 0.012*"Cuba" + 0.011*"demo 2010"
topic #27 (0.033): 0.082*"blog" + 0.035*"post" + 0.028*"Back" + 0.021*"Amazon" + 0.017*"iOS" + 0.017*"release" + 0.017*"DVD" + 0.016*"Oprah" + 0.014*"web" + 0.013*"PC"
topic #25 (0.033): 0.048*"lol" + 0.041*"today" + 0.030*"people" + 0.029*"tweet" + 0.025*"time" + 0.024*"out" + 0.021*"you" + 0.018*"can" + 0.017*"who" + 0.017*"now"
topic #5 (0.033): 0.029*"student" + 0.024*"NBC" + 0.020*"season" + 0.019*"high school" + 0.017*"Wisconsin" + 0.016*"hostage" + 0.015*"fb" + 0.013*"Willie Nelson" + 0.012*"NY" + 0.012*"hostages"
topic #13 (0.033): 0.040*"AP" + 0.021*"DADT" + 0.012*"risk" + 0.012*"study" + 0.011*"report" + 0.011*"Breaking News" + 0.010*"child" + 0.010*"Pentagon" + 0.010*"add" + 0.009*"law"
topic diff=0.018822, rho=0.115567
PROGRESS: pass 2, at document #24000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.032*"U.S" + 0.025*"President" + 0.021*"Obama" + 0.019*"White House" + 0.015*"military" + 0.014*"President Obama" + 0.014*"S. Korea" + 0.012*"Seoul" + 0.012*"Sen" + 0.010*"Cancun"
topic #25 (0.033): 0.045*"lol" + 0.043*"today" + 0.030*"people" + 0.027*"tweet" + 0.025*"time" + 0.025*"out" + 0.021*"you" + 0.017*"can" + 0.017*"who" + 0.017*"love"
topic #27 (0.033): 0.081*"blog" + 0.036*"post" + 0.025*"Back" + 0.022*"Amazon" + 0.017*"release" + 0.016*"iOS" + 0.016*"DVD" + 0.016*"Oprah" + 0.015*"web" + 0.013*"PC"
topic #29 (0.033): 0.213*"Thanksgiving" + 0.063*"Happy Thanksgiving" + 0.038*"LOL" + 0.035*"family" + 0.034*"holiday" + 0.027*"Turkey" + 0.026*"friends" + 0.020*"tonight" + 0.020*"thanksgiving" + 0.015*"Black Friday"
topic #14 (0.033): 0.062*"tcot" + 0.018*"AM" + 0.017*"hanging" + 0.015*"Bush" + 0.014*"Obama" + 0.009*"Alaska" + 0.009*"woman" + 0.009*"teaparty" + 0.009*"jobs" + 0.009*"arrested"
topic diff=0.014988, rho=0.115567
PROGRESS: pass 2, at document #26000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.052*"Blog" + 0.024*"We" + 0.024*"week" + 0.023*"re" + 0.020*"book" + 0.016*"basketball" + 0.014*"believe" + 0.013*"learn" + 0.012*"Post" + 0.012*"see"
topic #19 (0.033): 0.050*"Leslie Nielsen" + 0.039*"Today" + 0.035*"Love" + 0.023*"Time" + 0.020*"Music" + 0.020*"Life" + 0.019*"right now" + 0.019*"nice" + 0.018*"need" + 0.015*"Actor"
topic #20 (0.033): 0.041*"live" + 0.039*"TV" + 0.032*"Watch" + 0.031*"world" + 0.022*"interview" + 0.022*"Check it" + 0.022*"Hope" + 0.022*"climate" + 0.020*"families" + 0.020*"Check it out"
topic #12 (0.033): 0.066*"God" + 0.034*"Who" + 0.019*"NPR" + 0.019*"al" + 0.017*"local" + 0.013*"help" + 0.013*"America" + 0.012*"Who's" + 0.012*"left" + 0.011*"Lanvin"
topic #23 (0.033): 0.028*"game" + 0.027*"show" + 0.024*"football" + 0.023*"haha" + 0.016*"Kanye West" + 0.013*"team" + 0.012*"net" + 0.011*"win" + 0.011*"beautiful" + 0.011*"Disney"
topic diff=0.012915, rho=0.115567
PROGRESS: pass 2, at document #28000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.055*"Black Friday" + 0.027*"Sarah Palin" + 0.020*"HIV" + 0.019*"read" + 0.017*"newspaper" + 0.016*"Here" + 0.016*"Canada" + 0.014*"DWTS" + 0.014*"sales" + 0.014*"gay"
topic #26 (0.033): 0.041*"India" + 0.038*"news" + 0.020*"New Zealand" + 0.018*"PM" + 0.014*"mine" + 0.014*"dead" + 0.013*"business" + 0.013*"killed" + 0.011*"energy" + 0.011*"Australia"
topic #2 (0.033): 0.035*"News" + 0.024*"NFL" + 0.022*"Texas" + 0.020*"Tom DeLay" + 0.015*"You" + 0.015*"bit.ly" + 0.012*"English" + 0.011*"Report" + 0.011*"Wall Street" + 0.011*"Tory"
topic #25 (0.033): 0.043*"today" + 0.041*"lol" + 0.029*"people" + 0.025*"out" + 0.025*"time" + 0.024*"tweet" + 0.022*"you" + 0.018*"can" + 0.018*"who" + 0.017*"us"
topic #24 (0.033): 0.087*"who" + 0.020*"Arizona" + 0.020*"Oh" + 0.019*"Nice" + 0.018*"Yes" + 0.016*"media" + 0.015*"me a" + 0.015*"I'm going" + 0.014*"college" + 0.013*"right"
topic diff=0.020011, rho=0.115567
PROGRESS: pass 2, at document #30000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.212*"Thanksgiving" + 0.058*"Happy Thanksgiving" + 0.040*"LOL" + 0.034*"family" + 0.034*"holiday" + 0.028*"friends" + 0.026*"Turkey" + 0.024*"thanksgiving" + 0.020*"tonight" + 0.014*"Harry Potter"
topic #0 (0.033): 0.075*"Haiti" + 0.026*"UN" + 0.022*"cholera" + 0.021*"election" + 0.014*"Israeli" + 0.013*"Mexico" + 0.013*"earthquake" + 0.012*"Cholera" + 0.012*"Gaza" + 0.012*"Asian"
topic #26 (0.033): 0.039*"India" + 0.036*"news" + 0.021*"New Zealand" + 0.016*"PM" + 0.014*"dead" + 0.014*"mine" + 0.013*"business" + 0.012*"killed" + 0.011*"Australia" + 0.011*"energy"
topic #12 (0.033): 0.066*"God" + 0.035*"Who" + 0.019*"NPR" + 0.016*"local" + 0.016*"al" + 0.014*"America" + 0.013*"Who's" + 0.013*"help" + 0.012*"DREAM Act" + 0.012*"left"
topic #27 (0.033): 0.082*"blog" + 0.037*"post" + 0.024*"Amazon" + 0.019*"cc" + 0.018*"Back" + 0.018*"web" + 0.017*"release" + 0.017*"DVD" + 0.014*"iOS" + 0.014*"Oprah"
topic diff=0.048891, rho=0.115567
PROGRESS: pass 2, at document #32000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.052*"Leslie Nielsen" + 0.044*"Today" + 0.032*"Love" + 0.021*"Life" + 0.021*"Time" + 0.021*"nice" + 0.019*"Music" + 0.019*"right now" + 0.016*"need" + 0.015*"white"
topic #24 (0.033): 0.093*"who" + 0.022*"Arizona" + 0.019*"Nice" + 0.019*"Oh" + 0.016*"Yes" + 0.015*"media" + 0.015*"me a" + 0.014*"right" + 0.013*"a man" + 0.013*"I'm going"
topic #8 (0.033): 0.046*"Iran" + 0.042*"US" + 0.030*"Israel" + 0.027*"Afghanistan" + 0.020*"to win" + 0.015*"American" + 0.014*"Egypt" + 0.012*"Iraq" + 0.011*"Afghan" + 0.010*"Rio"
topic #12 (0.033): 0.066*"God" + 0.035*"Who" + 0.020*"NPR" + 0.015*"local" + 0.015*"al" + 0.014*"America" + 0.013*"Who's" + 0.012*"DREAM Act" + 0.012*"help" + 0.011*"left"
topic #9 (0.033): 0.031*"will" + 0.026*"kids" + 0.021*"work" + 0.015*"children" + 0.013*"do" + 0.013*"VIDEO" + 0.012*"don" + 0.012*"MSNBC" + 0.012*"play" + 0.012*"talk"
topic diff=0.028896, rho=0.115567
PROGRESS: pass 2, at document #34000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.047*"lol" + 0.042*"today" + 0.028*"people" + 0.025*"time" + 0.024*"out" + 0.021*"you" + 0.021*"tweet" + 0.018*"who" + 0.018*"can" + 0.018*"love"
topic #9 (0.033): 0.031*"will" + 0.025*"kids" + 0.021*"work" + 0.013*"children" + 0.013*"do" + 0.013*"don" + 0.012*"VIDEO" + 0.012*"talk" + 0.011*"MSNBC" + 0.011*"play"
topic #18 (0.033): 0.092*"WikiLeaks" + 0.087*"Wikileaks" + 0.038*"wikileaks" + 0.026*"cablegate" + 0.024*"US" + 0.020*"Wiki Leaks" + 0.017*"Guardian" + 0.017*"NYT" + 0.014*"cables" + 0.014*"documents"
topic #5 (0.033): 0.028*"student" + 0.022*"high school" + 0.021*"NBC" + 0.021*"season" + 0.017*"Wisconsin" + 0.016*"hostage" + 0.013*"Good" + 0.013*"hostages" + 0.013*"fb" + 0.013*"record"
topic #26 (0.033): 0.040*"India" + 0.033*"news" + 0.020*"New Zealand" + 0.015*"PM" + 0.015*"dead" + 0.014*"mine" + 0.014*"killed" + 0.013*"business" + 0.011*"Australia" + 0.011*"energy"
topic diff=0.033151, rho=0.115567
PROGRESS: pass 2, at document #36000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.031*"football" + 0.026*"game" + 0.025*"show" + 0.024*"haha" + 0.016*"team" + 0.015*"Kanye West" + 0.012*"beautiful" + 0.011*"Film" + 0.011*"turkeys" + 0.011*"Dallas"
topic #10 (0.033): 0.076*"para" + 0.061*"un" + 0.042*"como" + 0.036*"Como" + 0.027*"dos" + 0.025*"en" + 0.020*"la" + 0.020*"es" + 0.019*"pour" + 0.016*"se"
topic #2 (0.033): 0.043*"News" + 0.021*"NFL" + 0.020*"Texas" + 0.016*"You" + 0.015*"Tom DeLay" + 0.015*"English" + 0.012*"bit.ly" + 0.012*"Boston" + 0.011*"EUA" + 0.011*"Report"
topic #6 (0.033): 0.014*"U.S" + 0.014*"World" + 0.013*"Green" + 0.013*"News" + 0.011*"Will" + 0.009*"Study" + 0.009*"New York" + 0.009*"China" + 0.008*"Business" + 0.008*"Health"
topic #21 (0.033): 0.064*"CNN" + 0.028*"Pope" + 0.028*"Chicago" + 0.024*"NATO" + 0.023*"cancer" + 0.018*"Friday" + 0.018*"Retweet" + 0.016*"Photo" + 0.016*"up" + 0.015*"education"
topic diff=0.013704, rho=0.115567
PROGRESS: pass 2, at document #38000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.026*"student" + 0.021*"high school" + 0.020*"NBC" + 0.019*"season" + 0.019*"Wisconsin" + 0.017*"hostage" + 0.014*"Good" + 0.014*"fb" + 0.012*"smile" + 0.012*"hostages"
topic #13 (0.033): 0.051*"AP" + 0.015*"DADT" + 0.015*"study" + 0.011*"report" + 0.011*"risk" + 0.010*"child" + 0.009*"Breaking News" + 0.009*"prison" + 0.009*"Art" + 0.009*"Pentagon"
topic #25 (0.033): 0.047*"today" + 0.041*"lol" + 0.028*"people" + 0.027*"out" + 0.025*"time" + 0.022*"you" + 0.021*"tweet" + 0.018*"who" + 0.017*"love" + 0.017*"can"
topic #18 (0.033): 0.097*"WikiLeaks" + 0.088*"Wikileaks" + 0.038*"wikileaks" + 0.029*"cablegate" + 0.025*"US" + 0.018*"Wiki Leaks" + 0.016*"NYT" + 0.016*"Guardian" + 0.014*"documents" + 0.013*"cables"
topic #21 (0.033): 0.060*"CNN" + 0.029*"Chicago" + 0.027*"Pope" + 0.026*"NATO" + 0.023*"cancer" + 0.019*"Friday" + 0.017*"Retweet" + 0.015*"up" + 0.015*"Photo" + 0.014*"education"
topic diff=0.039748, rho=0.115567
bound: at document #0
-23.172 per-word bound, 9451055.5 perplexity estimate based on a held-out corpus of 2000 documents with 26137 words
PROGRESS: pass 2, at document #40000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.056*"Black Friday" + 0.025*"Sarah Palin" + 0.021*"HIV" + 0.019*"Here" + 0.019*"read" + 0.016*"newspaper" + 0.015*"gay" + 0.015*"Canada" + 0.014*"DWTS" + 0.012*"allies"
topic #24 (0.033): 0.093*"who" + 0.022*"Nice" + 0.019*"Oh" + 0.019*"Arizona" + 0.017*"Yes" + 0.017*"me a" + 0.015*"I'm going" + 0.015*"a man" + 0.014*"right" + 0.014*"media"
topic #14 (0.033): 0.062*"tcot" + 0.014*"sgp" + 0.014*"Obama" + 0.013*"Bush" + 0.011*"teaparty" + 0.011*"woman" + 0.010*"AM" + 0.010*"arrested" + 0.009*"FBI" + 0.009*"Police"
topic #29 (0.033): 0.210*"Thanksgiving" + 0.058*"Happy Thanksgiving" + 0.035*"LOL" + 0.035*"holiday" + 0.034*"family" + 0.028*"friends" + 0.027*"thanksgiving" + 0.026*"Turkey" + 0.023*"tonight" + 0.014*"Harry Potter"
topic #2 (0.033): 0.042*"News" + 0.022*"NFL" + 0.020*"Texas" + 0.016*"English" + 0.015*"Tom DeLay" + 0.014*"You" + 0.013*"Report" + 0.013*"songs" + 0.012*"Boston" + 0.011*"bit.ly"
topic diff=0.028106, rho=0.115567
PROGRESS: pass 2, at document #42000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.076*"para" + 0.060*"un" + 0.038*"como" + 0.030*"Como" + 0.026*"dos" + 0.024*"en" + 0.020*"la" + 0.019*"es" + 0.019*"pour" + 0.016*"se"
topic #12 (0.033): 0.069*"God" + 0.037*"Who" + 0.017*"NPR" + 0.015*"Celebrity" + 0.015*"left" + 0.014*"local" + 0.014*"Who's" + 0.014*"America" + 0.013*"guy" + 0.012*"help"
topic #18 (0.033): 0.093*"WikiLeaks" + 0.088*"Wikileaks" + 0.039*"wikileaks" + 0.029*"cablegate" + 0.026*"US" + 0.019*"Guardian" + 0.018*"Wiki Leaks" + 0.015*"NYT" + 0.014*"documents" + 0.012*"cables"
topic #11 (0.033): 0.062*"via" + 0.045*"iPad" + 0.044*"iPhone" + 0.038*"Google" + 0.035*"Apple" + 0.030*"Facebook" + 0.022*"Social Media" + 0.019*"app" + 0.019*"Christmas" + 0.017*"Android"
topic #22 (0.033): 0.063*"Blog" + 0.023*"We" + 0.022*"book" + 0.022*"re" + 0.017*"week" + 0.015*"food" + 0.014*"basketball" + 0.013*"Post" + 0.013*"WTF" + 0.012*"change"
topic diff=0.031884, rho=0.115567
PROGRESS: pass 2, at document #44000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.075*"para" + 0.059*"un" + 0.037*"como" + 0.029*"Como" + 0.026*"dos" + 0.025*"en" + 0.020*"la" + 0.018*"es" + 0.018*"pour" + 0.015*"ll"
topic #11 (0.033): 0.060*"via" + 0.045*"iPad" + 0.042*"iPhone" + 0.039*"Google" + 0.038*"Apple" + 0.030*"Facebook" + 0.022*"Social Media" + 0.019*"Christmas" + 0.019*"Android" + 0.018*"app"
topic #2 (0.033): 0.040*"News" + 0.026*"LMAO" + 0.020*"NFL" + 0.018*"Texas" + 0.015*"English" + 0.014*"Tom DeLay" + 0.013*"You" + 0.011*"Report" + 0.011*"songs" + 0.011*"bit.ly"
topic #19 (0.033): 0.050*"Leslie Nielsen" + 0.046*"Today" + 0.032*"Love" + 0.023*"Time" + 0.021*"right now" + 0.020*"Life" + 0.020*"nice" + 0.017*"Music" + 0.017*"need" + 0.015*"white"
topic #24 (0.033): 0.095*"who" + 0.021*"Nice" + 0.020*"Arizona" + 0.019*"Oh" + 0.018*"Yes" + 0.017*"me a" + 0.016*"media" + 0.016*"I'm going" + 0.015*"a man" + 0.014*"right"
topic diff=0.035572, rho=0.115567
PROGRESS: pass 2, at document #46000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.041*"AP" + 0.018*"DADT" + 0.013*"study" + 0.011*"risk" + 0.010*"child" + 0.010*"report" + 0.010*"Pentagon" + 0.009*"Debt" + 0.009*"prison" + 0.008*"Art"
topic #12 (0.033): 0.065*"God" + 0.041*"Who" + 0.020*"NPR" + 0.015*"left" + 0.014*"Who's" + 0.014*"local" + 0.013*"Celebrity" + 0.012*"America" + 0.012*"help" + 0.011*"laptop"
topic #16 (0.033): 0.051*"Black Friday" + 0.027*"Sarah Palin" + 0.021*"HIV" + 0.020*"read" + 0.018*"Here" + 0.017*"DWTS" + 0.017*"Canada" + 0.017*"Bristol" + 0.013*"gay" + 0.013*"newspaper"
topic #18 (0.033): 0.107*"WikiLeaks" + 0.087*"Wikileaks" + 0.037*"NYT" + 0.032*"wikileaks" + 0.026*"cablegate" + 0.025*"US" + 0.018*"Opinion" + 0.016*"Guardian" + 0.016*"Wiki Leaks" + 0.014*"documents"
topic #24 (0.033): 0.087*"who" + 0.047*"twit" + 0.021*"Arizona" + 0.020*"Nice" + 0.019*"Oh" + 0.019*"Yes" + 0.017*"me a" + 0.016*"I'm going" + 0.015*"a man" + 0.014*"media"
topic diff=0.111166, rho=0.115567
PROGRESS: pass 2, at document #48000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.041*"live" + 0.039*"TV" + 0.035*"world" + 0.029*"Watch" + 0.024*"interview" + 0.021*"Hope" + 0.021*"families" + 0.021*"climate" + 0.018*"Check it" + 0.016*"Check it out"
topic #6 (0.033): 0.015*"NYT" + 0.014*"U.S" + 0.012*"World" + 0.012*"Will" + 0.011*"News" + 0.010*"Study" + 0.010*"Economist" + 0.009*"Green" + 0.009*"Tea Party" + 0.009*"New York"
topic #7 (0.033): 0.166*"Twitter" + 0.080*"Facebook" + 0.033*"NYC" + 0.022*"Tweet" + 0.016*"Lady Gaga" + 0.016*"photos" + 0.013*"Chinese" + 0.012*"Temperature" + 0.010*"social media" + 0.010*"Internet"
topic #13 (0.033): 0.042*"AP" + 0.023*"DADT" + 0.013*"study" + 0.011*"Pentagon" + 0.011*"risk" + 0.010*"child" + 0.010*"report" + 0.009*"Portland" + 0.009*"prison" + 0.009*"Debt"
topic #23 (0.033): 0.046*"haha" + 0.031*"game" + 0.030*"football" + 0.029*"show" + 0.015*"Kanye West" + 0.014*"NBA" + 0.014*"team" + 0.012*"Dallas" + 0.012*"beautiful" + 0.012*"turkeys"
topic diff=0.021393, rho=0.115567
PROGRESS: pass 2, at document #50000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #7 (0.033): 0.161*"Twitter" + 0.079*"Facebook" + 0.034*"NYC" + 0.022*"Tweet" + 0.017*"photos" + 0.015*"Lady Gaga" + 0.013*"Chinese" + 0.011*"People" + 0.011*"Temperature" + 0.010*"social media"
topic #12 (0.033): 0.066*"God" + 0.039*"Who" + 0.025*"GOD" + 0.021*"NPR" + 0.014*"Who's" + 0.013*"left" + 0.013*"local" + 0.012*"America" + 0.011*"guy" + 0.011*"Celebrity"
topic #15 (0.033): 0.029*"snow" + 0.029*"USA" + 0.020*"women" + 0.020*"travel" + 0.019*"here" + 0.014*"England" + 0.013*"Thailand" + 0.013*"sex" + 0.013*"film" + 0.011*"Paris"
topic #5 (0.033): 0.026*"season" + 0.024*"student" + 0.020*"NBC" + 0.019*"high school" + 0.016*"Wisconsin" + 0.015*"fb" + 0.015*"Good" + 0.014*"record" + 0.014*"Los Angeles" + 0.013*"hostage"
topic #10 (0.033): 0.075*"para" + 0.053*"un" + 0.035*"como" + 0.029*"dos" + 0.024*"Como" + 0.021*"en" + 0.020*"pour" + 0.018*"la" + 0.017*"ll" + 0.017*"es"
topic diff=0.027293, rho=0.115567
PROGRESS: pass 2, at document #52000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.088*"who" + 0.034*"twit" + 0.021*"Oh" + 0.021*"me a" + 0.020*"Arizona" + 0.019*"Nice" + 0.017*"Yes" + 0.016*"a man" + 0.015*"Bears" + 0.015*"right"
topic #26 (0.033): 0.035*"India" + 0.028*"news" + 0.020*"New Zealand" + 0.014*"mine" + 0.013*"dead" + 0.013*"business" + 0.013*"PM" + 0.012*"killed" + 0.011*"energy" + 0.010*"NZ"
topic #4 (0.033): 0.031*"U.S" + 0.025*"President" + 0.023*"Obama" + 0.019*"White House" + 0.016*"President Obama" + 0.014*"TIME" + 0.012*"POTUS" + 0.012*"military" + 0.012*"Sen" + 0.011*"Don"
topic #12 (0.033): 0.078*"God" + 0.038*"Who" + 0.023*"GOD" + 0.020*"NPR" + 0.014*"Who's" + 0.014*"left" + 0.013*"local" + 0.012*"guy" + 0.012*"help" + 0.012*"America"
topic #1 (0.033): 0.070*"TSA" + 0.062*"GOP" + 0.056*"Obama" + 0.021*"ppl" + 0.021*"Senate" + 0.020*"Palin" + 0.016*"Republicans" + 0.016*"vote" + 0.015*"Congress" + 0.014*"House"
topic diff=0.042464, rho=0.115567
PROGRESS: pass 2, at document #54000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.025*"season" + 0.024*"student" + 0.020*"NBC" + 0.018*"high school" + 0.016*"Wisconsin" + 0.015*"Good" + 0.014*"record" + 0.014*"fb" + 0.014*"Football" + 0.014*"Los Angeles"
topic #19 (0.033): 0.048*"Leslie Nielsen" + 0.046*"Today" + 0.032*"Love" + 0.023*"Music" + 0.021*"right now" + 0.021*"Time" + 0.019*"nice" + 0.018*"Life" + 0.017*"Washington" + 0.016*"white"
topic #13 (0.033): 0.043*"AP" + 0.024*"DADT" + 0.013*"study" + 0.011*"Pentagon" + 0.011*"child" + 0.011*"risk" + 0.010*"report" + 0.009*"debate" + 0.009*"Portland" + 0.009*"weather"
topic #20 (0.033): 0.042*"TV" + 0.041*"live" + 0.032*"world" + 0.028*"Watch" + 0.027*"interview" + 0.022*"Hope" + 0.020*"families" + 0.020*"climate" + 0.019*"Check it" + 0.018*"Radio"
topic #21 (0.033): 0.052*"CNN" + 0.036*"Chicago" + 0.031*"Retweet" + 0.026*"Pope" + 0.024*"Friday" + 0.023*"cancer" + 0.021*"NATO" + 0.016*"Illinois" + 0.016*"education" + 0.014*"Nation"
topic diff=0.018070, rho=0.115567
PROGRESS: pass 2, at document #56000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.030*"snow" + 0.026*"USA" + 0.023*"here" + 0.020*"women" + 0.018*"travel" + 0.014*"Paris" + 0.014*"film" + 0.013*"England" + 0.013*"sex" + 0.011*"Thailand"
topic #24 (0.033): 0.090*"who" + 0.028*"twit" + 0.021*"me a" + 0.020*"Oh" + 0.020*"Nice" + 0.019*"Arizona" + 0.016*"Yes" + 0.016*"a man" + 0.015*"I'm going" + 0.014*"fat"
topic #1 (0.033): 0.072*"TSA" + 0.058*"GOP" + 0.053*"Obama" + 0.023*"ppl" + 0.021*"Senate" + 0.019*"Palin" + 0.015*"Republicans" + 0.015*"vote" + 0.014*"House" + 0.014*"Congress"
topic #11 (0.033): 0.059*"via" + 0.047*"iPad" + 0.044*"iPhone" + 0.035*"Google" + 0.034*"Apple" + 0.026*"Facebook" + 0.019*"Christmas" + 0.019*"Social Media" + 0.019*"iTunes" + 0.018*"app"
topic #27 (0.033): 0.079*"blog" + 0.033*"post" + 0.025*"Blogger" + 0.022*"Amazon" + 0.019*"DVD" + 0.016*"release" + 0.016*"Win" + 0.015*"Oprah" + 0.015*"web" + 0.011*"PC"
topic diff=0.031775, rho=0.115567
PROGRESS: pass 2, at document #58000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.072*"TSA" + 0.054*"GOP" + 0.052*"Obama" + 0.025*"ppl" + 0.021*"Senate" + 0.019*"Palin" + 0.015*"vote" + 0.015*"Republicans" + 0.014*"House" + 0.014*"Congress"
topic #28 (0.033): 0.183*"YouTube" + 0.173*"video" + 0.048*"Video" + 0.025*"Holiday" + 0.020*"Cyber Monday" + 0.019*"music" + 0.014*"Live" + 0.013*"Check" + 0.011*"Tonight" + 0.008*"Photos"
topic #23 (0.033): 0.045*"haha" + 0.035*"game" + 0.035*"show" + 0.032*"football" + 0.017*"Kanye West" + 0.014*"team" + 0.014*"beautiful" + 0.013*"New Orleans" + 0.012*"NBA" + 0.012*"turkeys"
topic #20 (0.033): 0.041*"live" + 0.040*"TV" + 0.034*"Watch" + 0.031*"world" + 0.025*"interview" + 0.021*"Hope" + 0.020*"climate" + 0.020*"families" + 0.019*"Check it" + 0.017*"Radio"
topic #19 (0.033): 0.048*"Leslie Nielsen" + 0.047*"Today" + 0.038*"Love" + 0.024*"right now" + 0.022*"Music" + 0.021*"Time" + 0.020*"nice" + 0.019*"white" + 0.017*"Life" + 0.015*"Washington"
topic diff=0.020514, rho=0.115567
bound: at document #0
-24.504 per-word bound, 23793047.9 perplexity estimate based on a held-out corpus of 2000 documents with 40495 words
PROGRESS: pass 2, at document #60000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.014*"U.S" + 0.012*"World" + 0.011*"NYT" + 0.010*"Will" + 0.010*"Study" + 0.010*"News" + 0.009*"Women" + 0.009*"New York" + 0.008*"Tea Party" + 0.008*"Green"
topic #23 (0.033): 0.042*"haha" + 0.036*"football" + 0.035*"game" + 0.033*"show" + 0.016*"Kanye West" + 0.014*"OK" + 0.014*"team" + 0.014*"hair" + 0.013*"net" + 0.013*"beautiful"
topic #13 (0.033): 0.041*"AP" + 0.020*"DADT" + 0.015*"study" + 0.013*"child" + 0.012*"risk" + 0.011*"report" + 0.010*"Pentagon" + 0.010*"weather" + 0.010*"friend" + 0.009*"prison"
topic #9 (0.033): 0.030*"will" + 0.020*"work" + 0.019*"kids" + 0.014*"do" + 0.013*"em" + 0.012*"play" + 0.012*"music" + 0.012*"don" + 0.012*"children" + 0.012*"first"
topic #28 (0.033): 0.168*"video" + 0.166*"YouTube" + 0.048*"Video" + 0.026*"Holiday" + 0.021*"Cyber Monday" + 0.020*"music" + 0.015*"Live" + 0.013*"Check" + 0.011*"Tonight" + 0.008*"Photos"
topic diff=0.023047, rho=0.115567
PROGRESS: pass 2, at document #62000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.079*"blog" + 0.035*"post" + 0.024*"Amazon" + 0.023*"DVD" + 0.018*"Blogger" + 0.016*"release" + 0.016*"Oprah" + 0.014*"Win" + 0.014*"web" + 0.011*"check it"
topic #10 (0.033): 0.089*"para" + 0.045*"un" + 0.044*"como" + 0.037*"dos" + 0.023*"en" + 0.019*"pour" + 0.019*"la" + 0.017*"ll" + 0.017*"Como" + 0.016*"se"
topic #15 (0.033): 0.027*"snow" + 0.024*"here" + 0.023*"USA" + 0.022*"women" + 0.018*"travel" + 0.013*"film" + 0.013*"sex" + 0.013*"England" + 0.012*"Paris" + 0.011*"Hollywood"
topic #23 (0.033): 0.041*"haha" + 0.035*"football" + 0.034*"game" + 0.033*"show" + 0.015*"Kanye West" + 0.014*"team" + 0.014*"hair" + 0.013*"OK" + 0.013*"NBA" + 0.012*"net"
topic #14 (0.033): 0.062*"tcot" + 0.017*"Bush" + 0.014*"sgp" + 0.013*"Obama" + 0.012*"jobs" + 0.011*"teaparty" + 0.010*"woman" + 0.009*"GM" + 0.009*"job" + 0.009*"arrested"
topic diff=0.034879, rho=0.115567
PROGRESS: pass 2, at document #64000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.029*"U.S" + 0.021*"President" + 0.019*"Obama" + 0.018*"White House" + 0.015*"military" + 0.014*"President Obama" + 0.013*"Cancun" + 0.011*"TIME" + 0.010*"Sen" + 0.010*"S. Korea"
topic #17 (0.033): 0.052*"China" + 0.043*"North Korea" + 0.028*"Ireland" + 0.027*"South Korea" + 0.019*"Korea" + 0.017*"Japan" + 0.017*"Irish" + 0.015*"US" + 0.015*"attack" + 0.014*"Europe"
topic #24 (0.033): 0.093*"who" + 0.025*"me a" + 0.020*"a man" + 0.020*"Nice" + 0.019*"twit" + 0.018*"Oh" + 0.017*"Yes" + 0.016*"I'm going" + 0.015*"right" + 0.014*"Arizona"
topic #10 (0.033): 0.093*"para" + 0.050*"un" + 0.043*"como" + 0.033*"dos" + 0.021*"en" + 0.020*"pour" + 0.018*"la" + 0.016*"ll" + 0.015*"se" + 0.015*"Como"
topic #7 (0.033): 0.153*"Twitter" + 0.079*"Facebook" + 0.033*"NYC" + 0.025*"Tweet" + 0.023*"website" + 0.016*"photos" + 0.014*"Lady Gaga" + 0.014*"Chinese" + 0.011*"People" + 0.010*"Internet"
topic diff=0.028158, rho=0.115567
PROGRESS: pass 2, at document #66000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.038*"AP" + 0.020*"DADT" + 0.015*"study" + 0.014*"child" + 0.013*"risk" + 0.011*"friend" + 0.011*"report" + 0.009*"Pentagon" + 0.009*"prison" + 0.009*"weather"
topic #14 (0.033): 0.061*"tcot" + 0.018*"Bush" + 0.014*"Obama" + 0.012*"jobs" + 0.011*"GM" + 0.011*"sgp" + 0.011*"teaparty" + 0.011*"woman" + 0.010*"arrested" + 0.009*"Ohio"
topic #9 (0.033): 0.031*"will" + 0.020*"kids" + 0.020*"work" + 0.017*"do" + 0.016*"em" + 0.013*"mind" + 0.013*"play" + 0.012*"children" + 0.012*"don" + 0.012*"talk"
topic #20 (0.033): 0.041*"live" + 0.038*"TV" + 0.034*"world" + 0.031*"Watch" + 0.024*"Radio" + 0.022*"interview" + 0.021*"climate" + 0.021*"Hope" + 0.019*"climate change" + 0.018*"families"
topic #29 (0.033): 0.199*"Thanksgiving" + 0.064*"Happy Thanksgiving" + 0.053*"LOL" + 0.042*"thanksgiving" + 0.034*"family" + 0.028*"friends" + 0.028*"holiday" + 0.026*"Turkey" + 0.023*"tonight" + 0.016*"Happy thanksgiving"
topic diff=0.032473, rho=0.115567
PROGRESS: pass 2, at document #68000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.097*"WikiLeaks" + 0.081*"Wikileaks" + 0.032*"wikileaks" + 0.030*"cablegate" + 0.027*"NYT" + 0.023*"US" + 0.021*"Wiki Leaks" + 0.016*"documents" + 0.014*"Guardian" + 0.012*"China"
topic #24 (0.033): 0.095*"who" + 0.023*"me a" + 0.020*"I'm going" + 0.020*"a man" + 0.019*"Arizona" + 0.018*"Oh" + 0.017*"Nice" + 0.017*"Yes" + 0.015*"right" + 0.015*"always"
topic #29 (0.033): 0.199*"Thanksgiving" + 0.068*"Happy Thanksgiving" + 0.053*"LOL" + 0.041*"thanksgiving" + 0.033*"family" + 0.027*"friends" + 0.027*"holiday" + 0.025*"Turkey" + 0.023*"tonight" + 0.016*"Happy thanksgiving"
topic #5 (0.033): 0.025*"season" + 0.024*"student" + 0.023*"high school" + 0.020*"Wisconsin" + 0.018*"NBC" + 0.016*"hostage" + 0.014*"Good" + 0.013*"hostages" + 0.013*"smile" + 0.013*"record"
topic #15 (0.033): 0.025*"snow" + 0.022*"here" + 0.021*"women" + 0.021*"travel" + 0.021*"USA" + 0.015*"Paris" + 0.014*"sex" + 0.013*"film" + 0.012*"England" + 0.011*"baby"
topic diff=0.017301, rho=0.115567
PROGRESS: pass 2, at document #70000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.062*"via" + 0.039*"iPad" + 0.034*"iPhone" + 0.032*"Google" + 0.030*"Apple" + 0.026*"Facebook" + 0.024*"iTunes" + 0.021*"Christmas" + 0.017*"app" + 0.016*"online"
topic #23 (0.033): 0.052*"haha" + 0.034*"game" + 0.034*"football" + 0.032*"show" + 0.016*"Kanye West" + 0.015*"beautiful" + 0.015*"hair" + 0.013*"thoughts" + 0.013*"team" + 0.012*"turkeys"
topic #16 (0.033): 0.045*"Black Friday" + 0.027*"Sarah Palin" + 0.027*"HIV" + 0.020*"DJ" + 0.019*"Canada" + 0.017*"gay" + 0.017*"DWTS" + 0.014*"Here" + 0.013*"read" + 0.012*"If"
topic #14 (0.033): 0.070*"tcot" + 0.016*"Bush" + 0.013*"Obama" + 0.012*"teaparty" + 0.011*"GM" + 0.011*"sgp" + 0.010*"woman" + 0.010*"jobs" + 0.010*"arrested" + 0.008*"single"
topic #1 (0.033): 0.106*"TSA" + 0.059*"Obama" + 0.043*"GOP" + 0.027*"ppl" + 0.020*"Senate" + 0.016*"Palin" + 0.013*"vote" + 0.013*"Republicans" + 0.013*"Congress" + 0.013*"House"
topic diff=0.020200, rho=0.115567
PROGRESS: pass 2, at document #72000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.015*"U.S" + 0.012*"World" + 0.011*"News" + 0.010*"NYT" + 0.010*"Will" + 0.009*"New York" + 0.009*"Cancer" + 0.009*"Study" + 0.009*"BP" + 0.009*"Pres"
topic #15 (0.033): 0.025*"snow" + 0.024*"here" + 0.022*"USA" + 0.021*"women" + 0.019*"travel" + 0.017*"England" + 0.015*"sex" + 0.014*"film" + 0.013*"Paris" + 0.010*"baby"
topic #18 (0.033): 0.094*"WikiLeaks" + 0.084*"Wikileaks" + 0.034*"cablegate" + 0.033*"wikileaks" + 0.024*"US" + 0.021*"NYT" + 0.020*"Wiki Leaks" + 0.015*"Guardian" + 0.014*"documents" + 0.013*"cables"
topic #5 (0.033): 0.025*"season" + 0.021*"student" + 0.019*"high school" + 0.018*"Wisconsin" + 0.018*"NBC" + 0.015*"fb" + 0.015*"smile" + 0.014*"Good" + 0.013*"hostage" + 0.013*"record"
topic #29 (0.033): 0.202*"Thanksgiving" + 0.067*"Happy Thanksgiving" + 0.051*"LOL" + 0.038*"thanksgiving" + 0.034*"family" + 0.028*"friends" + 0.027*"holiday" + 0.026*"Turkey" + 0.022*"tonight" + 0.014*"Black Friday"
topic diff=0.014711, rho=0.115567
PROGRESS: pass 2, at document #74000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.041*"live" + 0.036*"TV" + 0.034*"world" + 0.031*"Watch" + 0.027*"Hope" + 0.025*"interview" + 0.023*"Radio" + 0.021*"climate" + 0.018*"climate change" + 0.018*"Check it"
topic #22 (0.033): 0.041*"Blog" + 0.026*"see" + 0.023*"We" + 0.022*"re" + 0.019*"artist" + 0.017*"book" + 0.016*"food" + 0.015*"birth" + 0.015*"WTF" + 0.015*"heroes"
topic #3 (0.033): 0.052*"UK" + 0.038*"photography" + 0.037*"London" + 0.026*"police" + 0.018*"British" + 0.017*"students" + 0.015*"protest" + 0.015*"BBC" + 0.013*"Police" + 0.012*"demo 2010"
topic #9 (0.033): 0.032*"will" + 0.020*"work" + 0.017*"kids" + 0.015*"do" + 0.014*"VIDEO" + 0.014*"em" + 0.014*"MSNBC" + 0.013*"mind" + 0.013*"children" + 0.012*"first"
topic #17 (0.033): 0.058*"China" + 0.045*"North Korea" + 0.029*"South Korea" + 0.023*"Ireland" + 0.022*"Korea" + 0.018*"US" + 0.017*"attack" + 0.017*"Japan" + 0.014*"Europe" + 0.014*"Reuters"
topic diff=0.033408, rho=0.115567
PROGRESS: pass 2, at document #76000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.065*"tcot" + 0.015*"Bush" + 0.014*"Obama" + 0.011*"teaparty" + 0.011*"woman" + 0.010*"jobs" + 0.010*"GM" + 0.010*"arrested" + 0.008*"sgp" + 0.008*"job"
topic #12 (0.033): 0.119*"God" + 0.039*"Who" + 0.016*"guy" + 0.016*"GOD" + 0.015*"left" + 0.014*"help" + 0.013*"Who's" + 0.012*"al" + 0.012*"NPR" + 0.011*"local"
topic #26 (0.033): 0.040*"India" + 0.029*"news" + 0.023*"business" + 0.019*"New Zealand" + 0.015*"dead" + 0.015*"mine" + 0.013*"energy" + 0.011*"killed" + 0.011*"PM" + 0.011*"Australia"
topic #11 (0.033): 0.057*"via" + 0.044*"iPad" + 0.035*"iPhone" + 0.032*"Google" + 0.030*"Apple" + 0.026*"Facebook" + 0.022*"Christmas" + 0.022*"iTunes" + 0.016*"online" + 0.015*"app"
topic #23 (0.033): 0.048*"haha" + 0.036*"game" + 0.035*"show" + 0.032*"football" + 0.016*"Kanye West" + 0.015*"beautiful" + 0.014*"team" + 0.014*"net" + 0.014*"hair" + 0.013*"thoughts"
topic diff=0.028586, rho=0.115567
PROGRESS: pass 2, at document #78000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.053*"Leslie Nielsen" + 0.044*"Today" + 0.041*"Love" + 0.022*"nice" + 0.021*"right now" + 0.021*"Time" + 0.020*"white" + 0.019*"Life" + 0.017*"Music" + 0.015*"Actor"
topic #20 (0.033): 0.042*"live" + 0.037*"TV" + 0.036*"world" + 0.033*"Watch" + 0.026*"Hope" + 0.025*"interview" + 0.020*"Radio" + 0.020*"climate" + 0.018*"climate change" + 0.017*"homeless"
topic #9 (0.033): 0.034*"will" + 0.021*"work" + 0.018*"em" + 0.017*"do" + 0.015*"kids" + 0.014*"mind" + 0.014*"first" + 0.013*"VIDEO" + 0.013*"play" + 0.013*"something"
topic #8 (0.033): 0.051*"Iran" + 0.042*"US" + 0.031*"Israel" + 0.026*"to win" + 0.025*"BBC" + 0.023*"Afghanistan" + 0.019*"Brasil" + 0.015*"American" + 0.013*"Egypt" + 0.012*"pode"
topic #0 (0.033): 0.100*"Haiti" + 0.031*"cholera" + 0.022*"UN" + 0.019*"Mexico" + 0.018*"Cholera" + 0.015*"election" + 0.015*"death" + 0.013*"Asian" + 0.013*"tweeted" + 0.011*"Israeli"
topic diff=0.029256, rho=0.115567
bound: at document #0
-22.643 per-word bound, 6550923.5 perplexity estimate based on a held-out corpus of 2000 documents with 49203 words
PROGRESS: pass 2, at document #80000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.057*"China" + 0.046*"North Korea" + 0.030*"South Korea" + 0.021*"Korea" + 0.020*"Ireland" + 0.018*"US" + 0.017*"attack" + 0.017*"Japan" + 0.017*"Reuters" + 0.013*"Europe"
topic #29 (0.033): 0.196*"Thanksgiving" + 0.071*"Happy Thanksgiving" + 0.040*"LOL" + 0.037*"thanksgiving" + 0.035*"family" + 0.033*"friends" + 0.030*"holiday" + 0.026*"Turkey" + 0.023*"tonight" + 0.013*"Black Friday"
topic #14 (0.033): 0.057*"tcot" + 0.017*"Bush" + 0.014*"Obama" + 0.012*"GM" + 0.011*"jobs" + 0.011*"woman" + 0.010*"teaparty" + 0.009*"arrested" + 0.008*"job" + 0.008*"Alaska"
topic #26 (0.033): 0.039*"India" + 0.030*"news" + 0.022*"business" + 0.018*"New Zealand" + 0.015*"dead" + 0.014*"mine" + 0.013*"energy" + 0.012*"killed" + 0.011*"PM" + 0.011*"Australia"
topic #12 (0.033): 0.109*"God" + 0.039*"Who" + 0.023*"al" + 0.017*"guy" + 0.016*"left" + 0.015*"GOD" + 0.015*"green" + 0.014*"help" + 0.013*"NPR" + 0.013*"Chile"
topic diff=0.060533, rho=0.115567
PROGRESS: pass 2, at document #82000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.094*"WikiLeaks" + 0.092*"Wikileaks" + 0.036*"wikileaks" + 0.035*"cablegate" + 0.031*"Guardian" + 0.023*"US" + 0.019*"Wiki Leaks" + 0.017*"NYT" + 0.016*"documents" + 0.015*"cables"
topic #4 (0.033): 0.028*"U.S" + 0.023*"President" + 0.023*"cooking" + 0.021*"Obama" + 0.016*"White House" + 0.016*"S. Korea" + 0.016*"military" + 0.015*"Cancun" + 0.013*"President Obama" + 0.012*"Seoul"
topic #25 (0.033): 0.047*"lol" + 0.040*"today" + 0.031*"people" + 0.027*"you" + 0.027*"time" + 0.022*"out" + 0.020*"can" + 0.020*"who" + 0.018*"ur" + 0.018*"am"
topic #11 (0.033): 0.060*"via" + 0.039*"iPad" + 0.034*"Google" + 0.030*"iPhone" + 0.029*"Facebook" + 0.027*"Apple" + 0.021*"Christmas" + 0.019*"online" + 0.018*"iTunes" + 0.015*"Free"
topic #22 (0.033): 0.046*"Blog" + 0.027*"re" + 0.023*"see" + 0.023*"We" + 0.017*"believe" + 0.015*"food" + 0.015*"book" + 0.015*"WTF" + 0.014*"Post" + 0.014*"artist"
topic diff=0.123695, rho=0.115567
PROGRESS: pass 2, at document #84000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.027*"NBC" + 0.025*"season" + 0.021*"high school" + 0.021*"student" + 0.017*"EEUU" + 0.017*"Wisconsin" + 0.016*"smile" + 0.016*"record" + 0.014*"Football" + 0.012*"Good"
topic #23 (0.033): 0.053*"haha" + 0.035*"game" + 0.032*"show" + 0.030*"football" + 0.017*"Kanye West" + 0.017*"beautiful" + 0.016*"net" + 0.015*"team" + 0.014*"hair" + 0.013*"thoughts"
topic #12 (0.033): 0.099*"God" + 0.039*"Who" + 0.021*"al" + 0.017*"left" + 0.016*"guy" + 0.016*"NPR" + 0.013*"GOD" + 0.013*"green" + 0.013*"help" + 0.012*"Who's"
topic #28 (0.033): 0.141*"video" + 0.081*"YouTube" + 0.064*"Video" + 0.028*"Cyber Monday" + 0.027*"Holiday" + 0.024*"music" + 0.017*"ad" + 0.017*"Check" + 0.014*"Live" + 0.014*"Photography"
topic #9 (0.033): 0.035*"will" + 0.020*"work" + 0.018*"kids" + 0.017*"do" + 0.017*"em" + 0.014*"first" + 0.013*"play" + 0.013*"don" + 0.013*"mind" + 0.012*"something"
topic diff=0.018648, rho=0.115567
PROGRESS: pass 2, at document #86000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.081*"Haiti" + 0.030*"UN" + 0.024*"cholera" + 0.020*"Yahoo" + 0.019*"Gaza" + 0.018*"Lebanon" + 0.018*"Mexico" + 0.017*"Israeli" + 0.016*"death" + 0.015*"Cholera"
topic #1 (0.033): 0.079*"TSA" + 0.060*"Obama" + 0.047*"GOP" + 0.031*"ppl" + 0.019*"Senate" + 0.017*"Palin" + 0.016*"Congress" + 0.014*"House" + 0.013*"politics" + 0.012*"Republicans"
topic #16 (0.033): 0.040*"Black Friday" + 0.035*"Canada" + 0.027*"read" + 0.025*"newspaper" + 0.025*"HIV" + 0.024*"Sarah Palin" + 0.017*"gay" + 0.016*"DWTS" + 0.015*"Here" + 0.011*"If"
topic #11 (0.033): 0.071*"via" + 0.036*"iPad" + 0.034*"Google" + 0.029*"iPhone" + 0.028*"Facebook" + 0.026*"Apple" + 0.022*"Christmas" + 0.018*"online" + 0.018*"iTunes" + 0.016*"Free"
topic #29 (0.033): 0.206*"Thanksgiving" + 0.083*"Happy Thanksgiving" + 0.043*"LOL" + 0.034*"family" + 0.034*"Turkey" + 0.034*"friends" + 0.034*"thanksgiving" + 0.027*"holiday" + 0.021*"tonight" + 0.011*"eat"
topic diff=0.032695, rho=0.115567
PROGRESS: pass 2, at document #88000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.033*"USA" + 0.024*"women" + 0.021*"here" + 0.017*"sex" + 0.017*"snow" + 0.016*"travel" + 0.014*"England" + 0.013*"Paris" + 0.013*"film" + 0.011*"baby"
topic #18 (0.033): 0.100*"WikiLeaks" + 0.094*"Wikileaks" + 0.033*"wikileaks" + 0.030*"cablegate" + 0.026*"Guardian" + 0.024*"US" + 0.018*"Wiki Leaks" + 0.016*"documents" + 0.016*"NYT" + 0.014*"cables"
topic #7 (0.033): 0.160*"Twitter" + 0.083*"Facebook" + 0.028*"NYC" + 0.028*"Tweet" + 0.015*"website" + 0.014*"photos" + 0.014*"Chinese" + 0.013*"COICA" + 0.012*"Thx" + 0.012*"Tweets"
topic #1 (0.033): 0.080*"TSA" + 0.059*"Obama" + 0.045*"GOP" + 0.030*"ppl" + 0.018*"Senate" + 0.017*"Palin" + 0.017*"politics" + 0.016*"Congress" + 0.013*"House" + 0.012*"Republicans"
topic #20 (0.033): 0.045*"live" + 0.037*"TV" + 0.034*"world" + 0.032*"Watch" + 0.027*"Hope" + 0.020*"interview" + 0.019*"climate" + 0.017*"climate change" + 0.017*"Check it" + 0.017*"COP16"
topic diff=0.031657, rho=0.115567
PROGRESS: pass 2, at document #90000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.014*"U.S" + 0.013*"World" + 0.012*"News" + 0.010*"Will" + 0.009*"New York" + 0.008*"Women" + 0.008*"Study" + 0.008*"NYT" + 0.008*"Business" + 0.007*"China"
topic #16 (0.033): 0.040*"Black Friday" + 0.031*"Canada" + 0.027*"read" + 0.024*"HIV" + 0.022*"Sarah Palin" + 0.022*"newspaper" + 0.019*"DWTS" + 0.016*"gay" + 0.014*"Here" + 0.011*"If"
topic #23 (0.033): 0.049*"haha" + 0.041*"game" + 0.032*"football" + 0.031*"show" + 0.017*"beautiful" + 0.016*"team" + 0.015*"hair" + 0.014*"Kanye West" + 0.013*"thoughts" + 0.013*"why"
topic #29 (0.033): 0.206*"Thanksgiving" + 0.081*"Happy Thanksgiving" + 0.042*"LOL" + 0.033*"family" + 0.033*"Turkey" + 0.032*"friends" + 0.032*"thanksgiving" + 0.028*"holiday" + 0.020*"tonight" + 0.013*"eat"
topic #18 (0.033): 0.101*"WikiLeaks" + 0.091*"Wikileaks" + 0.032*"wikileaks" + 0.032*"cablegate" + 0.025*"Guardian" + 0.025*"US" + 0.019*"Wiki Leaks" + 0.016*"documents" + 0.016*"NYT" + 0.015*"cables"
topic diff=0.024666, rho=0.115567
PROGRESS: pass 2, at document #92000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.067*"via" + 0.038*"Google" + 0.036*"iPad" + 0.032*"iPhone" + 0.031*"Facebook" + 0.026*"Apple" + 0.021*"Christmas" + 0.019*"online" + 0.017*"iTunes" + 0.016*"Free"
topic #9 (0.033): 0.032*"will" + 0.023*"em" + 0.021*"work" + 0.019*"do" + 0.017*"kids" + 0.016*"Chelsea" + 0.015*"first" + 0.014*"play" + 0.013*"talk" + 0.013*"VIDEO"
topic #16 (0.033): 0.040*"Black Friday" + 0.029*"Canada" + 0.027*"read" + 0.024*"HIV" + 0.023*"Sarah Palin" + 0.020*"newspaper" + 0.018*"DWTS" + 0.015*"gay" + 0.014*"Here" + 0.011*"If"
topic #19 (0.033): 0.042*"Leslie Nielsen" + 0.041*"Love" + 0.039*"Today" + 0.027*"Time" + 0.023*"right now" + 0.022*"nice" + 0.020*"Life" + 0.020*"Jews" + 0.017*"Oregon" + 0.016*"Hamas"
topic #2 (0.033): 0.037*"News" + 0.023*"bit.ly" + 0.022*"NFL" + 0.019*"Texas" + 0.019*"EUA" + 0.015*"son" + 0.014*"Report" + 0.014*"LMAO" + 0.014*"English" + 0.014*"Jewish"
topic diff=0.023687, rho=0.115567
PROGRESS: pass 2, at document #94000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.079*"Iran" + 0.043*"Israel" + 0.040*"US" + 0.030*"Afghanistan" + 0.021*"Iraq" + 0.019*"Brasil" + 0.018*"Egypt" + 0.015*"to win" + 0.014*"American" + 0.012*"Afghan"
topic #0 (0.033): 0.078*"Haiti" + 0.026*"UN" + 0.024*"cholera" + 0.018*"Gaza" + 0.018*"Mexico" + 0.017*"Israeli" + 0.016*"death" + 0.016*"Yahoo" + 0.015*"tweeted" + 0.014*"election"
topic #25 (0.033): 0.072*"lol" + 0.036*"today" + 0.030*"people" + 0.026*"time" + 0.026*"you" + 0.020*"ur" + 0.020*"who" + 0.019*"can" + 0.018*"out" + 0.018*"love"
topic #12 (0.033): 0.092*"God" + 0.040*"Who" + 0.019*"al" + 0.016*"left" + 0.016*"Damn" + 0.014*"NPR" + 0.014*"guy" + 0.013*"Who's" + 0.013*"Singapore" + 0.012*"GOD"
topic #27 (0.033): 0.092*"blog" + 0.041*"post" + 0.023*"DVD" + 0.019*"Complexo do Alemão" + 0.019*"Education" + 0.016*"release" + 0.016*"recipes" + 0.015*"web" + 0.014*"Amazon" + 0.014*"PC"
topic diff=0.019054, rho=0.115567
PROGRESS: pass 2, at document #96000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.204*"Thanksgiving" + 0.076*"Happy Thanksgiving" + 0.041*"LOL" + 0.035*"family" + 0.033*"friends" + 0.032*"holiday" + 0.031*"thanksgiving" + 0.031*"Turkey" + 0.020*"tonight" + 0.012*"Black Friday"
topic #23 (0.033): 0.043*"haha" + 0.037*"game" + 0.030*"show" + 0.029*"football" + 0.018*"beautiful" + 0.017*"team" + 0.014*"hair" + 0.013*"Kanye West" + 0.012*"net" + 0.012*"thoughts"
topic #21 (0.033): 0.048*"CNN" + 0.033*"NATO" + 0.030*"education" + 0.027*"Chicago" + 0.024*"Pope" + 0.022*"cancer" + 0.018*"Venezuela" + 0.018*"dans" + 0.017*"Retweet" + 0.017*"Friday"
topic #11 (0.033): 0.071*"via" + 0.037*"iPad" + 0.037*"Google" + 0.033*"iPhone" + 0.030*"Facebook" + 0.027*"Apple" + 0.021*"Christmas" + 0.020*"online" + 0.016*"iTunes" + 0.016*"Free"
topic #2 (0.033): 0.033*"News" + 0.030*"NFL" + 0.027*"English" + 0.021*"Texas" + 0.020*"bit.ly" + 0.016*"EUA" + 0.016*"LMAO" + 0.014*"Report" + 0.013*"Tom DeLay" + 0.013*"son"
topic diff=0.019119, rho=0.115567
PROGRESS: pass 2, at document #98000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.118*"para" + 0.064*"un" + 0.044*"como" + 0.038*"dos" + 0.029*"en" + 0.025*"es" + 0.022*"la" + 0.020*"se" + 0.017*"pour" + 0.016*"si"
topic #19 (0.033): 0.047*"Love" + 0.039*"Today" + 0.039*"Leslie Nielsen" + 0.026*"Time" + 0.022*"Life" + 0.021*"nice" + 0.020*"right now" + 0.017*"Jews" + 0.017*"Oregon" + 0.017*"Actor"
topic #28 (0.033): 0.114*"video" + 0.064*"YouTube" + 0.063*"Video" + 0.031*"Cyber Monday" + 0.026*"Holiday" + 0.024*"music" + 0.016*"Check" + 0.016*"Live" + 0.012*"ad" + 0.012*"Photography"
topic #21 (0.033): 0.047*"CNN" + 0.032*"NATO" + 0.029*"Venezuela" + 0.029*"education" + 0.028*"Chicago" + 0.024*"Pope" + 0.021*"cancer" + 0.018*"Retweet" + 0.017*"dans" + 0.017*"Friday"
topic #4 (0.033): 0.030*"U.S" + 0.023*"President" + 0.019*"Obama" + 0.016*"White House" + 0.016*"Seoul" + 0.015*"S. Korea" + 0.015*"Cancun" + 0.015*"military" + 0.013*"cooking" + 0.012*"President Obama"
topic diff=0.035656, rho=0.115567
bound: at document #0
-24.157 per-word bound, 18710724.7 perplexity estimate based on a held-out corpus of 2000 documents with 44486 words
PROGRESS: pass 2, at document #100000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.034*"USA" + 0.028*"women" + 0.024*"here" + 0.017*"snow" + 0.016*"travel" + 0.014*"sex" + 0.014*"film" + 0.013*"Paris" + 0.013*"England" + 0.012*"Thailand"
topic #26 (0.033): 0.039*"India" + 0.036*"news" + 0.021*"New Zealand" + 0.018*"business" + 0.014*"Australia" + 0.014*"dead" + 0.013*"PM" + 0.013*"mine" + 0.013*"killed" + 0.012*"police"
topic #10 (0.033): 0.109*"para" + 0.067*"un" + 0.045*"como" + 0.034*"dos" + 0.029*"en" + 0.028*"es" + 0.022*"la" + 0.021*"pour" + 0.019*"se" + 0.016*"si"
topic #5 (0.033): 0.022*"student" + 0.020*"high school" + 0.020*"season" + 0.019*"NBC" + 0.019*"smile" + 0.015*"Wisconsin" + 0.014*"record" + 0.014*"Football" + 0.013*"NY" + 0.013*"EEUU"
topic #17 (0.033): 0.054*"China" + 0.044*"North Korea" + 0.025*"South Korea" + 0.024*"Ireland" + 0.021*"Korea" + 0.020*"Reuters" + 0.017*"US" + 0.016*"Japan" + 0.016*"Europe" + 0.015*"Irish"
topic diff=0.021616, rho=0.115567
PROGRESS: pass 2, at document #102000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.076*"tcot" + 0.016*"Bush" + 0.014*"teaparty" + 0.013*"Obama" + 0.012*"jobs" + 0.011*"Islam" + 0.010*"Muslim" + 0.010*"arrested" + 0.010*"GM" + 0.009*"woman"
topic #17 (0.033): 0.053*"China" + 0.042*"North Korea" + 0.027*"Ireland" + 0.025*"South Korea" + 0.022*"Korea" + 0.019*"Reuters" + 0.017*"Japan" + 0.017*"US" + 0.016*"Irish" + 0.015*"attack"
topic #26 (0.033): 0.039*"India" + 0.033*"news" + 0.022*"business" + 0.021*"New Zealand" + 0.015*"PM" + 0.014*"dead" + 0.014*"mine" + 0.014*"Australia" + 0.014*"murder" + 0.013*"killed"
topic #23 (0.033): 0.040*"haha" + 0.033*"game" + 0.029*"show" + 0.025*"football" + 0.020*"net" + 0.016*"team" + 0.015*"beautiful" + 0.012*"win" + 0.012*"Kanye West" + 0.011*"hair"
topic #21 (0.033): 0.040*"CNN" + 0.032*"NATO" + 0.031*"Venezuela" + 0.028*"cancer" + 0.027*"dans" + 0.027*"Chicago" + 0.024*"Pope" + 0.023*"education" + 0.018*"Retweet" + 0.016*"Friday"
topic diff=0.089345, rho=0.115567
PROGRESS: pass 2, at document #104000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.109*"video" + 0.069*"Video" + 0.057*"YouTube" + 0.031*"Cyber Monday" + 0.027*"Holiday" + 0.022*"music" + 0.014*"Live" + 0.014*"Check" + 0.011*"ad" + 0.011*"Photography"
topic #12 (0.033): 0.082*"God" + 0.037*"Who" + 0.027*"al" + 0.016*"left" + 0.014*"America" + 0.014*"NPR" + 0.013*"Who's" + 0.013*"Celebrity" + 0.012*"guy" + 0.012*"help"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.069*"Happy Thanksgiving" + 0.041*"LOL" + 0.034*"friends" + 0.033*"holiday" + 0.033*"family" + 0.030*"Turkey" + 0.027*"thanksgiving" + 0.020*"tonight" + 0.013*"Black Friday"
topic #16 (0.033): 0.039*"Black Friday" + 0.029*"read" + 0.027*"newspaper" + 0.025*"HIV" + 0.024*"Sarah Palin" + 0.023*"Canada" + 0.019*"DWTS" + 0.015*"Here" + 0.014*"gay" + 0.012*"Bristol"
topic #24 (0.033): 0.094*"who" + 0.022*"me a" + 0.020*"Nice" + 0.018*"a man" + 0.017*"actor" + 0.016*"Arizona" + 0.016*"Yes" + 0.016*"always" + 0.016*"right" + 0.015*"Oh"
topic diff=0.091373, rho=0.115567
PROGRESS: pass 2, at document #106000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.097*"para" + 0.076*"un" + 0.042*"como" + 0.031*"es" + 0.029*"en" + 0.027*"pour" + 0.027*"dos" + 0.023*"la" + 0.018*"México" + 0.018*"se"
topic #24 (0.033): 0.094*"who" + 0.022*"me a" + 0.020*"Nice" + 0.017*"a man" + 0.016*"Arizona" + 0.016*"actor" + 0.016*"Oh" + 0.015*"Yes" + 0.015*"media" + 0.015*"right"
topic #3 (0.033): 0.087*"UK" + 0.033*"London" + 0.025*"police" + 0.021*"students" + 0.017*"Labour" + 0.016*"British" + 0.015*"protest" + 0.015*"photography" + 0.013*"demo 2010" + 0.013*"Police"
topic #13 (0.033): 0.057*"AP" + 0.016*"Amanda Knox" + 0.015*"child" + 0.015*"DADT" + 0.013*"study" + 0.012*"risk" + 0.010*"law" + 0.010*"prison" + 0.010*"report" + 0.009*"Pentagon"
topic #1 (0.033): 0.070*"TSA" + 0.060*"Obama" + 0.053*"GOP" + 0.021*"Senate" + 0.020*"ppl" + 0.019*"Congress" + 0.019*"Palin" + 0.015*"House" + 0.014*"Republicans" + 0.014*"vote"
topic diff=0.024789, rho=0.115567
PROGRESS: pass 2, at document #108000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.092*"WikiLeaks" + 0.092*"Wikileaks" + 0.043*"wikileaks" + 0.042*"cablegate" + 0.026*"US" + 0.021*"Guardian" + 0.018*"Wiki Leaks" + 0.015*"documents" + 0.013*"cables" + 0.012*"NYT"
topic #7 (0.033): 0.162*"Twitter" + 0.080*"Facebook" + 0.030*"Tweet" + 0.025*"NYC" + 0.017*"Lmao" + 0.013*"Tweets" + 0.013*"photos" + 0.013*"Chinese" + 0.012*"website" + 0.011*"People"
topic #22 (0.033): 0.053*"book" + 0.036*"Blog" + 0.023*"re" + 0.023*"We" + 0.021*"see" + 0.016*"Gracias" + 0.016*"food" + 0.013*"reason" + 0.013*"basketball" + 0.012*"believe"
topic #2 (0.033): 0.034*"News" + 0.023*"LMAO" + 0.021*"You" + 0.021*"MUSIC" + 0.020*"NFL" + 0.017*"English" + 0.017*"Texas" + 0.015*"USD" + 0.015*"Boston" + 0.014*"bit.ly"
topic #17 (0.033): 0.053*"China" + 0.042*"North Korea" + 0.026*"Ireland" + 0.026*"South Korea" + 0.024*"Japan" + 0.021*"Korea" + 0.018*"Reuters" + 0.017*"US" + 0.016*"BBC News" + 0.015*"attack"
topic diff=0.024716, rho=0.115567
PROGRESS: pass 2, at document #110000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.041*"News" + 0.021*"LMAO" + 0.021*"NFL" + 0.019*"You" + 0.018*"MUSIC" + 0.017*"Texas" + 0.016*"English" + 0.015*"Portuguese" + 0.014*"bit.ly" + 0.013*"Boston"
topic #12 (0.033): 0.131*"Celebrity" + 0.072*"God" + 0.034*"Who" + 0.020*"al" + 0.014*"NPR" + 0.014*"left" + 0.013*"Who's" + 0.012*"America" + 0.011*"green" + 0.010*"poor"
topic #26 (0.033): 0.038*"India" + 0.034*"news" + 0.021*"New Zealand" + 0.019*"business" + 0.015*"killed" + 0.015*"dead" + 0.015*"Australia" + 0.014*"mine" + 0.012*"crash" + 0.012*"police"
topic #17 (0.033): 0.052*"China" + 0.040*"North Korea" + 0.032*"Irish" + 0.028*"Ireland" + 0.025*"South Korea" + 0.024*"Japan" + 0.022*"Korea" + 0.017*"US" + 0.016*"Reuters" + 0.016*"BBC News"
topic #19 (0.033): 0.049*"Leslie Nielsen" + 0.047*"Today" + 0.041*"Love" + 0.031*"white" + 0.026*"Time" + 0.020*"Life" + 0.019*"nice" + 0.018*"Actor" + 0.018*"right now" + 0.016*"need"
topic diff=0.078254, rho=0.115567
PROGRESS: pass 2, at document #112000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.207*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.037*"LOL" + 0.035*"family" + 0.033*"holiday" + 0.032*"friends" + 0.029*"Turkey" + 0.025*"thanksgiving" + 0.020*"tonight" + 0.016*"Black Friday"
topic #26 (0.033): 0.037*"India" + 0.032*"news" + 0.021*"New Zealand" + 0.017*"business" + 0.016*"dead" + 0.015*"Australia" + 0.015*"killed" + 0.013*"mine" + 0.012*"crash" + 0.011*"police"
topic #23 (0.033): 0.035*"haha" + 0.032*"football" + 0.030*"game" + 0.029*"show" + 0.020*"Spanish" + 0.018*"team" + 0.015*"Italian" + 0.015*"reply" + 0.015*"net" + 0.014*"beautiful"
topic #7 (0.033): 0.225*"Twitter" + 0.074*"Facebook" + 0.025*"Tweet" + 0.020*"NYC" + 0.016*"uploaded" + 0.012*"Chinese" + 0.012*"Lmao" + 0.012*"Tweets" + 0.011*"Thx" + 0.011*"photos"
topic #2 (0.033): 0.038*"News" + 0.022*"NFL" + 0.020*"LMAO" + 0.018*"You" + 0.016*"MUSIC" + 0.016*"Texas" + 0.016*"English" + 0.015*"Boston" + 0.014*"Portuguese" + 0.014*"bit.ly"
topic diff=0.030414, rho=0.115567
PROGRESS: pass 2, at document #114000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.186*"week" + 0.035*"Blog" + 0.034*"book" + 0.019*"We" + 0.017*"re" + 0.015*"see" + 0.012*"Gracias" + 0.012*"reason" + 0.012*"believe" + 0.012*"food"
topic #10 (0.033): 0.102*"para" + 0.072*"un" + 0.043*"como" + 0.031*"dos" + 0.029*"es" + 0.029*"en" + 0.023*"la" + 0.021*"pour" + 0.018*"se" + 0.018*"este"
topic #18 (0.033): 0.097*"WikiLeaks" + 0.091*"Wikileaks" + 0.038*"wikileaks" + 0.036*"cablegate" + 0.026*"US" + 0.019*"Guardian" + 0.017*"Wiki Leaks" + 0.015*"documents" + 0.013*"NYT" + 0.011*"cables"
topic #8 (0.033): 0.053*"Iran" + 0.043*"US" + 0.031*"Israel" + 0.027*"Afghanistan" + 0.024*"Pakistan" + 0.019*"Brasil" + 0.018*"to win" + 0.015*"Iraq" + 0.014*"American" + 0.012*"Russia"
topic #24 (0.033): 0.092*"who" + 0.020*"Nice" + 0.019*"a man" + 0.018*"me a" + 0.018*"Yes" + 0.016*"Oh" + 0.015*"Madrid" + 0.015*"right" + 0.015*"always" + 0.015*"hate"
topic diff=0.015349, rho=0.115567
PROGRESS: pass 2, at document #116000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.056*"via" + 0.048*"Google" + 0.047*"iPad" + 0.041*"iPhone" + 0.040*"Apple" + 0.036*"Facebook" + 0.023*"Android" + 0.017*"Microsoft" + 0.017*"Christmas" + 0.015*"Beatles"
topic #4 (0.033): 0.029*"Mc" + 0.027*"U.S" + 0.024*"President" + 0.017*"White House" + 0.017*"Obama" + 0.016*"Hoy" + 0.015*"Cancun" + 0.014*"President Obama" + 0.014*"military" + 0.014*"S. Korea"
topic #3 (0.033): 0.080*"UK" + 0.034*"London" + 0.023*"police" + 0.020*"students" + 0.017*"protest" + 0.016*"British" + 0.014*"Police" + 0.013*"photography" + 0.012*"demo 2010" + 0.012*"Colombia"
topic #28 (0.033): 0.107*"video" + 0.093*"YouTube" + 0.066*"Video" + 0.031*"Cyber Monday" + 0.028*"Holiday" + 0.023*"music" + 0.014*"Check" + 0.012*"Live" + 0.010*"Photography" + 0.010*"Page"
topic #18 (0.033): 0.099*"WikiLeaks" + 0.090*"Wikileaks" + 0.035*"wikileaks" + 0.033*"cablegate" + 0.025*"US" + 0.019*"Wiki Leaks" + 0.018*"Guardian" + 0.015*"documents" + 0.013*"NYT" + 0.012*"jaja"
topic diff=0.030737, rho=0.115567
PROGRESS: pass 2, at document #118000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.044*"Black Friday" + 0.024*"Sarah Palin" + 0.024*"HIV" + 0.023*"read" + 0.018*"Canada" + 0.018*"If" + 0.017*"newspaper" + 0.016*"DWTS" + 0.015*"Here" + 0.014*"gay"
topic #26 (0.033): 0.044*"India" + 0.034*"news" + 0.019*"New Zealand" + 0.018*"NZ" + 0.015*"business" + 0.015*"killed" + 0.014*"Australia" + 0.014*"dead" + 0.014*"PM" + 0.013*"energy"
topic #29 (0.033): 0.209*"Thanksgiving" + 0.073*"Happy Thanksgiving" + 0.041*"LOL" + 0.037*"family" + 0.034*"friends" + 0.030*"holiday" + 0.029*"Turkey" + 0.023*"thanksgiving" + 0.019*"tonight" + 0.016*"Black Friday"
topic #5 (0.033): 0.057*"fb" + 0.026*"Football" + 0.021*"season" + 0.020*"NBC" + 0.018*"student" + 0.018*"smile" + 0.016*"San Diego" + 0.015*"Sports" + 0.015*"high school" + 0.015*"Los Angeles"
topic #7 (0.033): 0.200*"Twitter" + 0.080*"Facebook" + 0.036*"NYC" + 0.023*"Tweet" + 0.013*"Thx" + 0.012*"uploaded" + 0.012*"Chinese" + 0.011*"photos" + 0.010*"Lmao" + 0.010*"Tweets"
topic diff=0.016918, rho=0.115567
bound: at document #0
-23.296 per-word bound, 10301317.7 perplexity estimate based on a held-out corpus of 2000 documents with 33817 words
PROGRESS: pass 2, at document #120000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.095*"who" + 0.020*"Nice" + 0.020*"a man" + 0.018*"me a" + 0.017*"always" + 0.017*"Yes" + 0.016*"Madrid" + 0.016*"hate" + 0.015*"Arizona" + 0.015*"actor"
topic #15 (0.033): 0.039*"USA" + 0.025*"women" + 0.025*"French" + 0.019*"England" + 0.019*"Thailand" + 0.019*"snow" + 0.018*"here" + 0.017*"travel" + 0.016*"sex" + 0.013*"Thai"
topic #23 (0.033): 0.031*"game" + 0.031*"show" + 0.030*"haha" + 0.030*"football" + 0.018*"team" + 0.016*"Spanish" + 0.014*"beautiful" + 0.014*"NBA" + 0.014*"net" + 0.014*"Kanye West"
topic #6 (0.033): 0.020*"jajaja" + 0.016*"News" + 0.015*"U.S" + 0.013*"World" + 0.010*"Women" + 0.010*"Will" + 0.010*"New York" + 0.009*"Billion" + 0.008*"Health" + 0.008*"Study"
topic #3 (0.033): 0.075*"UK" + 0.037*"London" + 0.026*"police" + 0.020*"students" + 0.018*"Cuba" + 0.017*"protest" + 0.015*"British" + 0.014*"Police" + 0.012*"Colombia" + 0.011*"demo 2010"
topic diff=0.031567, rho=0.115567
PROGRESS: pass 2, at document #122000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.053*"Love" + 0.047*"Leslie Nielsen" + 0.041*"Today" + 0.023*"white" + 0.022*"Time" + 0.021*"Life" + 0.019*"Music" + 0.018*"nice" + 0.018*"right now" + 0.017*"Actor"
topic #25 (0.033): 0.049*"lol" + 0.036*"today" + 0.031*"people" + 0.025*"time" + 0.023*"you" + 0.021*"ur" + 0.021*"who" + 0.020*"can" + 0.020*"out" + 0.018*"love"
topic #22 (0.033): 0.123*"week" + 0.041*"Blog" + 0.028*"book" + 0.024*"Vintage" + 0.020*"We" + 0.018*"re" + 0.015*"see" + 0.012*"believe" + 0.012*"Gracias" + 0.012*"reason"
topic #24 (0.033): 0.096*"who" + 0.021*"a man" + 0.021*"Nice" + 0.019*"me a" + 0.018*"always" + 0.017*"hate" + 0.016*"Yes" + 0.015*"actor" + 0.015*"right" + 0.015*"Madrid"
topic #14 (0.033): 0.057*"tcot" + 0.020*"Bush" + 0.012*"jobs" + 0.012*"Obama" + 0.012*"GM" + 0.011*"Colombian" + 0.009*"FBI" + 0.009*"woman" + 0.009*"arrested" + 0.009*"teaparty"
topic diff=0.017068, rho=0.115567
PROGRESS: pass 2, at document #124000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.045*"live" + 0.044*"TV" + 0.038*"Watch" + 0.033*"world" + 0.027*"Hope" + 0.024*"interview" + 0.023*"climate" + 0.020*"families" + 0.019*"Check it" + 0.017*"Check it out"
topic #22 (0.033): 0.107*"week" + 0.046*"Blog" + 0.027*"book" + 0.021*"Vintage" + 0.020*"We" + 0.019*"re" + 0.015*"see" + 0.014*"Post" + 0.013*"reason" + 0.012*"Gracias"
topic #6 (0.033): 0.016*"jajaja" + 0.016*"News" + 0.015*"U.S" + 0.013*"World" + 0.012*"BP" + 0.010*"New York" + 0.010*"Will" + 0.010*"Women" + 0.008*"Billion" + 0.008*"Business"
topic #27 (0.033): 0.084*"blog" + 0.042*"release" + 0.033*"post" + 0.021*"Amazon" + 0.019*"DVD" + 0.018*"Complexo do Alemão" + 0.018*"Oprah" + 0.017*"PC" + 0.016*"recipes" + 0.015*"web"
topic #8 (0.033): 0.046*"Iran" + 0.045*"US" + 0.035*"Israel" + 0.027*"Afghanistan" + 0.021*"Pakistan" + 0.021*"Brasil" + 0.017*"to win" + 0.015*"American" + 0.015*"Iraq" + 0.013*"pode"
topic diff=0.037954, rho=0.115567
PROGRESS: pass 2, at document #126000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.033*"will" + 0.027*"em" + 0.022*"do" + 0.020*"kids" + 0.020*"work" + 0.015*"mind" + 0.015*"soul" + 0.014*"children" + 0.014*"VIDEO" + 0.012*"Thank you"
topic #29 (0.033): 0.207*"Thanksgiving" + 0.077*"Happy Thanksgiving" + 0.043*"LOL" + 0.042*"family" + 0.033*"friends" + 0.028*"holiday" + 0.026*"Turkey" + 0.025*"thanksgiving" + 0.018*"tonight" + 0.015*"Black Friday"
topic #21 (0.033): 0.070*"CNN" + 0.028*"Retweet" + 0.028*"NATO" + 0.027*"Venezuela" + 0.024*"Chicago" + 0.022*"Pope" + 0.019*"dans" + 0.019*"cancer" + 0.018*"Friday" + 0.017*"Arsenal"
topic #1 (0.033): 0.053*"TSA" + 0.051*"Obama" + 0.049*"GOP" + 0.023*"Congress" + 0.023*"ppl" + 0.018*"Republicans" + 0.018*"Senate" + 0.016*"Palin" + 0.015*"House" + 0.014*"vote"
topic #8 (0.033): 0.045*"US" + 0.045*"Iran" + 0.034*"Israel" + 0.027*"Afghanistan" + 0.021*"Pakistan" + 0.020*"Brasil" + 0.016*"to win" + 0.015*"American" + 0.015*"Iraq" + 0.013*"pode"
topic diff=0.050525, rho=0.115567
PROGRESS: pass 2, at document #128000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.079*"UK" + 0.037*"London" + 0.023*"police" + 0.021*"students" + 0.018*"British" + 0.016*"protest" + 0.015*"Cuba" + 0.014*"Colombia" + 0.013*"Police" + 0.012*"photography"
topic #23 (0.033): 0.036*"game" + 0.029*"show" + 0.029*"football" + 0.027*"haha" + 0.020*"Dallas" + 0.018*"team" + 0.017*"beautiful" + 0.017*"NBA" + 0.014*"win" + 0.014*"net"
topic #11 (0.033): 0.055*"via" + 0.045*"Google" + 0.041*"iPad" + 0.039*"iPhone" + 0.038*"Facebook" + 0.036*"Apple" + 0.019*"Android" + 0.017*"iTunes" + 0.017*"Christmas" + 0.016*"app"
topic #22 (0.033): 0.088*"week" + 0.044*"Blog" + 0.026*"book" + 0.021*"We" + 0.020*"re" + 0.017*"Vintage" + 0.014*"see" + 0.014*"basketball" + 0.013*"believe" + 0.013*"Post"
topic #5 (0.033): 0.037*"fb" + 0.026*"season" + 0.023*"Football" + 0.022*"student" + 0.018*"NBC" + 0.017*"high school" + 0.017*"smile" + 0.015*"Los Angeles" + 0.014*"Sports" + 0.014*"Wisconsin"
topic diff=0.032520, rho=0.115567
PROGRESS: pass 2, at document #130000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.063*"CNN" + 0.034*"Retweet" + 0.028*"Chicago" + 0.026*"NATO" + 0.022*"Venezuela" + 0.022*"Pope" + 0.020*"cancer" + 0.019*"Friday" + 0.016*"education" + 0.016*"dans"
topic #28 (0.033): 0.125*"video" + 0.070*"Video" + 0.067*"YouTube" + 0.024*"music" + 0.024*"Holiday" + 0.023*"Cyber Monday" + 0.021*"Check" + 0.013*"HD" + 0.011*"Live" + 0.010*"Photos"
topic #29 (0.033): 0.203*"Thanksgiving" + 0.075*"Happy Thanksgiving" + 0.046*"LOL" + 0.043*"family" + 0.034*"friends" + 0.028*"holiday" + 0.026*"thanksgiving" + 0.025*"Turkey" + 0.019*"tonight" + 0.016*"Black Friday"
topic #4 (0.033): 0.025*"U.S" + 0.024*"Seoul" + 0.022*"President" + 0.020*"Hoy" + 0.018*"Obama" + 0.017*"cooking" + 0.015*"Mc" + 0.015*"Yahoo! News" + 0.014*"President Obama" + 0.014*"White House"
topic #16 (0.033): 0.045*"Black Friday" + 0.023*"Sarah Palin" + 0.022*"HIV" + 0.020*"read" + 0.016*"If" + 0.015*"Here" + 0.015*"DWTS" + 0.015*"Canada" + 0.012*"newspaper" + 0.012*"to show"
topic diff=0.020940, rho=0.115567
PROGRESS: pass 2, at document #132000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.128*"video" + 0.067*"YouTube" + 0.066*"Video" + 0.028*"Holiday" + 0.025*"music" + 0.023*"Cyber Monday" + 0.021*"Check" + 0.013*"HD" + 0.013*"Photography" + 0.011*"Live"
topic #0 (0.033): 0.073*"Haiti" + 0.034*"Amazon.com" + 0.027*"cholera" + 0.023*"UN" + 0.019*"Asian" + 0.016*"Dutch" + 0.016*"Israeli" + 0.015*"death" + 0.015*"gold" + 0.015*"Gaza"
topic #9 (0.033): 0.032*"will" + 0.019*"work" + 0.019*"kids" + 0.019*"em" + 0.018*"do" + 0.016*"mind" + 0.015*"VIDEO" + 0.013*"soul" + 0.013*"children" + 0.013*"talk"
topic #6 (0.033): 0.016*"News" + 0.014*"U.S" + 0.013*"World" + 0.012*"BP" + 0.011*"jajaja" + 0.010*"New York" + 0.010*"Will" + 0.009*"Women" + 0.009*"Business" + 0.008*"Earth"
topic #29 (0.033): 0.203*"Thanksgiving" + 0.076*"Happy Thanksgiving" + 0.044*"LOL" + 0.041*"family" + 0.033*"friends" + 0.029*"holiday" + 0.025*"thanksgiving" + 0.024*"Turkey" + 0.020*"tonight" + 0.016*"Black Friday"
topic diff=0.025535, rho=0.115567
PROGRESS: pass 2, at document #134000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.092*"WikiLeaks" + 0.079*"Wikileaks" + 0.032*"wikileaks" + 0.027*"US" + 0.025*"cablegate" + 0.023*"Asia" + 0.020*"Wiki Leaks" + 0.019*"Guardian" + 0.014*"documents" + 0.013*"ha"
topic #13 (0.033): 0.042*"add" + 0.039*"AP" + 0.037*"Breaking News" + 0.015*"Rome" + 0.014*"DADT" + 0.013*"study" + 0.011*"child" + 0.010*"risk" + 0.009*"perfect" + 0.009*"weather"
topic #0 (0.033): 0.075*"Haiti" + 0.031*"Amazon.com" + 0.027*"cholera" + 0.023*"UN" + 0.018*"Asian" + 0.016*"death" + 0.015*"election" + 0.015*"Cholera" + 0.015*"Israeli" + 0.014*"Dutch"
topic #7 (0.033): 0.340*"Twitter" + 0.059*"Facebook" + 0.025*"NYC" + 0.020*"Tweet" + 0.017*"Lady Gaga" + 0.011*"photos" + 0.010*"Chinese" + 0.010*"website" + 0.009*"Lmao" + 0.008*"Simple"
topic #29 (0.033): 0.209*"Thanksgiving" + 0.079*"Happy Thanksgiving" + 0.041*"LOL" + 0.040*"family" + 0.034*"friends" + 0.030*"holiday" + 0.026*"Turkey" + 0.024*"thanksgiving" + 0.020*"tonight" + 0.018*"Black Friday"
topic diff=0.022052, rho=0.115567
PROGRESS: pass 2, at document #136000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.035*"game" + 0.030*"show" + 0.030*"RETWEET" + 0.026*"football" + 0.025*"haha" + 0.023*"net" + 0.019*"beautiful" + 0.018*"team" + 0.016*"Dallas" + 0.015*"NBA"
topic #11 (0.033): 0.055*"via" + 0.042*"Google" + 0.039*"Facebook" + 0.038*"iPhone" + 0.035*"iPad" + 0.031*"Apple" + 0.021*"Christmas" + 0.020*"iTunes" + 0.018*"Android" + 0.017*"online"
topic #3 (0.033): 0.073*"UK" + 0.036*"London" + 0.021*"police" + 0.019*"students" + 0.018*"British" + 0.018*"photography" + 0.016*"protest" + 0.013*"Cuba" + 0.012*"Police" + 0.012*"Colombia"
topic #26 (0.033): 0.043*"India" + 0.030*"news" + 0.023*"New Zealand" + 0.017*"business" + 0.016*"mine" + 0.015*"killed" + 0.015*"dead" + 0.014*"NZ" + 0.014*"Australia" + 0.013*"energy"
topic #13 (0.033): 0.040*"AP" + 0.037*"add" + 0.035*"Breaking News" + 0.014*"Rome" + 0.013*"DADT" + 0.012*"study" + 0.011*"child" + 0.010*"risk" + 0.009*"founder" + 0.009*"report"
topic diff=0.015713, rho=0.115567
PROGRESS: pass 2, at document #138000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.047*"live" + 0.042*"TV" + 0.036*"Watch" + 0.034*"world" + 0.032*"Hope" + 0.025*"interview" + 0.024*"Check it" + 0.023*"COP16" + 0.019*"Check it out" + 0.018*"families"
topic #11 (0.033): 0.050*"via" + 0.041*"Google" + 0.039*"Facebook" + 0.038*"iPhone" + 0.035*"iPad" + 0.030*"Apple" + 0.021*"Christmas" + 0.021*"Android" + 0.019*"iTunes" + 0.017*"online"
topic #1 (0.033): 0.062*"TSA" + 0.053*"Obama" + 0.039*"GOP" + 0.020*"ppl" + 0.019*"download" + 0.019*"Congress" + 0.019*"vote" + 0.019*"Senate" + 0.016*"Palin" + 0.014*"House"
topic #6 (0.033): 0.015*"Business" + 0.014*"News" + 0.014*"U.S" + 0.013*"World" + 0.012*"New York" + 0.010*"BP" + 0.009*"Will" + 0.009*"Women" + 0.009*"jajaja" + 0.008*"Study"
topic #8 (0.033): 0.045*"US" + 0.040*"Iran" + 0.036*"Israel" + 0.027*"Afghanistan" + 0.025*"Pakistan" + 0.019*"Egypt" + 0.016*"to win" + 0.014*"American" + 0.013*"Iraq" + 0.013*"Brasil"
topic diff=0.021481, rho=0.115567
bound: at document #0
-22.275 per-word bound, 5076641.0 perplexity estimate based on a held-out corpus of 2000 documents with 78501 words
PROGRESS: pass 2, at document #140000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.045*"US" + 0.042*"Iran" + 0.035*"Israel" + 0.027*"Afghanistan" + 0.022*"Pakistan" + 0.019*"Egypt" + 0.018*"to win" + 0.015*"American" + 0.012*"Iraq" + 0.012*"Russia"
topic #11 (0.033): 0.058*"via" + 0.048*"iPad" + 0.044*"iPhone" + 0.038*"Google" + 0.038*"Facebook" + 0.032*"Apple" + 0.020*"Christmas" + 0.019*"Android" + 0.019*"iTunes" + 0.017*"online"
topic #12 (0.033): 0.129*"God" + 0.032*"Who" + 0.025*"Celebrity" + 0.021*"al" + 0.021*"TED" + 0.012*"help" + 0.012*"Who's" + 0.011*"guy" + 0.011*"left" + 0.011*"dogs"
topic #21 (0.033): 0.054*"CNN" + 0.043*"Retweet" + 0.025*"NATO" + 0.024*"Chicago" + 0.021*"cancer" + 0.020*"Pope" + 0.020*"speech" + 0.020*"Venezuela" + 0.020*"Friday" + 0.020*"Toronto"
topic #4 (0.033): 0.029*"President" + 0.024*"U.S" + 0.020*"Obama" + 0.020*"White House" + 0.017*"Hoy" + 0.016*"President Obama" + 0.016*"Seoul" + 0.014*"military" + 0.014*"Cancun" + 0.014*"S. Korea"
topic diff=0.070787, rho=0.115567
PROGRESS: pass 2, at document #142000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.014*"U.S" + 0.014*"News" + 0.013*"Bloomberg" + 0.012*"Business" + 0.012*"World" + 0.011*"Will" + 0.010*"New York" + 0.009*"jajaja" + 0.009*"Women" + 0.009*"Health"
topic #22 (0.033): 0.063*"Blog" + 0.044*"week" + 0.023*"re" + 0.021*"We" + 0.021*"book" + 0.019*"believe" + 0.016*"change" + 0.015*"learn" + 0.014*"basketball" + 0.014*"see"
topic #10 (0.033): 0.125*"Como" + 0.071*"un" + 0.070*"para" + 0.039*"como" + 0.032*"es" + 0.029*"en" + 0.026*"la" + 0.018*"dos" + 0.017*"México" + 0.017*"este"
topic #20 (0.033): 0.049*"live" + 0.039*"TV" + 0.038*"world" + 0.035*"Watch" + 0.032*"Check it" + 0.030*"Hope" + 0.028*"Check it out" + 0.028*"COP16" + 0.027*"interview" + 0.018*"families"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.075*"Happy Thanksgiving" + 0.045*"LOL" + 0.039*"family" + 0.036*"friends" + 0.029*"holiday" + 0.024*"Turkey" + 0.023*"thanksgiving" + 0.018*"tonight" + 0.017*"Black Friday"
topic diff=0.025129, rho=0.115567
bound: at document #0
-23.350 per-word bound, 10693019.7 perplexity estimate based on a held-out corpus of 1749 documents with 49577 words
PROGRESS: pass 2, at document #143749/143749
performing inference on a chunk of 1749 documents
1748/1749 documents converged within 50 iterations
updating topics
merging changes from 1749 documents into a model of 143749 documents
topic #24 (0.033): 0.085*"who" + 0.032*"college" + 0.027*"fat" + 0.021*"hate" + 0.020*"Oh" + 0.020*"me a" + 0.020*"a man" + 0.019*"Nice" + 0.019*"always" + 0.017*"Yes"
topic #9 (0.033): 0.046*"kids" + 0.032*"will" + 0.018*"work" + 0.016*"don" + 0.015*"god" + 0.015*"down" + 0.014*"do" + 0.013*"mind" + 0.013*"talk" + 0.013*"soul"
topic #4 (0.033): 0.029*"President" + 0.024*"U.S" + 0.020*"Obama" + 0.018*"White House" + 0.016*"President Obama" + 0.016*"Hoy" + 0.015*"military" + 0.014*"Seoul" + 0.014*"S. Korea" + 0.014*"Sen"
topic #1 (0.033): 0.057*"Obama" + 0.054*"TSA" + 0.044*"GOP" + 0.031*"ppl" + 0.021*"Palin" + 0.019*"Senate" + 0.018*"Congress" + 0.016*"vote" + 0.016*"Republicans" + 0.015*"Sarah Palin"
topic #21 (0.033): 0.046*"CNN" + 0.043*"Retweet" + 0.028*"NATO" + 0.027*"speech" + 0.023*"cancer" + 0.023*"Chicago" + 0.022*"Friday" + 0.020*"Pope" + 0.018*"condoms" + 0.018*"Venezuela"
topic diff=0.015585, rho=0.115567
PROGRESS: pass 3, at document #2000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #26 (0.033): 0.033*"news" + 0.032*"India" + 0.019*"New Zealand" + 0.016*"business" + 0.015*"mine" + 0.014*"dead" + 0.014*"killed" + 0.013*"feeling" + 0.012*"Australia" + 0.012*"NZ"
topic #5 (0.033): 0.030*"NBC" + 0.024*"IM" + 0.021*"student" + 0.020*"fb" + 0.020*"season" + 0.018*"high school" + 0.017*"smile" + 0.016*"Football" + 0.015*"Wisconsin" + 0.015*"method"
topic #10 (0.033): 0.106*"Como" + 0.070*"un" + 0.068*"para" + 0.039*"como" + 0.034*"es" + 0.028*"en" + 0.025*"la" + 0.018*"dos" + 0.017*"este" + 0.016*"más"
topic #3 (0.033): 0.066*"UK" + 0.035*"London" + 0.020*"police" + 0.017*"protest" + 0.017*"students" + 0.017*"school" + 0.016*"British" + 0.013*"Colombia" + 0.011*"failure" + 0.011*"photography"
topic #2 (0.033): 0.033*"News" + 0.032*"bit.ly" + 0.022*"NFL" + 0.018*"English" + 0.015*"USD" + 0.015*"You" + 0.015*"Boston" + 0.015*"Texas" + 0.013*"LMAO" + 0.013*"Tom DeLay"
topic diff=0.025278, rho=0.114803
PROGRESS: pass 3, at document #4000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.046*"tcot" + 0.043*"hanging" + 0.016*"Bush" + 0.012*"jobs" + 0.012*"Obama" + 0.008*"arrested" + 0.008*"GM" + 0.008*"Ohio" + 0.008*"shot" + 0.008*"woman"
topic #29 (0.033): 0.199*"Thanksgiving" + 0.071*"Happy Thanksgiving" + 0.041*"LOL" + 0.036*"family" + 0.035*"friends" + 0.031*"holiday" + 0.023*"Turkey" + 0.022*"thanksgiving" + 0.018*"Black Friday" + 0.018*"tonight"
topic #22 (0.033): 0.059*"Blog" + 0.036*"week" + 0.023*"re" + 0.020*"believe" + 0.020*"book" + 0.020*"We" + 0.017*"basketball" + 0.017*"change" + 0.015*"learn" + 0.013*"Gracias"
topic #12 (0.033): 0.106*"God" + 0.030*"Who" + 0.024*"al" + 0.019*"Celebrity" + 0.017*"help" + 0.014*"TED" + 0.013*"dogs" + 0.012*"Who's" + 0.012*"fashion" + 0.012*"guy"
topic #11 (0.033): 0.053*"via" + 0.046*"iPad" + 0.046*"iPhone" + 0.044*"Google" + 0.039*"Facebook" + 0.030*"Apple" + 0.021*"Android" + 0.016*"iTunes" + 0.016*"online" + 0.016*"Christmas"
topic diff=0.042244, rho=0.114803
PROGRESS: pass 3, at document #6000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.044*"Iran" + 0.043*"US" + 0.030*"Israel" + 0.026*"Afghanistan" + 0.019*"Pakistan" + 0.016*"Egypt" + 0.016*"to win" + 0.014*"Nato" + 0.014*"Russia" + 0.013*"Afghan"
topic #29 (0.033): 0.198*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.039*"LOL" + 0.036*"family" + 0.034*"friends" + 0.031*"holiday" + 0.023*"Turkey" + 0.022*"thanksgiving" + 0.018*"tonight" + 0.017*"Black Friday"
topic #26 (0.033): 0.033*"India" + 0.031*"news" + 0.020*"New Zealand" + 0.015*"business" + 0.015*"mine" + 0.014*"killed" + 0.014*"dead" + 0.012*"NZ" + 0.012*"Australia" + 0.012*"police"
topic #6 (0.033): 0.015*"U.S" + 0.011*"World" + 0.011*"News" + 0.011*"Business" + 0.010*"Health" + 0.010*"Will" + 0.010*"New York" + 0.009*"Bloomberg" + 0.008*"Women" + 0.007*"NYT"
topic #13 (0.033): 0.046*"AP" + 0.022*"DADT" + 0.019*"Breaking News" + 0.018*"add" + 0.012*"study" + 0.011*"Art" + 0.011*"prison" + 0.011*"risk" + 0.010*"child" + 0.010*"Pentagon"
topic diff=0.054777, rho=0.114803
PROGRESS: pass 3, at document #8000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.048*"Leslie Nielsen" + 0.041*"Love" + 0.036*"Today" + 0.027*"Music" + 0.025*"need" + 0.022*"Time" + 0.018*"Life" + 0.017*"right now" + 0.017*"nice" + 0.016*"Jets"
topic #9 (0.033): 0.038*"kids" + 0.031*"will" + 0.018*"work" + 0.014*"do" + 0.013*"don" + 0.012*"down" + 0.012*"VIDEO" + 0.012*"em" + 0.012*"talk" + 0.012*"children"
topic #14 (0.033): 0.050*"tcot" + 0.034*"hanging" + 0.015*"Bush" + 0.012*"Obama" + 0.011*"jobs" + 0.009*"woman" + 0.008*"arrested" + 0.008*"shot" + 0.008*"Police" + 0.008*"Ohio"
topic #3 (0.033): 0.064*"UK" + 0.034*"London" + 0.023*"police" + 0.018*"protest" + 0.018*"students" + 0.016*"British" + 0.015*"school" + 0.014*"Colombia" + 0.012*"Police" + 0.010*"photography"
topic #27 (0.033): 0.086*"blog" + 0.048*"Back" + 0.032*"post" + 0.021*"Amazon" + 0.018*"release" + 0.017*"Education" + 0.016*"web" + 0.016*"DVD" + 0.014*"Oprah" + 0.012*"PC"
topic diff=0.038329, rho=0.114803
PROGRESS: pass 3, at document #10000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.084*"blog" + 0.045*"Back" + 0.034*"post" + 0.022*"Amazon" + 0.019*"release" + 0.016*"Education" + 0.016*"web" + 0.016*"DVD" + 0.013*"Oprah" + 0.013*"PC"
topic #22 (0.033): 0.056*"Blog" + 0.031*"week" + 0.023*"We" + 0.021*"re" + 0.020*"book" + 0.017*"believe" + 0.016*"basketball" + 0.014*"change" + 0.014*"learn" + 0.012*"see"
topic #9 (0.033): 0.037*"kids" + 0.031*"will" + 0.019*"work" + 0.014*"do" + 0.013*"don" + 0.012*"em" + 0.012*"VIDEO" + 0.012*"down" + 0.012*"talk" + 0.011*"children"
topic #19 (0.033): 0.049*"Leslie Nielsen" + 0.040*"Love" + 0.036*"Today" + 0.025*"Music" + 0.025*"need" + 0.022*"Time" + 0.018*"right now" + 0.017*"nice" + 0.017*"Life" + 0.017*"Actor"
topic #26 (0.033): 0.033*"news" + 0.031*"India" + 0.021*"New Zealand" + 0.016*"business" + 0.014*"dead" + 0.014*"killed" + 0.014*"mine" + 0.013*"police" + 0.011*"Australia" + 0.011*"NZ"
topic diff=0.040060, rho=0.114803
PROGRESS: pass 3, at document #12000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.048*"Leslie Nielsen" + 0.039*"Love" + 0.036*"Today" + 0.025*"Music" + 0.024*"need" + 0.022*"Time" + 0.018*"nice" + 0.017*"right now" + 0.017*"Life" + 0.017*"Actor"
topic #1 (0.033): 0.071*"TSA" + 0.053*"Obama" + 0.037*"GOP" + 0.022*"Senate" + 0.021*"ppl" + 0.018*"Palin" + 0.017*"Congress" + 0.015*"Republicans" + 0.015*"House" + 0.014*"vote"
topic #20 (0.033): 0.044*"live" + 0.038*"TV" + 0.035*"Watch" + 0.034*"world" + 0.028*"Check it" + 0.025*"Check it out" + 0.022*"Hope" + 0.022*"interview" + 0.022*"COP16" + 0.020*"climate"
topic #25 (0.033): 0.041*"lol" + 0.034*"tweet" + 0.034*"today" + 0.032*"people" + 0.024*"time" + 0.023*"you" + 0.022*"give me" + 0.020*"out" + 0.019*"twitter" + 0.019*"now"
topic #16 (0.033): 0.048*"Black Friday" + 0.024*"party" + 0.022*"read" + 0.021*"Sarah Palin" + 0.019*"HIV" + 0.018*"newspaper" + 0.015*"DWTS" + 0.015*"Canada" + 0.015*"cash" + 0.015*"gay"
topic diff=0.042755, rho=0.114803
PROGRESS: pass 3, at document #14000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.035*"kids" + 0.031*"will" + 0.020*"work" + 0.013*"do" + 0.012*"don" + 0.012*"talk" + 0.012*"children" + 0.012*"MSNBC" + 0.012*"VIDEO" + 0.011*"play"
topic #13 (0.033): 0.041*"AP" + 0.020*"DADT" + 0.014*"Breaking News" + 0.013*"add" + 0.013*"study" + 0.012*"risk" + 0.010*"Art" + 0.010*"Pentagon" + 0.010*"prison" + 0.010*"report"
topic #5 (0.033): 0.026*"NBC" + 0.025*"student" + 0.020*"fb" + 0.018*"high school" + 0.018*"season" + 0.017*"Wisconsin" + 0.015*"IM" + 0.013*"Football" + 0.013*"smile" + 0.012*"NY"
topic #24 (0.033): 0.084*"who" + 0.020*"Oh" + 0.020*"college" + 0.019*"Nice" + 0.019*"fat" + 0.017*"hate" + 0.016*"Yes" + 0.016*"me a" + 0.016*"a man" + 0.014*"Arizona"
topic #25 (0.033): 0.040*"lol" + 0.035*"today" + 0.034*"tweet" + 0.032*"people" + 0.025*"time" + 0.023*"you" + 0.021*"give me" + 0.021*"out" + 0.019*"twitter" + 0.019*"now"
topic diff=0.048076, rho=0.114803
PROGRESS: pass 3, at document #16000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.045*"Iran" + 0.039*"US" + 0.027*"Israel" + 0.026*"Afghanistan" + 0.019*"Pakistan" + 0.016*"Egypt" + 0.014*"to win" + 0.014*"Russia" + 0.014*"Afghan" + 0.013*"Iraq"
topic #22 (0.033): 0.057*"Blog" + 0.026*"week" + 0.024*"We" + 0.024*"re" + 0.020*"book" + 0.015*"basketball" + 0.015*"believe" + 0.014*"change" + 0.013*"WTF" + 0.013*"Post"
topic #24 (0.033): 0.085*"who" + 0.021*"Oh" + 0.019*"college" + 0.018*"Nice" + 0.018*"fat" + 0.016*"hate" + 0.016*"Yes" + 0.015*"a man" + 0.015*"me a" + 0.015*"media"
topic #21 (0.033): 0.037*"CNN" + 0.029*"Pope" + 0.026*"Chicago" + 0.026*"NATO" + 0.022*"Retweet" + 0.020*"Venezuela" + 0.020*"cancer" + 0.018*"Friday" + 0.018*"condoms" + 0.018*"education"
topic #15 (0.033): 0.022*"USA" + 0.021*"Home" + 0.021*"sex" + 0.018*"here" + 0.018*"women" + 0.018*"travel" + 0.017*"Thailand" + 0.017*"snow" + 0.013*"England" + 0.013*"Thai"
topic diff=0.025237, rho=0.114803
PROGRESS: pass 3, at document #18000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.044*"live" + 0.038*"TV" + 0.035*"Watch" + 0.032*"world" + 0.026*"Check it" + 0.023*"Check it out" + 0.022*"Hope" + 0.021*"interview" + 0.020*"climate" + 0.018*"families"
topic #23 (0.033): 0.031*"game" + 0.026*"haha" + 0.025*"show" + 0.024*"football" + 0.016*"Kanye West" + 0.014*"team" + 0.014*"net" + 0.013*"Nicki Minaj" + 0.013*"win" + 0.013*"RETWEET"
topic #18 (0.033): 0.096*"WikiLeaks" + 0.087*"Wikileaks" + 0.038*"wikileaks" + 0.025*"cablegate" + 0.022*"US" + 0.020*"Wiki Leaks" + 0.015*"cables" + 0.015*"NYT" + 0.015*"Guardian" + 0.014*"documents"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.062*"Happy Thanksgiving" + 0.041*"LOL" + 0.035*"family" + 0.034*"holiday" + 0.028*"friends" + 0.027*"Turkey" + 0.019*"thanksgiving" + 0.019*"tonight" + 0.016*"Black Friday"
topic #24 (0.033): 0.087*"who" + 0.021*"Oh" + 0.018*"Nice" + 0.018*"college" + 0.017*"fat" + 0.016*"Yes" + 0.016*"hate" + 0.015*"me a" + 0.015*"a man" + 0.015*"media"
topic diff=0.041480, rho=0.114803
bound: at document #0
-23.061 per-word bound, 8749398.4 perplexity estimate based on a held-out corpus of 2000 documents with 20435 words
PROGRESS: pass 3, at document #20000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.016*"U.S" + 0.011*"World" + 0.011*"News" + 0.011*"New York" + 0.010*"Will" + 0.009*"jajaja" + 0.009*"Health" + 0.009*"Business" + 0.008*"Women" + 0.008*"Study"
topic #16 (0.033): 0.051*"Black Friday" + 0.024*"Sarah Palin" + 0.024*"read" + 0.021*"HIV" + 0.020*"newspaper" + 0.019*"party" + 0.015*"Canada" + 0.015*"DWTS" + 0.015*"gay" + 0.014*"Here"
topic #11 (0.033): 0.054*"via" + 0.052*"iPad" + 0.045*"iPhone" + 0.043*"Google" + 0.037*"Apple" + 0.036*"Facebook" + 0.024*"Android" + 0.016*"iOS" + 0.016*"Social Media" + 0.016*"iTunes"
topic #15 (0.033): 0.024*"travel" + 0.021*"USA" + 0.020*"sex" + 0.020*"Home" + 0.018*"here" + 0.018*"women" + 0.017*"snow" + 0.016*"Thailand" + 0.012*"England" + 0.012*"film"
topic #0 (0.033): 0.085*"Haiti" + 0.027*"UN" + 0.027*"cholera" + 0.022*"election" + 0.015*"Cholera" + 0.013*"Mexico" + 0.013*"Gaza" + 0.012*"Israeli" + 0.011*"earthquake" + 0.011*"death"
topic diff=0.048152, rho=0.114803
PROGRESS: pass 3, at document #22000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.048*"lol" + 0.040*"today" + 0.031*"people" + 0.029*"tweet" + 0.026*"time" + 0.024*"out" + 0.021*"you" + 0.017*"can" + 0.017*"who" + 0.017*"love"
topic #14 (0.033): 0.059*"tcot" + 0.019*"hanging" + 0.015*"Bush" + 0.013*"Obama" + 0.012*"jobs" + 0.010*"teaparty" + 0.010*"woman" + 0.009*"Ohio" + 0.009*"Alaska" + 0.009*"Police"
topic #13 (0.033): 0.039*"AP" + 0.021*"DADT" + 0.013*"study" + 0.012*"risk" + 0.012*"report" + 0.011*"Breaking News" + 0.010*"child" + 0.010*"Pentagon" + 0.010*"add" + 0.009*"prison"
topic #22 (0.033): 0.053*"Blog" + 0.025*"We" + 0.024*"re" + 0.022*"week" + 0.021*"book" + 0.016*"basketball" + 0.014*"believe" + 0.014*"WTF" + 0.014*"Post" + 0.012*"see"
topic #11 (0.033): 0.057*"via" + 0.051*"iPad" + 0.044*"iPhone" + 0.043*"Google" + 0.037*"Apple" + 0.035*"Facebook" + 0.023*"Android" + 0.017*"Social Media" + 0.016*"iOS" + 0.015*"Christmas"
topic diff=0.018292, rho=0.114803
PROGRESS: pass 3, at document #24000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.052*"Black Friday" + 0.024*"Sarah Palin" + 0.023*"read" + 0.021*"HIV" + 0.020*"newspaper" + 0.016*"Canada" + 0.016*"party" + 0.016*"DWTS" + 0.015*"Here" + 0.014*"gay"
topic #1 (0.033): 0.074*"TSA" + 0.056*"Obama" + 0.044*"GOP" + 0.023*"Senate" + 0.019*"Palin" + 0.017*"Congress" + 0.016*"ppl" + 0.016*"House" + 0.016*"Republicans" + 0.014*"vote"
topic #24 (0.033): 0.090*"who" + 0.019*"Oh" + 0.018*"Nice" + 0.018*"Yes" + 0.018*"media" + 0.017*"I'm going" + 0.016*"me a" + 0.015*"Arizona" + 0.015*"college" + 0.014*"hate"
topic #11 (0.033): 0.058*"via" + 0.050*"iPad" + 0.046*"iPhone" + 0.041*"Google" + 0.036*"Apple" + 0.034*"Facebook" + 0.022*"Android" + 0.018*"Social Media" + 0.016*"app" + 0.015*"Christmas"
topic #2 (0.033): 0.037*"News" + 0.022*"Texas" + 0.022*"NFL" + 0.019*"Tom DeLay" + 0.017*"bit.ly" + 0.015*"You" + 0.013*"Report" + 0.013*"English" + 0.011*"Tory" + 0.011*"songs"
topic diff=0.014934, rho=0.114803
PROGRESS: pass 3, at document #26000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.086*"CNN" + 0.028*"Chicago" + 0.027*"Pope" + 0.021*"NATO" + 0.020*"cancer" + 0.017*"Retweet" + 0.017*"Friday" + 0.017*"Nation" + 0.016*"Photo" + 0.016*"condoms"
topic #18 (0.033): 0.103*"WikiLeaks" + 0.085*"Wikileaks" + 0.037*"wikileaks" + 0.027*"cablegate" + 0.023*"US" + 0.020*"Wiki Leaks" + 0.018*"NYT" + 0.016*"Guardian" + 0.014*"cables" + 0.013*"documents"
topic #16 (0.033): 0.053*"Black Friday" + 0.025*"Sarah Palin" + 0.022*"read" + 0.021*"HIV" + 0.019*"newspaper" + 0.016*"Here" + 0.016*"Canada" + 0.015*"DWTS" + 0.015*"party" + 0.014*"gay"
topic #12 (0.033): 0.066*"God" + 0.033*"Who" + 0.019*"NPR" + 0.019*"al" + 0.017*"local" + 0.014*"America" + 0.013*"help" + 0.012*"Who's" + 0.012*"left" + 0.011*"Lanvin"
topic #17 (0.033): 0.056*"China" + 0.047*"North Korea" + 0.029*"South Korea" + 0.026*"Ireland" + 0.022*"Korea" + 0.017*"Irish" + 0.017*"U.S" + 0.016*"Japan" + 0.016*"attack" + 0.015*"US"
topic diff=0.012412, rho=0.114803
PROGRESS: pass 3, at document #28000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.072*"para" + 0.065*"un" + 0.048*"Como" + 0.042*"como" + 0.026*"en" + 0.025*"es" + 0.023*"la" + 0.018*"min" + 0.017*"dos" + 0.016*"hoy"
topic #15 (0.033): 0.027*"USA" + 0.023*"travel" + 0.020*"women" + 0.019*"here" + 0.019*"snow" + 0.017*"sex" + 0.016*"Home" + 0.014*"England" + 0.012*"film" + 0.011*"Thailand"
topic #1 (0.033): 0.080*"TSA" + 0.052*"Obama" + 0.044*"GOP" + 0.023*"Senate" + 0.018*"Congress" + 0.017*"Palin" + 0.017*"Republicans" + 0.017*"ppl" + 0.016*"House" + 0.014*"vote"
topic #23 (0.033): 0.035*"football" + 0.028*"game" + 0.027*"show" + 0.022*"haha" + 0.016*"Kanye West" + 0.014*"Nicki Minaj" + 0.013*"team" + 0.012*"win" + 0.012*"net" + 0.011*"beautiful"
topic #6 (0.033): 0.015*"U.S" + 0.013*"World" + 0.011*"News" + 0.010*"Business" + 0.010*"Will" + 0.010*"New York" + 0.010*"Study" + 0.007*"Earth" + 0.007*"China" + 0.007*"Health"
topic diff=0.019715, rho=0.114803
PROGRESS: pass 3, at document #30000/143749
performing inference on a chunk of 2000 documents
1998/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.084*"blog" + 0.038*"post" + 0.024*"Amazon" + 0.019*"Back" + 0.019*"cc" + 0.019*"web" + 0.017*"release" + 0.017*"DVD" + 0.014*"Oprah" + 0.012*"Education"
topic #26 (0.033): 0.039*"India" + 0.037*"news" + 0.021*"New Zealand" + 0.019*"PM" + 0.015*"dead" + 0.014*"mine" + 0.013*"business" + 0.012*"killed" + 0.012*"Australia" + 0.011*"energy"
topic #25 (0.033): 0.043*"today" + 0.041*"lol" + 0.031*"people" + 0.025*"time" + 0.024*"out" + 0.023*"tweet" + 0.021*"you" + 0.019*"who" + 0.018*"us" + 0.018*"can"
topic #15 (0.033): 0.028*"USA" + 0.026*"travel" + 0.021*"women" + 0.020*"here" + 0.020*"snow" + 0.017*"sex" + 0.015*"Home" + 0.014*"film" + 0.013*"England" + 0.011*"French"
topic #12 (0.033): 0.066*"God" + 0.034*"Who" + 0.019*"NPR" + 0.016*"local" + 0.016*"al" + 0.015*"America" + 0.013*"Who's" + 0.012*"help" + 0.012*"DREAM Act" + 0.012*"left"
topic diff=0.048907, rho=0.114803
PROGRESS: pass 3, at document #32000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.027*"student" + 0.022*"high school" + 0.022*"NBC" + 0.020*"season" + 0.018*"Wisconsin" + 0.017*"hostage" + 0.014*"Good" + 0.013*"hostages" + 0.013*"Willie Nelson" + 0.013*"fb"
topic #8 (0.033): 0.044*"Iran" + 0.042*"US" + 0.029*"Israel" + 0.028*"Afghanistan" + 0.017*"to win" + 0.016*"Pakistan" + 0.015*"American" + 0.014*"Egypt" + 0.013*"Iraq" + 0.012*"Russia"
topic #21 (0.033): 0.068*"CNN" + 0.029*"Pope" + 0.029*"Chicago" + 0.022*"cancer" + 0.020*"NATO" + 0.019*"Retweet" + 0.017*"Friday" + 0.016*"Photo" + 0.016*"condoms" + 0.016*"Nation"
topic #11 (0.033): 0.056*"via" + 0.049*"iPhone" + 0.048*"iPad" + 0.039*"Google" + 0.036*"Apple" + 0.034*"Facebook" + 0.024*"Social Media" + 0.019*"Android" + 0.017*"Christmas" + 0.017*"iTunes"
topic #13 (0.033): 0.050*"AP" + 0.017*"DADT" + 0.014*"study" + 0.011*"report" + 0.011*"prison" + 0.011*"child" + 0.010*"risk" + 0.010*"Breaking News" + 0.009*"law" + 0.008*"Death"
topic diff=0.028742, rho=0.114803
PROGRESS: pass 3, at document #34000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.034*"football" + 0.029*"game" + 0.025*"show" + 0.020*"haha" + 0.016*"team" + 0.015*"Kanye West" + 0.013*"beautiful" + 0.013*"Nicki Minaj" + 0.013*"Film" + 0.012*"Dallas"
topic #13 (0.033): 0.049*"AP" + 0.017*"DADT" + 0.015*"study" + 0.011*"report" + 0.011*"risk" + 0.011*"prison" + 0.010*"child" + 0.010*"law" + 0.009*"Breaking News" + 0.009*"access"
topic #24 (0.033): 0.096*"who" + 0.022*"Oh" + 0.020*"Nice" + 0.019*"Arizona" + 0.017*"I'm going" + 0.017*"me a" + 0.016*"Yes" + 0.015*"hate" + 0.015*"media" + 0.014*"right"
topic #25 (0.033): 0.046*"lol" + 0.041*"today" + 0.029*"people" + 0.025*"time" + 0.023*"out" + 0.021*"you" + 0.021*"tweet" + 0.019*"who" + 0.018*"can" + 0.018*"love"
topic #2 (0.033): 0.042*"News" + 0.022*"NFL" + 0.020*"Texas" + 0.016*"Tom DeLay" + 0.016*"You" + 0.015*"English" + 0.014*"bit.ly" + 0.012*"Report" + 0.011*"Boston" + 0.011*"LMAO"
topic diff=0.031879, rho=0.114803
PROGRESS: pass 3, at document #36000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.044*"US" + 0.043*"Iran" + 0.033*"Israel" + 0.027*"Afghanistan" + 0.017*"Pakistan" + 0.016*"to win" + 0.015*"American" + 0.013*"Egypt" + 0.012*"Russia" + 0.012*"Iraq"
topic #3 (0.033): 0.055*"UK" + 0.034*"London" + 0.029*"police" + 0.018*"students" + 0.017*"photography" + 0.016*"British" + 0.015*"protest" + 0.014*"Read" + 0.014*"Police" + 0.012*"demo 2010"
topic #19 (0.033): 0.055*"Leslie Nielsen" + 0.042*"Today" + 0.031*"Love" + 0.021*"right now" + 0.019*"Life" + 0.019*"nice" + 0.019*"Time" + 0.019*"need" + 0.016*"Music" + 0.016*"Actor"
topic #9 (0.033): 0.034*"will" + 0.029*"kids" + 0.020*"work" + 0.015*"children" + 0.014*"VIDEO" + 0.013*"do" + 0.012*"talk" + 0.012*"MSNBC" + 0.011*"don" + 0.011*"em"
topic #26 (0.033): 0.043*"India" + 0.033*"news" + 0.020*"New Zealand" + 0.019*"PM" + 0.015*"dead" + 0.014*"killed" + 0.013*"mine" + 0.013*"business" + 0.011*"police" + 0.011*"energy"
topic diff=0.013400, rho=0.114803
PROGRESS: pass 3, at document #38000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.049*"AP" + 0.016*"study" + 0.015*"DADT" + 0.012*"report" + 0.011*"risk" + 0.011*"child" + 0.010*"prison" + 0.009*"Breaking News" + 0.009*"Pentagon" + 0.009*"Art"
topic #28 (0.033): 0.172*"video" + 0.134*"YouTube" + 0.056*"Video" + 0.026*"Holiday" + 0.021*"Cyber Monday" + 0.017*"music" + 0.016*"Live" + 0.015*"Check" + 0.012*"Photography" + 0.010*"HD"
topic #6 (0.033): 0.014*"World" + 0.013*"U.S" + 0.013*"News" + 0.013*"Green" + 0.011*"Will" + 0.010*"New York" + 0.009*"Study" + 0.008*"Business" + 0.008*"Women" + 0.007*"Health"
topic #15 (0.033): 0.026*"travel" + 0.023*"USA" + 0.023*"women" + 0.022*"Thailand" + 0.022*"snow" + 0.020*"here" + 0.015*"Thai" + 0.015*"sex" + 0.014*"film" + 0.013*"England"
topic #21 (0.033): 0.063*"CNN" + 0.029*"Chicago" + 0.027*"Pope" + 0.024*"cancer" + 0.020*"NATO" + 0.019*"Friday" + 0.017*"Retweet" + 0.016*"Photo" + 0.015*"up" + 0.015*"education"
topic diff=0.039491, rho=0.114803
bound: at document #0
-23.171 per-word bound, 9444804.7 perplexity estimate based on a held-out corpus of 2000 documents with 26137 words
PROGRESS: pass 3, at document #40000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.055*"Black Friday" + 0.025*"Sarah Palin" + 0.021*"HIV" + 0.021*"read" + 0.018*"Here" + 0.016*"newspaper" + 0.016*"gay" + 0.015*"Canada" + 0.014*"DWTS" + 0.012*"allies"
topic #1 (0.033): 0.085*"TSA" + 0.054*"Obama" + 0.040*"GOP" + 0.021*"Senate" + 0.018*"ppl" + 0.016*"Congress" + 0.016*"Palin" + 0.015*"Republicans" + 0.014*"House" + 0.013*"vote"
topic #3 (0.033): 0.056*"UK" + 0.035*"London" + 0.028*"police" + 0.019*"students" + 0.016*"British" + 0.015*"protest" + 0.015*"photography" + 0.014*"Police" + 0.014*"demo 2010" + 0.014*"Britain"
topic #18 (0.033): 0.098*"WikiLeaks" + 0.091*"Wikileaks" + 0.040*"wikileaks" + 0.029*"cablegate" + 0.026*"US" + 0.018*"Wiki Leaks" + 0.017*"Guardian" + 0.016*"NYT" + 0.015*"documents" + 0.013*"cables"
topic #14 (0.033): 0.061*"tcot" + 0.014*"sgp" + 0.013*"Bush" + 0.013*"Obama" + 0.011*"teaparty" + 0.011*"jobs" + 0.011*"woman" + 0.010*"AM" + 0.010*"Police" + 0.010*"FBI"
topic diff=0.027976, rho=0.114803
PROGRESS: pass 3, at document #42000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.061*"via" + 0.044*"iPad" + 0.043*"iPhone" + 0.038*"Google" + 0.035*"Apple" + 0.030*"Facebook" + 0.024*"Social Media" + 0.019*"app" + 0.019*"Christmas" + 0.017*"Android"
topic #9 (0.033): 0.034*"will" + 0.028*"kids" + 0.019*"work" + 0.015*"children" + 0.013*"talk" + 0.012*"VIDEO" + 0.012*"do" + 0.012*"don" + 0.011*"we" + 0.011*"first"
topic #17 (0.033): 0.052*"China" + 0.045*"North Korea" + 0.030*"Ireland" + 0.028*"South Korea" + 0.020*"Korea" + 0.019*"Irish" + 0.017*"US" + 0.017*"attack" + 0.016*"Japan" + 0.015*"U.S"
topic #19 (0.033): 0.053*"Leslie Nielsen" + 0.046*"Today" + 0.032*"Love" + 0.021*"Time" + 0.020*"nice" + 0.020*"right now" + 0.020*"Life" + 0.017*"Music" + 0.017*"need" + 0.015*"white"
topic #3 (0.033): 0.055*"UK" + 0.035*"London" + 0.031*"police" + 0.019*"students" + 0.016*"British" + 0.015*"Police" + 0.015*"demo 2010" + 0.014*"protest" + 0.014*"Britain" + 0.013*"photography"
topic diff=0.031472, rho=0.114803
PROGRESS: pass 3, at document #44000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.061*"tcot" + 0.024*"Bush" + 0.015*"jobs" + 0.013*"teaparty" + 0.013*"sgp" + 0.012*"Obama" + 0.010*"GM" + 0.010*"woman" + 0.010*"AM" + 0.010*"FBI"
topic #11 (0.033): 0.059*"via" + 0.045*"iPad" + 0.042*"iPhone" + 0.039*"Google" + 0.037*"Apple" + 0.031*"Facebook" + 0.023*"Social Media" + 0.019*"Android" + 0.018*"Christmas" + 0.018*"app"
topic #4 (0.033): 0.028*"U.S" + 0.026*"President" + 0.021*"Obama" + 0.020*"White House" + 0.018*"President Obama" + 0.016*"Americans" + 0.014*"military" + 0.013*"POTUS" + 0.012*"Sen" + 0.012*"S. Korea"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.062*"LOL" + 0.058*"Happy Thanksgiving" + 0.033*"holiday" + 0.033*"family" + 0.030*"friends" + 0.026*"Turkey" + 0.025*"thanksgiving" + 0.023*"tonight" + 0.012*"Black Friday"
topic #17 (0.033): 0.052*"China" + 0.046*"North Korea" + 0.031*"Ireland" + 0.028*"South Korea" + 0.020*"Irish" + 0.019*"Korea" + 0.017*"US" + 0.017*"attack" + 0.015*"Japan" + 0.015*"U.S"
topic diff=0.035298, rho=0.114803
PROGRESS: pass 3, at document #46000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.034*"News" + 0.023*"LMAO" + 0.023*"Texas" + 0.019*"NFL" + 0.015*"Tom DeLay" + 0.014*"English" + 0.014*"You" + 0.013*"bit.ly" + 0.012*"Report" + 0.011*"Houston"
topic #22 (0.033): 0.062*"Blog" + 0.023*"re" + 0.023*"We" + 0.022*"book" + 0.018*"food" + 0.016*"week" + 0.014*"basketball" + 0.014*"see" + 0.013*"Post" + 0.013*"WTF"
topic #17 (0.033): 0.054*"China" + 0.045*"North Korea" + 0.031*"Ireland" + 0.027*"South Korea" + 0.022*"Irish" + 0.019*"Korea" + 0.017*"US" + 0.017*"attack" + 0.017*"Japan" + 0.016*"U.S"
topic #28 (0.033): 0.148*"video" + 0.107*"YouTube" + 0.055*"Video" + 0.027*"Holiday" + 0.024*"Cyber Monday" + 0.019*"music" + 0.016*"Check" + 0.016*"Live" + 0.011*"Photography" + 0.011*"Tonight"
topic #4 (0.033): 0.030*"U.S" + 0.025*"President" + 0.024*"Obama" + 0.021*"White House" + 0.018*"TIME" + 0.017*"Americans" + 0.016*"President Obama" + 0.013*"military" + 0.012*"Hill" + 0.012*"POTUS"
topic diff=0.111407, rho=0.114803
PROGRESS: pass 3, at document #48000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.040*"AP" + 0.023*"DADT" + 0.014*"study" + 0.012*"Pentagon" + 0.011*"risk" + 0.011*"child" + 0.011*"report" + 0.009*"prison" + 0.009*"Debt" + 0.009*"Portland"
topic #4 (0.033): 0.031*"U.S" + 0.025*"President" + 0.024*"Obama" + 0.020*"White House" + 0.017*"President Obama" + 0.017*"Americans" + 0.016*"TIME" + 0.012*"POTUS" + 0.012*"military" + 0.012*"Hill"
topic #8 (0.033): 0.056*"Iran" + 0.045*"US" + 0.030*"BBC" + 0.029*"Israel" + 0.025*"Afghanistan" + 0.020*"to win" + 0.017*"Pakistan" + 0.014*"Russia" + 0.013*"American" + 0.012*"Egypt"
topic #20 (0.033): 0.042*"live" + 0.039*"TV" + 0.036*"world" + 0.029*"Watch" + 0.024*"interview" + 0.022*"Hope" + 0.022*"families" + 0.021*"climate" + 0.019*"Check it" + 0.016*"Check it out"
topic #26 (0.033): 0.037*"India" + 0.028*"news" + 0.020*"New Zealand" + 0.017*"PM" + 0.014*"dead" + 0.013*"mine" + 0.013*"business" + 0.013*"killed" + 0.011*"police" + 0.010*"Australia"
topic diff=0.021050, rho=0.114803
PROGRESS: pass 3, at document #50000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.097*"Haiti" + 0.026*"cholera" + 0.022*"Atlantic" + 0.021*"Cholera" + 0.020*"UN" + 0.019*"election" + 0.015*"tweeted" + 0.015*"Mexico" + 0.014*"Israeli" + 0.012*"Gaza"
topic #11 (0.033): 0.056*"via" + 0.047*"iPad" + 0.042*"iPhone" + 0.037*"Apple" + 0.037*"Google" + 0.028*"Facebook" + 0.021*"Social Media" + 0.019*"iTunes" + 0.019*"Christmas" + 0.018*"app"
topic #1 (0.033): 0.070*"TSA" + 0.063*"GOP" + 0.058*"Obama" + 0.021*"Senate" + 0.020*"ppl" + 0.019*"Palin" + 0.017*"Republicans" + 0.016*"Congress" + 0.015*"vote" + 0.014*"House"
topic #6 (0.033): 0.014*"NYT" + 0.014*"U.S" + 0.013*"World" + 0.012*"Will" + 0.011*"News" + 0.011*"Economist" + 0.010*"Study" + 0.010*"New York" + 0.009*"Tea Party" + 0.009*"Green"
topic #9 (0.033): 0.033*"will" + 0.023*"kids" + 0.020*"work" + 0.014*"talk" + 0.013*"children" + 0.013*"first" + 0.013*"play" + 0.012*"MSNBC" + 0.012*"bet" + 0.012*"do"
topic diff=0.027498, rho=0.114803
PROGRESS: pass 3, at document #52000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.093*"Haiti" + 0.025*"cholera" + 0.021*"Atlantic" + 0.020*"UN" + 0.020*"Cholera" + 0.019*"election" + 0.015*"tweeted" + 0.014*"Mexico" + 0.013*"Israeli" + 0.012*"Gaza"
topic #24 (0.033): 0.090*"who" + 0.034*"twit" + 0.021*"Oh" + 0.021*"me a" + 0.020*"Arizona" + 0.019*"Nice" + 0.017*"Yes" + 0.016*"a man" + 0.016*"media" + 0.015*"right"
topic #21 (0.033): 0.057*"CNN" + 0.037*"Chicago" + 0.033*"Retweet" + 0.026*"Pope" + 0.023*"Friday" + 0.022*"cancer" + 0.017*"education" + 0.017*"NATO" + 0.016*"Illinois" + 0.015*"Nation"
topic #4 (0.033): 0.029*"U.S" + 0.025*"President" + 0.022*"Obama" + 0.020*"White House" + 0.017*"President Obama" + 0.016*"Americans" + 0.015*"TIME" + 0.013*"POTUS" + 0.012*"Sen" + 0.012*"military"
topic #7 (0.033): 0.165*"Twitter" + 0.077*"Facebook" + 0.035*"NYC" + 0.023*"Tweet" + 0.018*"photos" + 0.015*"Lady Gaga" + 0.013*"Chinese" + 0.010*"People" + 0.010*"website" + 0.010*"Internet"
topic diff=0.042141, rho=0.114803
PROGRESS: pass 3, at document #54000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.048*"Black Friday" + 0.027*"Sarah Palin" + 0.022*"HIV" + 0.019*"DWTS" + 0.019*"read" + 0.016*"Canada" + 0.016*"gay" + 0.015*"Here" + 0.015*"Bristol" + 0.012*"If"
topic #11 (0.033): 0.059*"via" + 0.048*"iPad" + 0.046*"iPhone" + 0.036*"Apple" + 0.035*"Google" + 0.027*"Facebook" + 0.021*"Social Media" + 0.019*"iTunes" + 0.019*"Christmas" + 0.018*"app"
topic #12 (0.033): 0.082*"God" + 0.039*"Who" + 0.021*"GOD" + 0.017*"NPR" + 0.016*"Who's" + 0.013*"left" + 0.012*"America" + 0.012*"guy" + 0.012*"help" + 0.012*"local"
topic #21 (0.033): 0.054*"CNN" + 0.036*"Chicago" + 0.031*"Retweet" + 0.026*"Pope" + 0.024*"Friday" + 0.023*"cancer" + 0.016*"NATO" + 0.016*"Illinois" + 0.016*"education" + 0.015*"Nation"
topic #24 (0.033): 0.092*"who" + 0.030*"twit" + 0.022*"me a" + 0.022*"Oh" + 0.020*"Arizona" + 0.019*"Nice" + 0.016*"Yes" + 0.016*"I'm going" + 0.016*"a man" + 0.015*"right"
topic diff=0.017520, rho=0.114803
PROGRESS: pass 3, at document #56000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.048*"China" + 0.045*"North Korea" + 0.033*"Ireland" + 0.029*"South Korea" + 0.021*"Korea" + 0.019*"Irish" + 0.017*"attack" + 0.016*"Japan" + 0.016*"US" + 0.014*"U.S"
topic #19 (0.033): 0.047*"Leslie Nielsen" + 0.044*"Today" + 0.035*"Love" + 0.022*"Music" + 0.022*"right now" + 0.021*"nice" + 0.019*"Time" + 0.017*"white" + 0.017*"Washington" + 0.016*"Life"
topic #12 (0.033): 0.081*"God" + 0.039*"Who" + 0.021*"GOD" + 0.018*"NPR" + 0.016*"Who's" + 0.013*"help" + 0.013*"left" + 0.013*"guy" + 0.012*"America" + 0.011*"local"
topic #4 (0.033): 0.028*"U.S" + 0.023*"President" + 0.021*"Obama" + 0.019*"White House" + 0.016*"President Obama" + 0.016*"Americans" + 0.014*"TIME" + 0.013*"military" + 0.011*"POTUS" + 0.011*"Sen"
topic #1 (0.033): 0.072*"TSA" + 0.058*"GOP" + 0.055*"Obama" + 0.023*"ppl" + 0.021*"Senate" + 0.019*"Palin" + 0.015*"Republicans" + 0.015*"vote" + 0.015*"Congress" + 0.014*"House"
topic diff=0.030998, rho=0.114803
PROGRESS: pass 3, at document #58000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.048*"Black Friday" + 0.026*"Sarah Palin" + 0.025*"HIV" + 0.021*"Canada" + 0.020*"DWTS" + 0.017*"gay" + 0.017*"read" + 0.015*"Here" + 0.013*"Bristol" + 0.012*"If"
topic #29 (0.033): 0.203*"Thanksgiving" + 0.064*"LOL" + 0.062*"Happy Thanksgiving" + 0.033*"family" + 0.031*"holiday" + 0.029*"thanksgiving" + 0.029*"friends" + 0.027*"Turkey" + 0.024*"tonight" + 0.015*"Black Friday"
topic #4 (0.033): 0.029*"U.S" + 0.021*"President" + 0.021*"Obama" + 0.019*"White House" + 0.016*"President Obama" + 0.015*"Americans" + 0.014*"military" + 0.013*"TIME" + 0.011*"Sen" + 0.011*"cooking"
topic #22 (0.033): 0.056*"Blog" + 0.026*"We" + 0.023*"re" + 0.021*"book" + 0.018*"WTF" + 0.017*"food" + 0.015*"week" + 0.014*"health" + 0.013*"basketball" + 0.013*"believe"
topic #7 (0.033): 0.159*"Twitter" + 0.078*"Facebook" + 0.035*"NYC" + 0.025*"Tweet" + 0.016*"photos" + 0.015*"Chinese" + 0.015*"Lady Gaga" + 0.011*"Thx" + 0.011*"website" + 0.010*"People"
topic diff=0.020534, rho=0.114803
bound: at document #0
-24.502 per-word bound, 23766798.3 perplexity estimate based on a held-out corpus of 2000 documents with 40495 words
PROGRESS: pass 3, at document #60000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.054*"Leslie Nielsen" + 0.045*"Today" + 0.044*"Love" + 0.026*"right now" + 0.020*"Music" + 0.020*"white" + 0.019*"nice" + 0.018*"Time" + 0.017*"Life" + 0.014*"need"
topic #11 (0.033): 0.054*"via" + 0.046*"iPad" + 0.042*"iPhone" + 0.033*"Google" + 0.033*"Apple" + 0.026*"Facebook" + 0.020*"Beatles" + 0.019*"iTunes" + 0.019*"Christmas" + 0.019*"Social Media"
topic #9 (0.033): 0.034*"will" + 0.021*"kids" + 0.020*"work" + 0.014*"do" + 0.013*"children" + 0.013*"em" + 0.012*"music" + 0.012*"play" + 0.012*"first" + 0.012*"MSNBC"
topic #22 (0.033): 0.053*"Blog" + 0.024*"We" + 0.022*"re" + 0.020*"book" + 0.020*"RTs" + 0.018*"food" + 0.016*"WTF" + 0.015*"week" + 0.014*"basketball" + 0.014*"believe"
topic #17 (0.033): 0.048*"China" + 0.045*"North Korea" + 0.031*"Ireland" + 0.029*"South Korea" + 0.019*"Korea" + 0.017*"Irish" + 0.016*"attack" + 0.016*"Japan" + 0.016*"US" + 0.014*"U.S"
topic diff=0.022412, rho=0.114803
PROGRESS: pass 3, at document #62000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #26 (0.033): 0.035*"India" + 0.029*"news" + 0.020*"New Zealand" + 0.016*"dead" + 0.014*"mine" + 0.013*"business" + 0.013*"energy" + 0.013*"PM" + 0.012*"killed" + 0.011*"police"
topic #18 (0.033): 0.103*"WikiLeaks" + 0.086*"Wikileaks" + 0.033*"wikileaks" + 0.029*"cablegate" + 0.027*"NYT" + 0.023*"US" + 0.021*"Wiki Leaks" + 0.017*"Guardian" + 0.015*"documents" + 0.012*"cables"
topic #4 (0.033): 0.029*"U.S" + 0.021*"President" + 0.020*"Obama" + 0.018*"White House" + 0.016*"President Obama" + 0.015*"Americans" + 0.015*"Cancun" + 0.014*"military" + 0.011*"TIME" + 0.011*"Sen"
topic #21 (0.033): 0.048*"CNN" + 0.043*"FARC" + 0.042*"Chicago" + 0.027*"Retweet" + 0.023*"cancer" + 0.023*"Pope" + 0.020*"Friday" + 0.017*"NATO" + 0.017*"Toronto" + 0.015*"education"
topic #1 (0.033): 0.075*"TSA" + 0.052*"Obama" + 0.049*"GOP" + 0.026*"ppl" + 0.020*"Senate" + 0.018*"Palin" + 0.015*"Congress" + 0.015*"vote" + 0.014*"Republicans" + 0.013*"House"
topic diff=0.034694, rho=0.114803
PROGRESS: pass 3, at document #64000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.045*"haha" + 0.036*"game" + 0.035*"football" + 0.034*"show" + 0.014*"Kanye West" + 0.014*"team" + 0.014*"beautiful" + 0.013*"Nicki Minaj" + 0.013*"hair" + 0.012*"OK"
topic #24 (0.033): 0.095*"who" + 0.025*"me a" + 0.020*"Nice" + 0.019*"a man" + 0.019*"twit" + 0.018*"Oh" + 0.018*"hate" + 0.017*"Yes" + 0.016*"right" + 0.016*"I'm going"
topic #7 (0.033): 0.156*"Twitter" + 0.079*"Facebook" + 0.034*"NYC" + 0.025*"Tweet" + 0.024*"website" + 0.017*"photos" + 0.015*"Chinese" + 0.014*"Lady Gaga" + 0.010*"People" + 0.010*"Internet"
topic #17 (0.033): 0.054*"China" + 0.044*"North Korea" + 0.028*"Ireland" + 0.028*"South Korea" + 0.020*"Korea" + 0.017*"Japan" + 0.016*"Irish" + 0.015*"attack" + 0.015*"US" + 0.015*"U.S"
topic #28 (0.033): 0.163*"video" + 0.146*"YouTube" + 0.050*"Video" + 0.027*"Holiday" + 0.022*"Cyber Monday" + 0.021*"music" + 0.014*"Check" + 0.014*"Live" + 0.011*"ad" + 0.011*"Tonight"
topic diff=0.027839, rho=0.114803
PROGRESS: pass 3, at document #66000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.041*"News" + 0.038*"son" + 0.020*"NFL" + 0.018*"Texas" + 0.017*"LMAO" + 0.015*"Houston" + 0.015*"Tom DeLay" + 0.013*"EUA" + 0.013*"Boston" + 0.013*"Report"
topic #21 (0.033): 0.049*"CNN" + 0.038*"Chicago" + 0.035*"FARC" + 0.030*"cancer" + 0.026*"Retweet" + 0.022*"Pope" + 0.020*"Friday" + 0.019*"NATO" + 0.016*"education" + 0.015*"up"
topic #20 (0.033): 0.042*"live" + 0.039*"TV" + 0.034*"world" + 0.032*"Watch" + 0.025*"Radio" + 0.023*"interview" + 0.022*"climate" + 0.021*"Hope" + 0.020*"climate change" + 0.018*"families"
topic #26 (0.033): 0.042*"India" + 0.028*"news" + 0.019*"New Zealand" + 0.017*"dead" + 0.014*"business" + 0.014*"mine" + 0.013*"PM" + 0.012*"energy" + 0.012*"killed" + 0.012*"police"
topic #22 (0.033): 0.042*"Blog" + 0.030*"see" + 0.026*"artist" + 0.024*"birth" + 0.023*"We" + 0.022*"heroes" + 0.021*"re" + 0.017*"food" + 0.017*"book" + 0.015*"RTs"
topic diff=0.032068, rho=0.114803
PROGRESS: pass 3, at document #68000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.059*"China" + 0.044*"North Korea" + 0.030*"South Korea" + 0.025*"Ireland" + 0.022*"Korea" + 0.017*"attack" + 0.017*"Japan" + 0.015*"US" + 0.015*"U.S" + 0.014*"Irish"
topic #1 (0.033): 0.070*"TSA" + 0.063*"Obama" + 0.050*"GOP" + 0.025*"ppl" + 0.020*"Senate" + 0.016*"Palin" + 0.015*"Republicans" + 0.014*"vote" + 0.014*"House" + 0.014*"Congress"
topic #27 (0.033): 0.089*"blog" + 0.033*"post" + 0.022*"Amazon" + 0.022*"DVD" + 0.016*"release" + 0.015*"web" + 0.014*"Blogger" + 0.014*"Oprah" + 0.014*"check it" + 0.012*"Win"
topic #20 (0.033): 0.043*"live" + 0.038*"TV" + 0.034*"world" + 0.032*"Watch" + 0.025*"Radio" + 0.024*"interview" + 0.022*"Hope" + 0.021*"climate" + 0.020*"Check it" + 0.019*"climate change"
topic #16 (0.033): 0.045*"Black Friday" + 0.030*"HIV" + 0.025*"Sarah Palin" + 0.022*"DJ" + 0.019*"gay" + 0.018*"DWTS" + 0.015*"Canada" + 0.015*"Here" + 0.014*"read" + 0.012*"Dec"
topic diff=0.017306, rho=0.114803
PROGRESS: pass 3, at document #70000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.123*"God" + 0.039*"Who" + 0.018*"GOD" + 0.016*"help" + 0.015*"left" + 0.014*"heart" + 0.014*"guy" + 0.014*"Who's" + 0.013*"America" + 0.013*"NPR"
topic #25 (0.033): 0.056*"lol" + 0.037*"today" + 0.032*"people" + 0.025*"time" + 0.025*"you" + 0.024*"ur" + 0.022*"can" + 0.020*"out" + 0.020*"who" + 0.018*"love"
topic #15 (0.033): 0.025*"snow" + 0.024*"women" + 0.024*"here" + 0.022*"USA" + 0.019*"travel" + 0.016*"England" + 0.014*"Paris" + 0.014*"sex" + 0.013*"film" + 0.011*"baby"
topic #16 (0.033): 0.044*"Black Friday" + 0.027*"HIV" + 0.027*"Sarah Palin" + 0.020*"DJ" + 0.018*"gay" + 0.017*"DWTS" + 0.017*"Canada" + 0.014*"read" + 0.014*"Here" + 0.012*"If"
topic #14 (0.033): 0.070*"tcot" + 0.016*"Bush" + 0.013*"Obama" + 0.012*"teaparty" + 0.012*"GM" + 0.012*"jobs" + 0.011*"sgp" + 0.010*"woman" + 0.010*"arrested" + 0.008*"single"
topic diff=0.019939, rho=0.114803
PROGRESS: pass 3, at document #72000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.041*"live" + 0.036*"TV" + 0.033*"world" + 0.029*"Watch" + 0.027*"Hope" + 0.025*"interview" + 0.024*"Radio" + 0.021*"climate" + 0.020*"climate change" + 0.019*"Check it"
topic #19 (0.033): 0.058*"Leslie Nielsen" + 0.046*"Love" + 0.043*"Today" + 0.023*"right now" + 0.022*"nice" + 0.020*"Music" + 0.019*"white" + 0.018*"Life" + 0.017*"Time" + 0.014*"need"
topic #21 (0.033): 0.042*"CNN" + 0.038*"Chicago" + 0.034*"cancer" + 0.025*"FARC" + 0.024*"Retweet" + 0.023*"Friday" + 0.019*"Pope" + 0.018*"NATO" + 0.016*"Photo" + 0.015*"up"
topic #0 (0.033): 0.104*"Haiti" + 0.032*"cholera" + 0.023*"UN" + 0.019*"Cholera" + 0.016*"Mexico" + 0.016*"election" + 0.014*"tweeted" + 0.013*"death" + 0.012*"Elections" + 0.010*"Asian"
topic #1 (0.033): 0.100*"TSA" + 0.060*"Obama" + 0.043*"GOP" + 0.028*"ppl" + 0.020*"Senate" + 0.016*"Palin" + 0.014*"vote" + 0.014*"Congress" + 0.013*"House" + 0.013*"Republicans"
topic diff=0.014562, rho=0.114803
PROGRESS: pass 3, at document #74000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.036*"will" + 0.020*"work" + 0.019*"kids" + 0.015*"do" + 0.015*"VIDEO" + 0.014*"em" + 0.014*"children" + 0.014*"MSNBC" + 0.013*"mind" + 0.013*"first"
topic #2 (0.033): 0.043*"News" + 0.026*"son" + 0.021*"NFL" + 0.019*"LMAO" + 0.018*"Texas" + 0.014*"Houston" + 0.014*"Boston" + 0.013*"Tom DeLay" + 0.012*"songs" + 0.012*"Las Vegas"
topic #0 (0.033): 0.101*"Haiti" + 0.032*"cholera" + 0.023*"UN" + 0.019*"Cholera" + 0.017*"Mexico" + 0.016*"election" + 0.013*"death" + 0.013*"tweeted" + 0.012*"Asian" + 0.012*"Elections"
topic #4 (0.033): 0.028*"U.S" + 0.025*"President" + 0.020*"Obama" + 0.017*"President Obama" + 0.017*"S. Korea" + 0.015*"White House" + 0.014*"Americans" + 0.014*"military" + 0.012*"American" + 0.012*"Cancun"
topic #22 (0.033): 0.041*"Blog" + 0.027*"see" + 0.023*"We" + 0.022*"re" + 0.019*"artist" + 0.018*"book" + 0.017*"food" + 0.016*"birth" + 0.015*"WTF" + 0.015*"heroes"
topic diff=0.032843, rho=0.114803
PROGRESS: pass 3, at document #76000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.040*"Blog" + 0.025*"see" + 0.023*"We" + 0.022*"re" + 0.019*"artist" + 0.018*"book" + 0.017*"food" + 0.015*"WTF" + 0.014*"birth" + 0.014*"health"
topic #7 (0.033): 0.150*"Twitter" + 0.081*"Facebook" + 0.029*"NYC" + 0.028*"Tweet" + 0.019*"website" + 0.016*"photos" + 0.015*"Chinese" + 0.012*"People" + 0.010*"Internet" + 0.010*"Lady Gaga"
topic #0 (0.033): 0.104*"Haiti" + 0.032*"cholera" + 0.023*"UN" + 0.019*"Cholera" + 0.016*"Mexico" + 0.016*"election" + 0.014*"death" + 0.013*"tweeted" + 0.012*"Asian" + 0.011*"gold"
topic #14 (0.033): 0.066*"tcot" + 0.015*"Bush" + 0.014*"Obama" + 0.012*"jobs" + 0.011*"teaparty" + 0.011*"GM" + 0.011*"woman" + 0.010*"arrested" + 0.009*"sgp" + 0.009*"job"
topic #1 (0.033): 0.093*"TSA" + 0.059*"Obama" + 0.045*"GOP" + 0.028*"ppl" + 0.020*"Senate" + 0.016*"Palin" + 0.016*"Congress" + 0.014*"vote" + 0.013*"House" + 0.013*"Republicans"
topic diff=0.028118, rho=0.114803
PROGRESS: pass 3, at document #78000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.047*"Blog" + 0.026*"see" + 0.024*"We" + 0.022*"re" + 0.018*"artist" + 0.017*"book" + 0.017*"food" + 0.015*"health" + 0.014*"WTF" + 0.014*"Post"
topic #28 (0.033): 0.156*"video" + 0.096*"YouTube" + 0.058*"Video" + 0.024*"Holiday" + 0.023*"Cyber Monday" + 0.023*"music" + 0.019*"Check" + 0.015*"Live" + 0.014*"Photography" + 0.013*"ad"
topic #24 (0.033): 0.097*"who" + 0.022*"me a" + 0.021*"Nice" + 0.020*"hate" + 0.019*"I'm going" + 0.019*"a man" + 0.018*"right" + 0.017*"Oh" + 0.017*"Yes" + 0.017*"always"
topic #3 (0.033): 0.052*"UK" + 0.035*"London" + 0.035*"BBC" + 0.032*"photography" + 0.025*"police" + 0.018*"British" + 0.017*"students" + 0.014*"protest" + 0.014*"Police" + 0.012*"Britain"
topic #6 (0.033): 0.014*"U.S" + 0.013*"News" + 0.012*"World" + 0.011*"New York" + 0.010*"Will" + 0.009*"NYT" + 0.009*"Cancer" + 0.009*"Study" + 0.009*"Green" + 0.008*"Pres"
topic diff=0.029093, rho=0.114803
bound: at document #0
-22.643 per-word bound, 6549463.7 perplexity estimate based on a held-out corpus of 2000 documents with 49203 words
PROGRESS: pass 3, at document #80000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.059*"China" + 0.048*"North Korea" + 0.031*"South Korea" + 0.022*"Korea" + 0.020*"Ireland" + 0.018*"attack" + 0.018*"US" + 0.016*"Reuters" + 0.016*"Japan" + 0.014*"U.S"
topic #26 (0.033): 0.039*"India" + 0.030*"news" + 0.022*"business" + 0.018*"New Zealand" + 0.015*"dead" + 0.014*"mine" + 0.013*"energy" + 0.013*"PM" + 0.011*"killed" + 0.011*"Australia"
topic #25 (0.033): 0.045*"lol" + 0.036*"today" + 0.032*"people" + 0.028*"time" + 0.027*"you" + 0.021*"can" + 0.020*"who" + 0.020*"out" + 0.019*"ur" + 0.018*"love"
topic #21 (0.033): 0.045*"CNN" + 0.031*"Chicago" + 0.028*"Pope" + 0.027*"cancer" + 0.023*"Retweet" + 0.020*"Friday" + 0.019*"FARC" + 0.018*"NATO" + 0.018*"condoms" + 0.017*"Venezuela"
topic #11 (0.033): 0.057*"via" + 0.042*"iPad" + 0.034*"Google" + 0.033*"iPhone" + 0.029*"Facebook" + 0.028*"Apple" + 0.022*"Christmas" + 0.020*"iTunes" + 0.018*"online" + 0.016*"app"
topic diff=0.059810, rho=0.114803
PROGRESS: pass 3, at document #82000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.058*"China" + 0.047*"North Korea" + 0.029*"South Korea" + 0.025*"Ireland" + 0.020*"Korea" + 0.019*"Europe" + 0.018*"US" + 0.018*"attack" + 0.016*"Reuters" + 0.015*"U.S"
topic #24 (0.033): 0.097*"who" + 0.023*"me a" + 0.021*"right" + 0.021*"Nice" + 0.019*"hate" + 0.018*"a man" + 0.017*"I'm going" + 0.017*"Yes" + 0.016*"Oh" + 0.015*"always"
topic #23 (0.033): 0.056*"haha" + 0.037*"game" + 0.032*"show" + 0.031*"football" + 0.019*"Kanye West" + 0.017*"net" + 0.016*"beautiful" + 0.015*"team" + 0.014*"hair" + 0.013*"thoughts"
topic #29 (0.033): 0.207*"Thanksgiving" + 0.074*"Happy Thanksgiving" + 0.040*"LOL" + 0.036*"thanksgiving" + 0.035*"family" + 0.034*"friends" + 0.033*"Turkey" + 0.028*"holiday" + 0.022*"tonight" + 0.012*"Black Friday"
topic #13 (0.033): 0.038*"AP" + 0.019*"DADT" + 0.015*"study" + 0.014*"risk" + 0.013*"child" + 0.011*"Debt" + 0.010*"Pentagon" + 0.010*"friend" + 0.009*"report" + 0.009*"law"
topic diff=0.123190, rho=0.114803
PROGRESS: pass 3, at document #84000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.057*"UK" + 0.031*"London" + 0.024*"photography" + 0.024*"BBC" + 0.023*"police" + 0.022*"students" + 0.019*"British" + 0.015*"protest" + 0.015*"Colombia" + 0.015*"school"
topic #4 (0.033): 0.027*"U.S" + 0.023*"President" + 0.021*"Obama" + 0.021*"cooking" + 0.016*"White House" + 0.016*"military" + 0.016*"Americans" + 0.016*"Cancun" + 0.015*"S. Korea" + 0.014*"President Obama"
topic #6 (0.033): 0.014*"U.S" + 0.013*"News" + 0.013*"World" + 0.009*"Will" + 0.009*"New York" + 0.009*"Women" + 0.009*"France" + 0.009*"Study" + 0.009*"NYT" + 0.008*"the Internet"
topic #9 (0.033): 0.038*"will" + 0.020*"work" + 0.020*"kids" + 0.017*"do" + 0.017*"em" + 0.014*"first" + 0.013*"play" + 0.013*"mind" + 0.013*"children" + 0.012*"something"
topic #14 (0.033): 0.074*"tcot" + 0.019*"Bush" + 0.016*"Islam" + 0.014*"Obama" + 0.013*"jobs" + 0.012*"Muslim" + 0.011*"woman" + 0.010*"teaparty" + 0.010*"GM" + 0.010*"state"
topic diff=0.018286, rho=0.114803
PROGRESS: pass 3, at document #86000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.083*"Iran" + 0.045*"Israel" + 0.043*"US" + 0.034*"Afghanistan" + 0.021*"Iraq" + 0.017*"NATO" + 0.016*"Russia" + 0.016*"Pakistan" + 0.015*"Egypt" + 0.015*"American"
topic #11 (0.033): 0.069*"via" + 0.036*"iPad" + 0.035*"Google" + 0.029*"iPhone" + 0.029*"Facebook" + 0.026*"Apple" + 0.022*"Christmas" + 0.018*"online" + 0.018*"iTunes" + 0.016*"Free"
topic #29 (0.033): 0.208*"Thanksgiving" + 0.083*"Happy Thanksgiving" + 0.044*"LOL" + 0.035*"family" + 0.035*"Turkey" + 0.034*"thanksgiving" + 0.034*"friends" + 0.028*"holiday" + 0.021*"tonight" + 0.012*"eat"
topic #15 (0.033): 0.036*"USA" + 0.028*"women" + 0.022*"here" + 0.019*"snow" + 0.018*"sex" + 0.016*"travel" + 0.015*"England" + 0.015*"Paris" + 0.012*"film" + 0.011*"baby"
topic #6 (0.033): 0.014*"U.S" + 0.012*"World" + 0.012*"News" + 0.010*"New York" + 0.009*"Will" + 0.009*"Women" + 0.009*"NYT" + 0.008*"Study" + 0.008*"France" + 0.008*"the Internet"
topic diff=0.032208, rho=0.114803
PROGRESS: pass 3, at document #88000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.099*"blog" + 0.045*"post" + 0.022*"Education" + 0.021*"DVD" + 0.020*"recipes" + 0.020*"Complexo do Alemão" + 0.017*"Amazon" + 0.015*"após" + 0.014*"web" + 0.013*"release"
topic #20 (0.033): 0.045*"live" + 0.037*"TV" + 0.035*"world" + 0.032*"Watch" + 0.028*"Hope" + 0.020*"interview" + 0.019*"climate" + 0.017*"climate change" + 0.017*"Check it" + 0.017*"COP16"
topic #22 (0.033): 0.040*"Blog" + 0.025*"re" + 0.025*"We" + 0.023*"see" + 0.018*"believe" + 0.017*"food" + 0.016*"book" + 0.015*"WTF" + 0.013*"health" + 0.013*"basketball"
topic #8 (0.033): 0.078*"Iran" + 0.043*"Israel" + 0.042*"US" + 0.033*"Afghanistan" + 0.020*"Iraq" + 0.018*"Pakistan" + 0.016*"NATO" + 0.016*"Brasil" + 0.015*"Russia" + 0.014*"Egypt"
topic #10 (0.033): 0.110*"para" + 0.066*"un" + 0.046*"como" + 0.039*"dos" + 0.030*"en" + 0.024*"es" + 0.022*"pour" + 0.022*"la" + 0.019*"se" + 0.015*"las"
topic diff=0.031361, rho=0.114803
PROGRESS: pass 3, at document #90000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.125*"video" + 0.078*"YouTube" + 0.061*"Video" + 0.031*"Cyber Monday" + 0.027*"Holiday" + 0.025*"music" + 0.016*"Check" + 0.014*"ad" + 0.014*"Photography" + 0.013*"Live"
topic #4 (0.033): 0.026*"U.S" + 0.022*"President" + 0.021*"Obama" + 0.021*"Seoul" + 0.018*"cooking" + 0.016*"Americans" + 0.016*"White House" + 0.015*"military" + 0.015*"Cancun" + 0.014*"President Obama"
topic #7 (0.033): 0.161*"Twitter" + 0.079*"Facebook" + 0.027*"NYC" + 0.027*"Tweet" + 0.015*"website" + 0.014*"Chinese" + 0.014*"photos" + 0.012*"Thx" + 0.012*"Tweets" + 0.012*"Lmao"
topic #1 (0.033): 0.075*"TSA" + 0.060*"Obama" + 0.044*"GOP" + 0.032*"ppl" + 0.018*"Senate" + 0.018*"Congress" + 0.016*"Palin" + 0.015*"Republicans" + 0.015*"politics" + 0.013*"House"
topic #24 (0.033): 0.098*"who" + 0.024*"me a" + 0.021*"Nice" + 0.020*"right" + 0.019*"hate" + 0.018*"a man" + 0.018*"Oh" + 0.017*"Arizona" + 0.017*"Yes" + 0.016*"I'm going"
topic diff=0.024343, rho=0.114803
PROGRESS: pass 3, at document #92000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.042*"live" + 0.036*"TV" + 0.035*"world" + 0.032*"Watch" + 0.027*"Hope" + 0.020*"interview" + 0.018*"Check it" + 0.018*"COP16" + 0.018*"families" + 0.018*"climate"
topic #11 (0.033): 0.066*"via" + 0.038*"Google" + 0.036*"iPad" + 0.032*"iPhone" + 0.031*"Facebook" + 0.026*"Apple" + 0.021*"Christmas" + 0.019*"online" + 0.017*"iTunes" + 0.017*"Social Media"
topic #13 (0.033): 0.052*"AP" + 0.019*"DADT" + 0.015*"risk" + 0.015*"study" + 0.013*"child" + 0.011*"prison" + 0.010*"report" + 0.010*"Pentagon" + 0.009*"law" + 0.008*"friend"
topic #2 (0.033): 0.037*"News" + 0.023*"bit.ly" + 0.022*"NFL" + 0.019*"Texas" + 0.019*"EUA" + 0.015*"son" + 0.014*"Report" + 0.014*"LMAO" + 0.014*"English" + 0.014*"Jewish"
topic #10 (0.033): 0.111*"para" + 0.065*"un" + 0.045*"como" + 0.039*"dos" + 0.031*"en" + 0.024*"es" + 0.023*"la" + 0.020*"pour" + 0.020*"se" + 0.014*"si"
topic diff=0.023228, rho=0.114803
PROGRESS: pass 3, at document #94000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.119*"video" + 0.069*"YouTube" + 0.063*"Video" + 0.028*"Cyber Monday" + 0.026*"Holiday" + 0.023*"music" + 0.016*"Check" + 0.015*"Live" + 0.013*"ad" + 0.013*"Photography"
topic #27 (0.033): 0.092*"blog" + 0.042*"post" + 0.023*"DVD" + 0.019*"Education" + 0.019*"Complexo do Alemão" + 0.016*"release" + 0.016*"recipes" + 0.015*"web" + 0.014*"Amazon" + 0.014*"PC"
topic #0 (0.033): 0.079*"Haiti" + 0.026*"UN" + 0.025*"cholera" + 0.018*"Gaza" + 0.017*"Mexico" + 0.017*"Israeli" + 0.016*"death" + 0.016*"Yahoo" + 0.015*"tweeted" + 0.015*"election"
topic #12 (0.033): 0.091*"God" + 0.039*"Who" + 0.019*"al" + 0.016*"left" + 0.016*"Damn" + 0.014*"heart" + 0.014*"NPR" + 0.014*"guy" + 0.013*"Who's" + 0.012*"Singapore"
topic #18 (0.033): 0.104*"WikiLeaks" + 0.094*"Wikileaks" + 0.034*"cablegate" + 0.030*"wikileaks" + 0.024*"US" + 0.024*"Guardian" + 0.021*"Wiki Leaks" + 0.017*"documents" + 0.016*"NYT" + 0.014*"cables"
topic diff=0.018866, rho=0.114803
PROGRESS: pass 3, at document #96000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.039*"Black Friday" + 0.025*"HIV" + 0.024*"Canada" + 0.024*"read" + 0.023*"Sarah Palin" + 0.020*"newspaper" + 0.018*"DWTS" + 0.015*"gay" + 0.014*"Here" + 0.011*"AIDS"
topic #0 (0.033): 0.077*"Haiti" + 0.027*"UN" + 0.024*"cholera" + 0.019*"followfriday" + 0.018*"Mexico" + 0.017*"Gaza" + 0.017*"Israeli" + 0.016*"death" + 0.015*"election" + 0.014*"Yahoo"
topic #14 (0.033): 0.088*"tcot" + 0.020*"Bush" + 0.016*"teaparty" + 0.014*"jobs" + 0.013*"Obama" + 0.013*"Islam" + 0.012*"ocra" + 0.011*"Muslim" + 0.010*"arrested" + 0.009*"woman"
topic #27 (0.033): 0.091*"blog" + 0.041*"post" + 0.021*"DVD" + 0.018*"Education" + 0.018*"Complexo do Alemão" + 0.016*"release" + 0.015*"web" + 0.014*"recipes" + 0.014*"Amazon" + 0.014*"Oprah"
topic #28 (0.033): 0.116*"video" + 0.065*"YouTube" + 0.063*"Video" + 0.032*"Cyber Monday" + 0.027*"Holiday" + 0.024*"music" + 0.016*"Check" + 0.016*"Live" + 0.013*"ad" + 0.012*"Photography"
topic diff=0.018998, rho=0.114803
PROGRESS: pass 3, at document #98000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.102*"WikiLeaks" + 0.096*"Wikileaks" + 0.036*"cablegate" + 0.031*"wikileaks" + 0.025*"US" + 0.023*"Guardian" + 0.020*"Wiki Leaks" + 0.017*"documents" + 0.016*"NYT" + 0.014*"cables"
topic #10 (0.033): 0.118*"para" + 0.064*"un" + 0.044*"como" + 0.038*"dos" + 0.029*"en" + 0.025*"es" + 0.022*"la" + 0.020*"se" + 0.017*"pour" + 0.016*"si"
topic #13 (0.033): 0.056*"AP" + 0.027*"Amanda Knox" + 0.016*"DADT" + 0.014*"study" + 0.013*"risk" + 0.012*"child" + 0.011*"prison" + 0.011*"Pentagon" + 0.010*"report" + 0.010*"law"
topic #29 (0.033): 0.203*"Thanksgiving" + 0.075*"Happy Thanksgiving" + 0.045*"LOL" + 0.035*"family" + 0.033*"friends" + 0.032*"holiday" + 0.031*"Turkey" + 0.030*"thanksgiving" + 0.019*"tonight" + 0.012*"eat"
topic #12 (0.033): 0.093*"God" + 0.038*"Who" + 0.021*"al" + 0.016*"heart" + 0.016*"left" + 0.014*"Damn" + 0.013*"guy" + 0.013*"NPR" + 0.012*"Who's" + 0.012*"GOD"
topic diff=0.035843, rho=0.114803
bound: at document #0
-24.156 per-word bound, 18695255.8 perplexity estimate based on a held-out corpus of 2000 documents with 44486 words
PROGRESS: pass 3, at document #100000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.108*"para" + 0.067*"un" + 0.045*"como" + 0.034*"dos" + 0.029*"en" + 0.027*"es" + 0.022*"la" + 0.021*"pour" + 0.019*"se" + 0.016*"si"
topic #9 (0.033): 0.037*"will" + 0.023*"em" + 0.021*"work" + 0.020*"do" + 0.019*"kids" + 0.014*"Chelsea" + 0.014*"children" + 0.013*"MSNBC" + 0.013*"talk" + 0.013*"first"
topic #12 (0.033): 0.089*"God" + 0.036*"Who" + 0.023*"al" + 0.016*"left" + 0.015*"heart" + 0.014*"Gary McKinnon" + 0.014*"NPR" + 0.013*"Damn" + 0.013*"guy" + 0.012*"Bible"
topic #7 (0.033): 0.163*"Twitter" + 0.080*"Facebook" + 0.027*"Tweet" + 0.024*"NYC" + 0.015*"photos" + 0.014*"Tweets" + 0.014*"Chinese" + 0.013*"website" + 0.011*"People" + 0.010*"Internet"
topic #13 (0.033): 0.056*"AP" + 0.024*"Amanda Knox" + 0.017*"DADT" + 0.014*"child" + 0.014*"study" + 0.013*"risk" + 0.010*"Pentagon" + 0.010*"report" + 0.010*"prison" + 0.009*"law"
topic diff=0.021648, rho=0.114803
PROGRESS: pass 3, at document #102000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.056*"lol" + 0.037*"today" + 0.030*"people" + 0.027*"time" + 0.024*"you" + 0.021*"am" + 0.021*"who" + 0.020*"out" + 0.019*"can" + 0.018*"ur"
topic #27 (0.033): 0.092*"blog" + 0.037*"post" + 0.022*"DVD" + 0.020*"web" + 0.016*"release" + 0.015*"Complexo do Alemão" + 0.015*"PC" + 0.014*"Amazon" + 0.014*"Education" + 0.014*"Oprah"
topic #28 (0.033): 0.109*"video" + 0.071*"Video" + 0.058*"YouTube" + 0.029*"Cyber Monday" + 0.026*"Holiday" + 0.022*"music" + 0.015*"Check" + 0.014*"Live" + 0.012*"ad" + 0.012*"Photography"
topic #19 (0.033): 0.053*"Leslie Nielsen" + 0.043*"Love" + 0.038*"white" + 0.037*"Today" + 0.022*"Time" + 0.022*"Life" + 0.021*"nice" + 0.020*"Actor" + 0.019*"right now" + 0.016*"need"
topic #10 (0.033): 0.103*"para" + 0.074*"un" + 0.042*"como" + 0.034*"pour" + 0.031*"dos" + 0.029*"en" + 0.028*"es" + 0.023*"la" + 0.018*"se" + 0.018*"México"
topic diff=0.089062, rho=0.114803
PROGRESS: pass 3, at document #104000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #7 (0.033): 0.166*"Twitter" + 0.078*"Facebook" + 0.030*"Tweet" + 0.024*"NYC" + 0.017*"Lmao" + 0.015*"Chinese" + 0.014*"photos" + 0.014*"Tweets" + 0.013*"website" + 0.013*"Simple"
topic #26 (0.033): 0.041*"India" + 0.032*"news" + 0.021*"New Zealand" + 0.020*"business" + 0.016*"PM" + 0.015*"murder" + 0.014*"dead" + 0.014*"mine" + 0.014*"Australia" + 0.013*"killed"
topic #25 (0.033): 0.059*"lol" + 0.040*"today" + 0.030*"people" + 0.027*"time" + 0.024*"you" + 0.022*"out" + 0.021*"who" + 0.020*"ur" + 0.019*"am" + 0.018*"can"
topic #14 (0.033): 0.084*"tcot" + 0.015*"Bush" + 0.015*"teaparty" + 0.014*"Obama" + 0.012*"Muslim" + 0.012*"jobs" + 0.010*"Islam" + 0.010*"GM" + 0.010*"arrested" + 0.010*"woman"
topic #18 (0.033): 0.094*"Wikileaks" + 0.092*"WikiLeaks" + 0.047*"cablegate" + 0.046*"wikileaks" + 0.027*"US" + 0.023*"Guardian" + 0.019*"Wiki Leaks" + 0.015*"documents" + 0.013*"cables" + 0.013*"NYT"
topic diff=0.091048, rho=0.114803
PROGRESS: pass 3, at document #106000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.038*"Blog" + 0.025*"re" + 0.023*"see" + 0.023*"We" + 0.020*"book" + 0.018*"Gracias" + 0.018*"food" + 0.015*"reason" + 0.014*"basketball" + 0.013*"Real Estate"
topic #0 (0.033): 0.068*"Haiti" + 0.027*"Yahoo" + 0.027*"UN" + 0.024*"cholera" + 0.019*"earthquake" + 0.018*"Mexico" + 0.017*"gold" + 0.016*"election" + 0.016*"death" + 0.015*"Asian"
topic #29 (0.033): 0.208*"Thanksgiving" + 0.070*"Happy Thanksgiving" + 0.041*"LOL" + 0.035*"holiday" + 0.034*"family" + 0.033*"friends" + 0.030*"Turkey" + 0.028*"thanksgiving" + 0.020*"tonight" + 0.016*"Black Friday"
topic #2 (0.033): 0.033*"News" + 0.022*"NFL" + 0.019*"Texas" + 0.019*"English" + 0.017*"USD" + 0.017*"bit.ly" + 0.015*"LMAO" + 0.014*"Boston" + 0.014*"Tom DeLay" + 0.014*"EUA"
topic #1 (0.033): 0.070*"TSA" + 0.060*"Obama" + 0.052*"GOP" + 0.021*"Senate" + 0.020*"ppl" + 0.020*"Congress" + 0.019*"Palin" + 0.015*"House" + 0.014*"Republicans" + 0.014*"vote"
topic diff=0.024135, rho=0.114803
PROGRESS: pass 3, at document #108000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.053*"Leslie Nielsen" + 0.043*"Love" + 0.042*"Today" + 0.030*"white" + 0.025*"Time" + 0.020*"nice" + 0.019*"Actor" + 0.019*"Life" + 0.019*"right now" + 0.016*"need"
topic #0 (0.033): 0.128*"Amazon.com" + 0.062*"Haiti" + 0.023*"UN" + 0.022*"Yahoo" + 0.021*"cholera" + 0.016*"earthquake" + 0.015*"gold" + 0.015*"election" + 0.015*"Mexico" + 0.014*"death"
topic #23 (0.033): 0.037*"football" + 0.033*"game" + 0.032*"haha" + 0.030*"show" + 0.019*"reply" + 0.018*"team" + 0.017*"net" + 0.016*"Kanye West" + 0.014*"Dallas" + 0.014*"Justin Bieber"
topic #4 (0.033): 0.045*"Mc" + 0.027*"U.S" + 0.023*"President" + 0.018*"Yahoo! News" + 0.018*"Obama" + 0.015*"White House" + 0.015*"turkey" + 0.015*"Americans" + 0.014*"S. Korea" + 0.014*"Cancun"
topic #27 (0.033): 0.090*"blog" + 0.084*"release" + 0.034*"post" + 0.027*"book" + 0.019*"DVD" + 0.017*"web" + 0.017*"Amazon" + 0.015*"PC" + 0.013*"Oprah" + 0.012*"Education"
topic diff=0.024229, rho=0.114803
PROGRESS: pass 3, at document #110000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.094*"who" + 0.020*"me a" + 0.019*"Nice" + 0.018*"a man" + 0.017*"Oh" + 0.016*"Yes" + 0.016*"right" + 0.016*"hate" + 0.016*"Arizona" + 0.015*"Madrid"
topic #12 (0.033): 0.129*"Celebrity" + 0.072*"God" + 0.033*"Who" + 0.019*"al" + 0.014*"left" + 0.014*"NPR" + 0.013*"heart" + 0.013*"Who's" + 0.012*"America" + 0.011*"green"
topic #23 (0.033): 0.034*"football" + 0.032*"haha" + 0.031*"game" + 0.028*"show" + 0.022*"Spanish" + 0.017*"team" + 0.016*"reply" + 0.016*"Italian" + 0.015*"net" + 0.014*"Kanye West"
topic #21 (0.033): 0.036*"CNN" + 0.027*"Chicago" + 0.026*"Venezuela" + 0.025*"Pope" + 0.024*"NATO" + 0.023*"cancer" + 0.022*"Arsenal" + 0.020*"education" + 0.019*"dans" + 0.019*"Retweet"
topic #13 (0.033): 0.118*"Breaking News" + 0.043*"AP" + 0.014*"DADT" + 0.012*"child" + 0.012*"study" + 0.011*"Amanda Knox" + 0.010*"Belgium" + 0.009*"risk" + 0.008*"report" + 0.008*"prison"
topic diff=0.077611, rho=0.114803
PROGRESS: pass 3, at document #112000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.091*"Amazon.com" + 0.090*"Haiti" + 0.036*"Dutch" + 0.035*"cholera" + 0.023*"UN" + 0.017*"Yahoo" + 0.015*"Cholera" + 0.015*"Asian" + 0.015*"election" + 0.015*"death"
topic #16 (0.033): 0.042*"Black Friday" + 0.027*"read" + 0.023*"Sarah Palin" + 0.022*"HIV" + 0.022*"newspaper" + 0.020*"If" + 0.018*"DWTS" + 0.018*"Canada" + 0.016*"Here" + 0.014*"gay"
topic #4 (0.033): 0.037*"Mc" + 0.027*"U.S" + 0.025*"President" + 0.017*"Obama" + 0.015*"Yahoo! News" + 0.015*"President Obama" + 0.015*"White House" + 0.015*"Cancun" + 0.014*"turkey" + 0.014*"Americans"
topic #10 (0.033): 0.092*"para" + 0.076*"un" + 0.040*"como" + 0.031*"es" + 0.030*"en" + 0.026*"dos" + 0.025*"la" + 0.023*"pour" + 0.018*"México" + 0.018*"este"
topic #22 (0.033): 0.207*"week" + 0.037*"book" + 0.033*"Blog" + 0.018*"We" + 0.018*"re" + 0.015*"see" + 0.013*"food" + 0.013*"Gracias" + 0.012*"believe" + 0.011*"reason"
topic diff=0.030590, rho=0.114803
PROGRESS: pass 3, at document #114000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.204*"Thanksgiving" + 0.072*"Happy Thanksgiving" + 0.042*"LOL" + 0.036*"family" + 0.034*"friends" + 0.032*"holiday" + 0.028*"Turkey" + 0.023*"thanksgiving" + 0.019*"tonight" + 0.017*"Black Friday"
topic #27 (0.033): 0.084*"blog" + 0.064*"release" + 0.034*"post" + 0.020*"book" + 0.018*"Amazon" + 0.017*"DVD" + 0.017*"PC" + 0.015*"web" + 0.015*"Complexo do Alemão" + 0.012*"Oprah"
topic #6 (0.033): 0.018*"News" + 0.016*"U.S" + 0.014*"World" + 0.011*"New York" + 0.010*"Will" + 0.010*"Women" + 0.010*"Billion" + 0.008*"Business" + 0.008*"Health" + 0.007*"NYT"
topic #24 (0.033): 0.093*"who" + 0.020*"Nice" + 0.019*"a man" + 0.018*"me a" + 0.018*"Yes" + 0.017*"hate" + 0.016*"Oh" + 0.016*"right" + 0.015*"Madrid" + 0.015*"always"
topic #11 (0.033): 0.063*"via" + 0.042*"Google" + 0.041*"iPad" + 0.040*"iPhone" + 0.034*"Apple" + 0.032*"Facebook" + 0.022*"Android" + 0.018*"Christmas" + 0.017*"Social Media" + 0.016*"Beatles"
topic diff=0.015010, rho=0.114803
PROGRESS: pass 3, at document #116000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.085*"Haiti" + 0.071*"Amazon.com" + 0.031*"cholera" + 0.029*"Dutch" + 0.023*"UN" + 0.017*"Gaza" + 0.016*"Israeli" + 0.016*"Yahoo" + 0.015*"Sky News" + 0.015*"Asian"
topic #13 (0.033): 0.087*"Breaking News" + 0.038*"AP" + 0.021*"DADT" + 0.013*"study" + 0.011*"child" + 0.010*"risk" + 0.009*"report" + 0.009*"law" + 0.008*"Amanda Knox" + 0.008*"Death"
topic #15 (0.033): 0.042*"USA" + 0.030*"French" + 0.030*"women" + 0.021*"Thailand" + 0.020*"here" + 0.018*"England" + 0.017*"travel" + 0.016*"Thai" + 0.016*"sex" + 0.014*"snow"
topic #5 (0.033): 0.065*"fb" + 0.022*"season" + 0.021*"NBC" + 0.019*"student" + 0.019*"Football" + 0.017*"smile" + 0.016*"Sports" + 0.015*"high school" + 0.014*"EEUU" + 0.014*"Wisconsin"
topic #24 (0.033): 0.094*"who" + 0.020*"Nice" + 0.018*"a man" + 0.018*"me a" + 0.017*"Yes" + 0.017*"hate" + 0.016*"right" + 0.016*"Oh" + 0.016*"Arizona" + 0.015*"always"
topic diff=0.030303, rho=0.114803
PROGRESS: pass 3, at document #118000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.111*"video" + 0.087*"YouTube" + 0.071*"Video" + 0.030*"Cyber Monday" + 0.027*"Holiday" + 0.023*"music" + 0.015*"Check" + 0.013*"Live" + 0.010*"Photography" + 0.010*"Page"
topic #5 (0.033): 0.057*"fb" + 0.027*"Football" + 0.021*"season" + 0.020*"NBC" + 0.019*"student" + 0.018*"smile" + 0.016*"Los Angeles" + 0.016*"San Diego" + 0.015*"Sports" + 0.015*"high school"
topic #19 (0.033): 0.053*"Love" + 0.047*"Leslie Nielsen" + 0.044*"Today" + 0.023*"Time" + 0.021*"white" + 0.020*"Life" + 0.018*"right now" + 0.017*"nice" + 0.017*"Actor" + 0.015*"Music"
topic #16 (0.033): 0.043*"Black Friday" + 0.024*"Sarah Palin" + 0.024*"HIV" + 0.024*"read" + 0.018*"If" + 0.017*"Canada" + 0.017*"newspaper" + 0.016*"DWTS" + 0.015*"Here" + 0.014*"gay"
topic #4 (0.033): 0.028*"Mc" + 0.025*"U.S" + 0.024*"President" + 0.017*"White House" + 0.017*"Obama" + 0.017*"Hoy" + 0.015*"turkey" + 0.015*"Americans" + 0.015*"President Obama" + 0.013*"S. Korea"
topic diff=0.017062, rho=0.114803
bound: at document #0
-23.295 per-word bound, 10294023.5 perplexity estimate based on a held-out corpus of 2000 documents with 33817 words
PROGRESS: pass 3, at document #120000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.020*"jajaja" + 0.016*"News" + 0.015*"U.S" + 0.013*"World" + 0.010*"Women" + 0.010*"New York" + 0.010*"Will" + 0.008*"Billion" + 0.008*"Business" + 0.008*"Health"
topic #13 (0.033): 0.074*"Breaking News" + 0.038*"AP" + 0.019*"DADT" + 0.018*"Rome" + 0.013*"study" + 0.012*"child" + 0.011*"risk" + 0.009*"report" + 0.009*"law" + 0.008*"Amanda Knox"
topic #21 (0.033): 0.038*"CNN" + 0.031*"Chicago" + 0.029*"Venezuela" + 0.025*"Retweet" + 0.024*"dans" + 0.023*"NATO" + 0.020*"cancer" + 0.020*"Pope" + 0.018*"Friday" + 0.018*"Arsenal"
topic #27 (0.033): 0.085*"blog" + 0.050*"release" + 0.036*"post" + 0.020*"Amazon" + 0.019*"DVD" + 0.019*"PC" + 0.017*"web" + 0.016*"book" + 0.015*"Oprah" + 0.011*"Complexo do Alemão"
topic #20 (0.033): 0.048*"live" + 0.045*"TV" + 0.037*"Watch" + 0.033*"world" + 0.029*"Hope" + 0.023*"climate" + 0.022*"interview" + 0.020*"families" + 0.018*"Check it" + 0.018*"climate change"
topic diff=0.031259, rho=0.114803
PROGRESS: pass 3, at document #122000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #26 (0.033): 0.045*"India" + 0.035*"news" + 0.021*"New Zealand" + 0.021*"NZ" + 0.020*"PM" + 0.015*"killed" + 0.014*"business" + 0.014*"dead" + 0.013*"mine" + 0.013*"Australia"
topic #11 (0.033): 0.057*"via" + 0.045*"Google" + 0.045*"iPad" + 0.040*"iPhone" + 0.038*"Facebook" + 0.037*"Apple" + 0.020*"Android" + 0.017*"Christmas" + 0.017*"iTunes" + 0.016*"Social Media"
topic #29 (0.033): 0.214*"Thanksgiving" + 0.077*"Happy Thanksgiving" + 0.040*"family" + 0.040*"LOL" + 0.034*"friends" + 0.028*"holiday" + 0.027*"Turkey" + 0.024*"thanksgiving" + 0.018*"tonight" + 0.015*"Black Friday"
topic #25 (0.033): 0.049*"lol" + 0.035*"today" + 0.032*"people" + 0.025*"time" + 0.023*"you" + 0.021*"who" + 0.021*"ur" + 0.020*"can" + 0.020*"out" + 0.018*"love"
topic #24 (0.033): 0.097*"who" + 0.021*"a man" + 0.021*"Nice" + 0.019*"hate" + 0.019*"me a" + 0.017*"always" + 0.016*"Yes" + 0.016*"right" + 0.015*"actor" + 0.015*"Oh"
topic diff=0.016661, rho=0.114803
PROGRESS: pass 3, at document #124000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.053*"tcot" + 0.019*"Bush" + 0.014*"jobs" + 0.011*"Obama" + 0.011*"GM" + 0.010*"FBI" + 0.009*"Colombian" + 0.009*"arrested" + 0.009*"woman" + 0.009*"Ohio"
topic #4 (0.033): 0.024*"U.S" + 0.024*"President" + 0.020*"Mc" + 0.018*"cooking" + 0.017*"Hoy" + 0.017*"Obama" + 0.016*"White House" + 0.015*"Yahoo! News" + 0.014*"turkey" + 0.014*"President Obama"
topic #10 (0.033): 0.099*"para" + 0.070*"un" + 0.047*"como" + 0.036*"dos" + 0.029*"es" + 0.028*"en" + 0.023*"la" + 0.018*"se" + 0.017*"pour" + 0.015*"si"
topic #5 (0.033): 0.045*"fb" + 0.024*"season" + 0.024*"Football" + 0.020*"student" + 0.019*"NBC" + 0.018*"smile" + 0.016*"high school" + 0.015*"Los Angeles" + 0.015*"quote" + 0.014*"Sports"
topic #2 (0.033): 0.040*"News" + 0.028*"Boston" + 0.027*"LMAO" + 0.022*"EUA" + 0.021*"English" + 0.021*"NFL" + 0.019*"bit.ly" + 0.015*"Texas" + 0.014*"USD" + 0.013*"Report"
topic diff=0.037498, rho=0.114803
PROGRESS: pass 3, at document #126000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.045*"live" + 0.044*"TV" + 0.039*"Watch" + 0.033*"world" + 0.027*"Hope" + 0.025*"interview" + 0.023*"climate" + 0.022*"families" + 0.019*"Check it" + 0.017*"Check it out"
topic #1 (0.033): 0.053*"TSA" + 0.052*"Obama" + 0.049*"GOP" + 0.023*"Congress" + 0.023*"ppl" + 0.018*"Republicans" + 0.018*"Senate" + 0.016*"Palin" + 0.015*"House" + 0.014*"vote"
topic #16 (0.033): 0.046*"Black Friday" + 0.024*"Sarah Palin" + 0.023*"read" + 0.022*"HIV" + 0.017*"If" + 0.017*"DWTS" + 0.015*"Here" + 0.014*"Canada" + 0.014*"gay" + 0.013*"newspaper"
topic #5 (0.033): 0.041*"fb" + 0.024*"Football" + 0.023*"season" + 0.021*"student" + 0.018*"smile" + 0.018*"NBC" + 0.016*"high school" + 0.015*"Los Angeles" + 0.014*"Sports" + 0.014*"Wisconsin"
topic #27 (0.033): 0.084*"blog" + 0.040*"release" + 0.034*"post" + 0.020*"Amazon" + 0.019*"Oprah" + 0.018*"DVD" + 0.018*"PC" + 0.018*"recipes" + 0.017*"Complexo do Alemão" + 0.015*"web"
topic diff=0.050459, rho=0.114803
PROGRESS: pass 3, at document #128000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.056*"TSA" + 0.054*"Obama" + 0.048*"GOP" + 0.023*"Congress" + 0.022*"ppl" + 0.018*"Senate" + 0.017*"Republicans" + 0.015*"vote" + 0.015*"House" + 0.015*"Palin"
topic #21 (0.033): 0.069*"CNN" + 0.029*"Retweet" + 0.027*"Chicago" + 0.025*"Venezuela" + 0.023*"Pope" + 0.020*"NATO" + 0.020*"cancer" + 0.019*"Friday" + 0.017*"dans" + 0.016*"education"
topic #22 (0.033): 0.089*"week" + 0.044*"Blog" + 0.027*"book" + 0.021*"We" + 0.020*"re" + 0.017*"Vintage" + 0.015*"see" + 0.014*"basketball" + 0.013*"believe" + 0.013*"Post"
topic #25 (0.033): 0.098*"tweet" + 0.047*"now" + 0.039*"lol" + 0.029*"today" + 0.028*"people" + 0.023*"time" + 0.021*"ur" + 0.021*"you" + 0.019*"who" + 0.019*"can"
topic #7 (0.033): 0.399*"Twitter" + 0.058*"Facebook" + 0.024*"NYC" + 0.017*"Tweet" + 0.012*"photos" + 0.009*"Chinese" + 0.008*"website" + 0.008*"Lmao" + 0.007*"Tweets" + 0.007*"uploaded"
topic diff=0.032044, rho=0.114803
PROGRESS: pass 3, at document #130000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.092*"who" + 0.023*"a man" + 0.022*"hate" + 0.021*"Nice" + 0.020*"always" + 0.019*"me a" + 0.018*"Oh" + 0.017*"media" + 0.016*"right" + 0.016*"Arizona"
topic #15 (0.033): 0.078*"Home" + 0.028*"USA" + 0.023*"women" + 0.021*"here" + 0.017*"snow" + 0.017*"England" + 0.017*"French" + 0.015*"sex" + 0.014*"travel" + 0.014*"Thailand"
topic #11 (0.033): 0.055*"via" + 0.044*"Google" + 0.040*"iPad" + 0.039*"Facebook" + 0.038*"iPhone" + 0.034*"Apple" + 0.020*"Android" + 0.018*"iTunes" + 0.017*"Christmas" + 0.017*"app"
topic #18 (0.033): 0.094*"WikiLeaks" + 0.080*"Wikileaks" + 0.031*"wikileaks" + 0.028*"cablegate" + 0.027*"US" + 0.026*"Asia" + 0.020*"Guardian" + 0.016*"Wiki Leaks" + 0.014*"documents" + 0.013*"ha"
topic #3 (0.033): 0.081*"UK" + 0.036*"London" + 0.021*"students" + 0.021*"police" + 0.019*"BBC" + 0.018*"British" + 0.017*"protest" + 0.013*"Cuba" + 0.013*"Police" + 0.013*"Colombia"
topic diff=0.020760, rho=0.114803
PROGRESS: pass 3, at document #132000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.072*"Home" + 0.031*"USA" + 0.024*"women" + 0.022*"here" + 0.018*"England" + 0.017*"snow" + 0.017*"French" + 0.015*"travel" + 0.015*"sex" + 0.013*"Thailand"
topic #26 (0.033): 0.048*"India" + 0.032*"news" + 0.019*"New Zealand" + 0.018*"business" + 0.016*"NZ" + 0.016*"killed" + 0.015*"PM" + 0.015*"Australia" + 0.014*"dead" + 0.014*"energy"
topic #3 (0.033): 0.080*"UK" + 0.036*"London" + 0.021*"students" + 0.020*"police" + 0.018*"BBC" + 0.018*"British" + 0.017*"photography" + 0.016*"protest" + 0.015*"Cuba" + 0.013*"Colombia"
topic #5 (0.033): 0.034*"fb" + 0.027*"season" + 0.023*"student" + 0.022*"Football" + 0.019*"smile" + 0.019*"NBC" + 0.018*"high school" + 0.016*"Los Angeles" + 0.015*"Wisconsin" + 0.014*"start"
topic #21 (0.033): 0.063*"CNN" + 0.041*"Retweet" + 0.028*"Chicago" + 0.022*"Venezuela" + 0.021*"Pope" + 0.020*"cancer" + 0.020*"Friday" + 0.019*"NATO" + 0.016*"education" + 0.015*"dans"
topic diff=0.025200, rho=0.114803
PROGRESS: pass 3, at document #134000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.059*"China" + 0.042*"North Korea" + 0.029*"South Korea" + 0.025*"Korea" + 0.025*"Ireland" + 0.020*"Japan" + 0.018*"Irish" + 0.018*"US" + 0.017*"U.S" + 0.014*"attack"
topic #18 (0.033): 0.093*"WikiLeaks" + 0.080*"Wikileaks" + 0.033*"wikileaks" + 0.027*"US" + 0.026*"cablegate" + 0.023*"Asia" + 0.020*"Wiki Leaks" + 0.019*"Guardian" + 0.014*"documents" + 0.013*"ha"
topic #7 (0.033): 0.343*"Twitter" + 0.059*"Facebook" + 0.025*"NYC" + 0.020*"Tweet" + 0.017*"Lady Gaga" + 0.012*"photos" + 0.010*"Chinese" + 0.010*"website" + 0.009*"Lmao" + 0.008*"Simple"
topic #1 (0.033): 0.057*"TSA" + 0.052*"Obama" + 0.041*"GOP" + 0.022*"ppl" + 0.022*"download" + 0.020*"Congress" + 0.019*"Senate" + 0.017*"vote" + 0.016*"Palin" + 0.015*"Republicans"
topic #3 (0.033): 0.076*"UK" + 0.037*"London" + 0.020*"students" + 0.020*"police" + 0.018*"BBC" + 0.018*"British" + 0.016*"protest" + 0.016*"photography" + 0.014*"Cuba" + 0.012*"Colombia"
topic diff=0.021673, rho=0.114803
PROGRESS: pass 3, at document #136000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.037*"will" + 0.021*"kids" + 0.019*"work" + 0.017*"do" + 0.017*"em" + 0.016*"mind" + 0.015*"VIDEO" + 0.014*"soul" + 0.014*"children" + 0.014*"talk"
topic #11 (0.033): 0.054*"via" + 0.042*"Google" + 0.039*"Facebook" + 0.038*"iPhone" + 0.035*"iPad" + 0.031*"Apple" + 0.021*"Christmas" + 0.020*"iTunes" + 0.018*"Android" + 0.017*"online"
topic #16 (0.033): 0.042*"Black Friday" + 0.034*"chat" + 0.029*"cash" + 0.022*"read" + 0.021*"Sarah Palin" + 0.018*"HIV" + 0.018*"DWTS" + 0.015*"If" + 0.014*"Here" + 0.013*"Dec"
topic #12 (0.033): 0.128*"God" + 0.032*"Who" + 0.031*"Celebrity" + 0.027*"TED" + 0.017*"al" + 0.016*"heart" + 0.013*"Who's" + 0.012*"guy" + 0.012*"help" + 0.011*"left"
topic #17 (0.033): 0.059*"China" + 0.042*"North Korea" + 0.030*"South Korea" + 0.025*"Ireland" + 0.025*"Korea" + 0.019*"Japan" + 0.018*"US" + 0.018*"Irish" + 0.017*"BBC News" + 0.017*"U.S"
topic diff=0.015551, rho=0.114803
PROGRESS: pass 3, at document #138000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.037*"game" + 0.029*"show" + 0.029*"RETWEET" + 0.026*"haha" + 0.026*"football" + 0.023*"net" + 0.018*"beautiful" + 0.018*"team" + 0.016*"Dallas" + 0.014*"NBA"
topic #6 (0.033): 0.015*"Business" + 0.014*"News" + 0.014*"U.S" + 0.013*"World" + 0.012*"New York" + 0.010*"BP" + 0.009*"Will" + 0.009*"Women" + 0.009*"jajaja" + 0.008*"Study"
topic #13 (0.033): 0.040*"AP" + 0.033*"add" + 0.032*"Breaking News" + 0.014*"study" + 0.013*"DADT" + 0.013*"Rome" + 0.011*"child" + 0.010*"Chronicle" + 0.010*"risk" + 0.010*"report"
topic #0 (0.033): 0.071*"Haiti" + 0.026*"cholera" + 0.025*"Amazon.com" + 0.023*"UN" + 0.018*"Israeli" + 0.017*"election" + 0.017*"Gaza" + 0.016*"Asian" + 0.016*"death" + 0.015*"Dutch"
topic #16 (0.033): 0.042*"Black Friday" + 0.031*"chat" + 0.028*"cash" + 0.022*"read" + 0.021*"Sarah Palin" + 0.018*"HIV" + 0.016*"DWTS" + 0.015*"If" + 0.014*"Here" + 0.013*"Dec"
topic diff=0.021363, rho=0.114803
bound: at document #0
-22.271 per-word bound, 5059371.6 perplexity estimate based on a held-out corpus of 2000 documents with 78501 words
PROGRESS: pass 3, at document #140000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.057*"Obama" + 0.055*"TSA" + 0.049*"GOP" + 0.022*"Palin" + 0.020*"Congress" + 0.019*"Senate" + 0.018*"ppl" + 0.017*"vote" + 0.017*"Republicans" + 0.015*"Sarah Palin"
topic #3 (0.033): 0.070*"UK" + 0.036*"London" + 0.021*"police" + 0.019*"students" + 0.019*"BBC" + 0.019*"British" + 0.015*"protest" + 0.015*"photography" + 0.013*"Cuba" + 0.012*"Colombia"
topic #13 (0.033): 0.038*"AP" + 0.029*"Breaking News" + 0.028*"DADT" + 0.028*"add" + 0.014*"study" + 0.011*"Rome" + 0.010*"child" + 0.010*"risk" + 0.010*"report" + 0.010*"prison"
topic #25 (0.033): 0.050*"tweet" + 0.044*"lol" + 0.033*"people" + 0.028*"today" + 0.026*"now" + 0.025*"time" + 0.024*"you" + 0.020*"can" + 0.019*"give me" + 0.019*"who"
topic #20 (0.033): 0.050*"live" + 0.040*"TV" + 0.039*"world" + 0.034*"Watch" + 0.034*"Check it" + 0.030*"COP16" + 0.030*"Hope" + 0.029*"Check it out" + 0.026*"interview" + 0.019*"families"
topic diff=0.070497, rho=0.114803
PROGRESS: pass 3, at document #142000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.135*"video" + 0.076*"Video" + 0.058*"YouTube" + 0.026*"Holiday" + 0.026*"music" + 0.022*"Cyber Monday" + 0.020*"Check" + 0.018*"WOW" + 0.018*"Live" + 0.011*"Photography"
topic #26 (0.033): 0.038*"news" + 0.036*"India" + 0.021*"New Zealand" + 0.018*"business" + 0.016*"dead" + 0.015*"killed" + 0.015*"mine" + 0.014*"feeling" + 0.014*"Australia" + 0.013*"PM"
topic #22 (0.033): 0.064*"Blog" + 0.045*"week" + 0.024*"re" + 0.021*"book" + 0.021*"We" + 0.019*"believe" + 0.016*"change" + 0.015*"learn" + 0.015*"basketball" + 0.014*"see"
topic #15 (0.033): 0.042*"Home" + 0.028*"USA" + 0.028*"sex" + 0.023*"women" + 0.021*"here" + 0.018*"girl" + 0.017*"snow" + 0.016*"England" + 0.015*"travel" + 0.015*"School"
topic #12 (0.033): 0.125*"God" + 0.032*"Who" + 0.025*"al" + 0.023*"Celebrity" + 0.018*"TED" + 0.014*"heart" + 0.012*"help" + 0.012*"Who's" + 0.012*"left" + 0.011*"guy"
topic diff=0.024844, rho=0.114803
bound: at document #0
-23.345 per-word bound, 10651855.1 perplexity estimate based on a held-out corpus of 1749 documents with 49577 words
PROGRESS: pass 3, at document #143749/143749
performing inference on a chunk of 1749 documents
1749/1749 documents converged within 50 iterations
updating topics
merging changes from 1749 documents into a model of 143749 documents
topic #6 (0.033): 0.014*"News" + 0.013*"U.S" + 0.012*"Business" + 0.012*"World" + 0.011*"Bloomberg" + 0.011*"Health" + 0.011*"Will" + 0.011*"New York" + 0.009*"Women" + 0.009*"Pres"
topic #1 (0.033): 0.057*"Obama" + 0.054*"TSA" + 0.043*"GOP" + 0.031*"ppl" + 0.021*"Palin" + 0.019*"Senate" + 0.018*"Congress" + 0.016*"vote" + 0.016*"Republicans" + 0.015*"Sarah Palin"
topic #3 (0.033): 0.066*"UK" + 0.037*"London" + 0.020*"police" + 0.019*"students" + 0.019*"school" + 0.018*"protest" + 0.018*"BBC" + 0.017*"British" + 0.015*"Colombia" + 0.013*"failure"
topic #17 (0.033): 0.053*"China" + 0.045*"North Korea" + 0.031*"South Korea" + 0.025*"Ireland" + 0.023*"Korea" + 0.017*"Irish" + 0.017*"Japan" + 0.016*"US" + 0.016*"U.S" + 0.014*"attack"
topic #26 (0.033): 0.037*"news" + 0.034*"India" + 0.020*"New Zealand" + 0.018*"business" + 0.015*"dead" + 0.015*"feeling" + 0.014*"killed" + 0.014*"mine" + 0.014*"Australia" + 0.013*"banned"
topic diff=0.015554, rho=0.114803
PROGRESS: pass 4, at document #2000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.036*"AP" + 0.023*"DADT" + 0.022*"Breaking News" + 0.021*"add" + 0.013*"study" + 0.012*"Art" + 0.011*"risk" + 0.011*"Notre Dame" + 0.010*"child" + 0.010*"prison"
topic #10 (0.033): 0.105*"Como" + 0.070*"un" + 0.068*"para" + 0.039*"como" + 0.034*"es" + 0.028*"en" + 0.025*"la" + 0.018*"dos" + 0.017*"este" + 0.016*"más"
topic #26 (0.033): 0.033*"news" + 0.032*"India" + 0.019*"New Zealand" + 0.016*"business" + 0.015*"dead" + 0.014*"mine" + 0.014*"killed" + 0.013*"feeling" + 0.013*"Australia" + 0.012*"police"
topic #12 (0.033): 0.109*"God" + 0.030*"Who" + 0.024*"al" + 0.020*"Celebrity" + 0.016*"help" + 0.015*"TED" + 0.014*"dogs" + 0.013*"heart" + 0.013*"guy" + 0.012*"fashion"
topic #15 (0.033): 0.034*"Home" + 0.029*"sex" + 0.027*"USA" + 0.022*"women" + 0.019*"here" + 0.017*"England" + 0.015*"snow" + 0.015*"girl" + 0.014*"travel" + 0.013*"School"
topic diff=0.025746, rho=0.114054
PROGRESS: pass 4, at document #4000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.046*"Leslie Nielsen" + 0.044*"Love" + 0.036*"Today" + 0.028*"need" + 0.027*"Music" + 0.022*"Time" + 0.019*"right now" + 0.018*"nice" + 0.018*"Jets" + 0.017*"Life"
topic #11 (0.033): 0.053*"via" + 0.046*"iPad" + 0.046*"iPhone" + 0.045*"Google" + 0.039*"Facebook" + 0.030*"Apple" + 0.021*"Android" + 0.017*"Social Media" + 0.016*"iTunes" + 0.016*"online"
topic #14 (0.033): 0.046*"tcot" + 0.043*"hanging" + 0.016*"Bush" + 0.013*"jobs" + 0.012*"Obama" + 0.009*"GM" + 0.009*"arrested" + 0.008*"Ohio" + 0.008*"shot" + 0.008*"woman"
topic #6 (0.033): 0.015*"U.S" + 0.012*"News" + 0.011*"World" + 0.011*"Business" + 0.010*"Health" + 0.010*"New York" + 0.010*"Will" + 0.009*"Bloomberg" + 0.008*"Women" + 0.007*"NYT"
topic #7 (0.033): 0.271*"Twitter" + 0.070*"Facebook" + 0.053*"NYC" + 0.019*"Tweet" + 0.012*"photos" + 0.012*"Lady Gaga" + 0.010*"Chinese" + 0.010*"website" + 0.009*"Founder" + 0.008*"Tweets"
topic diff=0.041593, rho=0.114054
PROGRESS: pass 4, at document #6000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.083*"who" + 0.027*"college" + 0.022*"fat" + 0.020*"hate" + 0.019*"Oh" + 0.018*"Nice" + 0.018*"a man" + 0.018*"me a" + 0.018*"Yes" + 0.016*"always"
topic #7 (0.033): 0.265*"Twitter" + 0.072*"Facebook" + 0.053*"NYC" + 0.019*"Tweet" + 0.012*"photos" + 0.012*"Lady Gaga" + 0.010*"Chinese" + 0.010*"website" + 0.009*"Founder" + 0.008*"social media"
topic #3 (0.033): 0.065*"UK" + 0.036*"London" + 0.021*"police" + 0.018*"BBC" + 0.018*"British" + 0.018*"students" + 0.017*"protest" + 0.015*"school" + 0.014*"Colombia" + 0.011*"Police"
topic #20 (0.033): 0.050*"live" + 0.038*"world" + 0.036*"TV" + 0.035*"Watch" + 0.034*"Check it" + 0.031*"Check it out" + 0.026*"Hope" + 0.025*"COP16" + 0.024*"interview" + 0.018*"families"
topic #26 (0.033): 0.033*"India" + 0.031*"news" + 0.020*"New Zealand" + 0.015*"business" + 0.015*"mine" + 0.014*"killed" + 0.014*"dead" + 0.013*"PM" + 0.012*"NZ" + 0.012*"Australia"
topic diff=0.054673, rho=0.114054
PROGRESS: pass 4, at document #8000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.040*"AP" + 0.020*"DADT" + 0.016*"Breaking News" + 0.015*"add" + 0.015*"study" + 0.011*"risk" + 0.011*"prison" + 0.011*"report" + 0.011*"Art" + 0.010*"child"
topic #8 (0.033): 0.046*"Iran" + 0.040*"US" + 0.029*"Israel" + 0.028*"Afghanistan" + 0.022*"Pakistan" + 0.018*"Russia" + 0.017*"Egypt" + 0.015*"NATO" + 0.015*"Afghan" + 0.013*"Iraq"
topic #4 (0.033): 0.026*"U.S" + 0.025*"President" + 0.020*"Obama" + 0.018*"White House" + 0.018*"turkey" + 0.017*"President Obama" + 0.015*"S. Korea" + 0.013*"Americans" + 0.013*"Seoul" + 0.012*"Hoy"
topic #11 (0.033): 0.048*"Google" + 0.047*"via" + 0.044*"iPad" + 0.044*"iPhone" + 0.040*"Facebook" + 0.032*"Apple" + 0.021*"Android" + 0.017*"iTunes" + 0.015*"Social Media" + 0.015*"iOS"
topic #21 (0.033): 0.039*"CNN" + 0.029*"Retweet" + 0.025*"Pope" + 0.023*"Chicago" + 0.022*"cancer" + 0.021*"NATO" + 0.020*"Venezuela" + 0.020*"speech" + 0.019*"condoms" + 0.018*"Friday"
topic diff=0.038004, rho=0.114054
PROGRESS: pass 4, at document #10000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.036*"game" + 0.027*"haha" + 0.026*"show" + 0.025*"football" + 0.017*"RETWEET" + 0.015*"net" + 0.015*"win" + 0.014*"Kanye West" + 0.014*"team" + 0.012*"beautiful"
topic #9 (0.033): 0.038*"kids" + 0.035*"will" + 0.019*"work" + 0.014*"do" + 0.012*"children" + 0.012*"VIDEO" + 0.012*"em" + 0.012*"down" + 0.012*"talk" + 0.012*"don"
topic #27 (0.033): 0.084*"blog" + 0.046*"Back" + 0.034*"post" + 0.022*"Amazon" + 0.019*"release" + 0.017*"Education" + 0.016*"web" + 0.016*"DVD" + 0.013*"Oprah" + 0.013*"PC"
topic #19 (0.033): 0.049*"Leslie Nielsen" + 0.040*"Love" + 0.035*"Today" + 0.025*"Music" + 0.025*"need" + 0.023*"Time" + 0.018*"right now" + 0.017*"nice" + 0.016*"Actor" + 0.016*"Life"
topic #21 (0.033): 0.040*"CNN" + 0.027*"Retweet" + 0.025*"Pope" + 0.024*"Chicago" + 0.022*"cancer" + 0.021*"NATO" + 0.020*"speech" + 0.019*"Venezuela" + 0.018*"condoms" + 0.018*"Friday"
topic diff=0.040055, rho=0.114054
PROGRESS: pass 4, at document #12000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.037*"kids" + 0.035*"will" + 0.019*"work" + 0.013*"do" + 0.013*"children" + 0.012*"VIDEO" + 0.012*"talk" + 0.011*"play" + 0.011*"down" + 0.011*"don"
topic #14 (0.033): 0.053*"tcot" + 0.030*"hanging" + 0.013*"Bush" + 0.011*"Obama" + 0.011*"jobs" + 0.009*"woman" + 0.009*"Police" + 0.009*"teaparty" + 0.009*"arrested" + 0.009*"Ohio"
topic #16 (0.033): 0.046*"Black Friday" + 0.040*"party" + 0.023*"read" + 0.021*"Sarah Palin" + 0.019*"HIV" + 0.018*"newspaper" + 0.015*"DWTS" + 0.015*"cash" + 0.014*"gay" + 0.014*"Here"
topic #11 (0.033): 0.050*"via" + 0.048*"iPad" + 0.046*"Google" + 0.046*"iPhone" + 0.038*"Facebook" + 0.035*"Apple" + 0.024*"Android" + 0.017*"iOS" + 0.016*"Social Media" + 0.016*"iTunes"
topic #19 (0.033): 0.048*"Leslie Nielsen" + 0.039*"Love" + 0.035*"Today" + 0.025*"Music" + 0.024*"need" + 0.022*"Time" + 0.018*"nice" + 0.017*"right now" + 0.017*"Actor" + 0.016*"Life"
topic diff=0.042487, rho=0.114054
PROGRESS: pass 4, at document #14000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.039*"AP" + 0.020*"DADT" + 0.015*"study" + 0.014*"Breaking News" + 0.013*"add" + 0.012*"risk" + 0.010*"report" + 0.010*"Art" + 0.010*"Pentagon" + 0.010*"prison"
topic #10 (0.033): 0.072*"para" + 0.070*"un" + 0.062*"Como" + 0.048*"como" + 0.029*"es" + 0.027*"en" + 0.024*"la" + 0.019*"eso" + 0.018*"si" + 0.017*"hoy"
topic #12 (0.033): 0.084*"God" + 0.031*"Who" + 0.027*"al" + 0.015*"NPR" + 0.015*"help" + 0.014*"Celebrity" + 0.012*"fashion" + 0.011*"local" + 0.011*"Who's" + 0.010*"heart"
topic #21 (0.033): 0.038*"CNN" + 0.025*"Pope" + 0.024*"Retweet" + 0.023*"Chicago" + 0.021*"cancer" + 0.021*"NATO" + 0.019*"Venezuela" + 0.019*"Friday" + 0.018*"condoms" + 0.018*"speech"
topic #15 (0.033): 0.024*"Home" + 0.024*"USA" + 0.023*"sex" + 0.021*"women" + 0.018*"travel" + 0.018*"here" + 0.016*"snow" + 0.014*"England" + 0.013*"film" + 0.012*"Thailand"
topic diff=0.047656, rho=0.114054
PROGRESS: pass 4, at document #16000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #18 (0.033): 0.095*"WikiLeaks" + 0.088*"Wikileaks" + 0.038*"wikileaks" + 0.026*"cablegate" + 0.021*"US" + 0.021*"Wiki Leaks" + 0.016*"Guardian" + 0.015*"NYT" + 0.015*"cables" + 0.014*"Wikipedia"
topic #17 (0.033): 0.053*"China" + 0.046*"North Korea" + 0.030*"South Korea" + 0.026*"Ireland" + 0.023*"Korea" + 0.018*"U.S" + 0.018*"Japan" + 0.018*"Irish" + 0.016*"attack" + 0.014*"US"
topic #0 (0.033): 0.081*"Haiti" + 0.027*"UN" + 0.026*"cholera" + 0.019*"election" + 0.015*"Cholera" + 0.014*"Gaza" + 0.013*"Israeli" + 0.012*"Mexico" + 0.012*"death" + 0.011*"Asian"
topic #13 (0.033): 0.038*"AP" + 0.021*"DADT" + 0.015*"study" + 0.014*"Breaking News" + 0.012*"add" + 0.012*"risk" + 0.011*"report" + 0.010*"child" + 0.010*"prison" + 0.010*"Pentagon"
topic #3 (0.033): 0.060*"UK" + 0.034*"London" + 0.024*"police" + 0.019*"students" + 0.018*"protest" + 0.017*"British" + 0.016*"BBC" + 0.014*"Cuba" + 0.013*"demo 2010" + 0.013*"school"
topic diff=0.024901, rho=0.114054
PROGRESS: pass 4, at document #18000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.059*"UK" + 0.034*"London" + 0.025*"police" + 0.019*"students" + 0.018*"protest" + 0.017*"British" + 0.016*"BBC" + 0.013*"Cuba" + 0.013*"school" + 0.012*"Police"
topic #2 (0.033): 0.027*"News" + 0.020*"bit.ly" + 0.018*"Texas" + 0.016*"Tom DeLay" + 0.016*"NFL" + 0.014*"English" + 0.014*"Report" + 0.014*"You" + 0.014*"Tory" + 0.013*"Boston"
topic #20 (0.033): 0.044*"live" + 0.038*"TV" + 0.035*"Watch" + 0.032*"world" + 0.026*"Check it" + 0.024*"Check it out" + 0.023*"Hope" + 0.021*"interview" + 0.020*"climate" + 0.018*"COP16"
topic #6 (0.033): 0.016*"U.S" + 0.012*"World" + 0.011*"New York" + 0.010*"Will" + 0.010*"News" + 0.010*"jajaja" + 0.009*"Business" + 0.009*"Health" + 0.008*"Women" + 0.008*"Study"
topic #10 (0.033): 0.074*"para" + 0.070*"un" + 0.058*"Como" + 0.046*"como" + 0.028*"es" + 0.026*"en" + 0.025*"la" + 0.018*"eso" + 0.017*"hoy" + 0.017*"si"
topic diff=0.041285, rho=0.114054
bound: at document #0
-23.062 per-word bound, 8755981.1 perplexity estimate based on a held-out corpus of 2000 documents with 20435 words
PROGRESS: pass 4, at document #20000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #20 (0.033): 0.045*"live" + 0.038*"TV" + 0.034*"Watch" + 0.033*"world" + 0.026*"Check it" + 0.023*"Check it out" + 0.021*"climate" + 0.021*"Hope" + 0.020*"interview" + 0.018*"families"
topic #16 (0.033): 0.050*"Black Friday" + 0.031*"party" + 0.024*"read" + 0.023*"Sarah Palin" + 0.020*"HIV" + 0.019*"newspaper" + 0.015*"DWTS" + 0.015*"gay" + 0.014*"Canada" + 0.013*"Here"
topic #6 (0.033): 0.015*"U.S" + 0.012*"World" + 0.011*"News" + 0.011*"New York" + 0.010*"Will" + 0.009*"jajaja" + 0.009*"Health" + 0.009*"Business" + 0.009*"Women" + 0.008*"Study"
topic #12 (0.033): 0.077*"God" + 0.032*"Who" + 0.023*"al" + 0.017*"NPR" + 0.013*"help" + 0.012*"local" + 0.012*"Who's" + 0.012*"Celebrity" + 0.011*"left" + 0.011*"fashion"
topic #2 (0.033): 0.038*"News" + 0.018*"bit.ly" + 0.018*"Texas" + 0.018*"Tom DeLay" + 0.018*"NFL" + 0.014*"Report" + 0.014*"English" + 0.013*"You" + 0.013*"Tory" + 0.012*"Boston"
topic diff=0.048505, rho=0.114054
PROGRESS: pass 4, at document #22000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #7 (0.033): 0.227*"Twitter" + 0.080*"Facebook" + 0.038*"NYC" + 0.017*"Tweet" + 0.012*"website" + 0.012*"Chinese" + 0.011*"photos" + 0.010*"social media" + 0.009*"Thx" + 0.009*"Lady Gaga"
topic #26 (0.033): 0.045*"India" + 0.042*"news" + 0.019*"New Zealand" + 0.014*"dead" + 0.014*"mine" + 0.013*"business" + 0.013*"PM" + 0.013*"killed" + 0.011*"police" + 0.011*"Australia"
topic #11 (0.033): 0.056*"via" + 0.051*"iPad" + 0.044*"iPhone" + 0.043*"Google" + 0.036*"Apple" + 0.035*"Facebook" + 0.023*"Android" + 0.018*"Social Media" + 0.018*"iOS" + 0.015*"app"
topic #25 (0.033): 0.048*"lol" + 0.039*"today" + 0.032*"people" + 0.029*"tweet" + 0.026*"time" + 0.024*"out" + 0.021*"you" + 0.018*"who" + 0.018*"can" + 0.017*"love"
topic #9 (0.033): 0.036*"will" + 0.034*"kids" + 0.020*"work" + 0.015*"children" + 0.014*"talk" + 0.014*"MSNBC" + 0.013*"VIDEO" + 0.013*"do" + 0.010*"first" + 0.010*"play"
topic diff=0.018303, rho=0.114054
PROGRESS: pass 4, at document #24000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #23 (0.033): 0.031*"game" + 0.027*"show" + 0.025*"haha" + 0.024*"football" + 0.016*"Kanye West" + 0.014*"Nicki Minaj" + 0.014*"team" + 0.013*"net" + 0.013*"win" + 0.012*"Dallas"
topic #10 (0.033): 0.072*"para" + 0.067*"un" + 0.053*"Como" + 0.043*"como" + 0.026*"es" + 0.025*"en" + 0.024*"la" + 0.019*"min" + 0.016*"dos" + 0.016*"eso"
topic #9 (0.033): 0.036*"will" + 0.033*"kids" + 0.020*"work" + 0.015*"children" + 0.014*"MSNBC" + 0.014*"talk" + 0.013*"VIDEO" + 0.012*"do" + 0.011*"play" + 0.010*"first"
topic #18 (0.033): 0.102*"WikiLeaks" + 0.086*"Wikileaks" + 0.037*"wikileaks" + 0.027*"cablegate" + 0.023*"US" + 0.022*"Wiki Leaks" + 0.018*"NYT" + 0.015*"Guardian" + 0.014*"cables" + 0.013*"documents"
topic #13 (0.033): 0.038*"AP" + 0.020*"DADT" + 0.015*"study" + 0.013*"report" + 0.012*"Breaking News" + 0.012*"risk" + 0.010*"friend" + 0.010*"child" + 0.010*"Pentagon" + 0.010*"prison"
topic diff=0.014849, rho=0.114054
PROGRESS: pass 4, at document #26000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.029*"U.S" + 0.025*"President" + 0.022*"turkey" + 0.021*"Obama" + 0.019*"White House" + 0.016*"President Obama" + 0.016*"Americans" + 0.014*"S. Korea" + 0.013*"Sen" + 0.012*"Seoul"
topic #3 (0.033): 0.054*"UK" + 0.036*"London" + 0.030*"police" + 0.019*"students" + 0.017*"demo 2010" + 0.016*"British" + 0.016*"protest" + 0.015*"BBC" + 0.014*"Police" + 0.013*"school"
topic #17 (0.033): 0.058*"China" + 0.047*"North Korea" + 0.030*"South Korea" + 0.026*"Ireland" + 0.022*"Korea" + 0.018*"U.S" + 0.017*"Irish" + 0.016*"Japan" + 0.016*"attack" + 0.016*"US"
topic #0 (0.033): 0.083*"Haiti" + 0.027*"UN" + 0.025*"cholera" + 0.024*"election" + 0.015*"Israeli" + 0.014*"Cholera" + 0.012*"Mexico" + 0.012*"Gaza" + 0.012*"death" + 0.012*"Asian"
topic #19 (0.033): 0.053*"Leslie Nielsen" + 0.038*"Today" + 0.036*"Love" + 0.020*"Time" + 0.020*"Music" + 0.020*"right now" + 0.019*"nice" + 0.018*"need" + 0.018*"Life" + 0.015*"Actor"
topic diff=0.012282, rho=0.114054
PROGRESS: pass 4, at document #28000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.027*"USA" + 0.023*"travel" + 0.022*"women" + 0.019*"here" + 0.019*"snow" + 0.018*"sex" + 0.017*"Home" + 0.014*"England" + 0.012*"film" + 0.011*"Thailand"
topic #4 (0.033): 0.028*"U.S" + 0.024*"President" + 0.022*"turkey" + 0.021*"Obama" + 0.018*"White House" + 0.016*"Americans" + 0.015*"President Obama" + 0.013*"Sen" + 0.013*"S. Korea" + 0.011*"military"
topic #24 (0.033): 0.094*"who" + 0.020*"Arizona" + 0.019*"Oh" + 0.019*"Nice" + 0.018*"media" + 0.018*"Yes" + 0.017*"I'm going" + 0.015*"me a" + 0.015*"hate" + 0.015*"right"
topic #17 (0.033): 0.056*"China" + 0.046*"North Korea" + 0.029*"Ireland" + 0.028*"South Korea" + 0.022*"Korea" + 0.018*"Irish" + 0.017*"U.S" + 0.016*"Japan" + 0.015*"US" + 0.015*"attack"
topic #3 (0.033): 0.056*"UK" + 0.037*"London" + 0.031*"police" + 0.019*"students" + 0.017*"British" + 0.016*"protest" + 0.016*"demo 2010" + 0.014*"Police" + 0.014*"BBC" + 0.013*"photography"
topic diff=0.019635, rho=0.114054
PROGRESS: pass 4, at document #30000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.068*"para" + 0.067*"un" + 0.044*"como" + 0.043*"Como" + 0.027*"en" + 0.022*"dos" + 0.022*"la" + 0.022*"es" + 0.021*"pour" + 0.016*"se"
topic #0 (0.033): 0.076*"Haiti" + 0.025*"UN" + 0.022*"cholera" + 0.022*"election" + 0.014*"Israeli" + 0.013*"earthquake" + 0.013*"Mexico" + 0.012*"Asian" + 0.012*"Cholera" + 0.012*"Gaza"
topic #7 (0.033): 0.210*"Twitter" + 0.084*"Facebook" + 0.035*"NYC" + 0.022*"photos" + 0.018*"Tweet" + 0.013*"Chinese" + 0.011*"social media" + 0.011*"website" + 0.010*"NYTimes" + 0.009*"Internet"
topic #14 (0.033): 0.059*"tcot" + 0.015*"Bush" + 0.014*"AM" + 0.013*"jobs" + 0.012*"hanging" + 0.012*"Obama" + 0.011*"teaparty" + 0.010*"FBI" + 0.010*"sgp" + 0.010*"woman"
topic #28 (0.033): 0.116*"video" + 0.070*"Video" + 0.063*"YouTube" + 0.031*"Holiday" + 0.023*"Cyber Monday" + 0.021*"music" + 0.019*"Check" + 0.013*"Photography" + 0.012*"Live" + 0.011*"Photos"
topic diff=0.048462, rho=0.114054
PROGRESS: pass 4, at document #32000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.056*"via" + 0.049*"iPhone" + 0.048*"iPad" + 0.039*"Google" + 0.035*"Apple" + 0.035*"Facebook" + 0.026*"Social Media" + 0.019*"Android" + 0.017*"Christmas" + 0.017*"iTunes"
topic #28 (0.033): 0.113*"video" + 0.073*"Video" + 0.061*"YouTube" + 0.030*"Holiday" + 0.023*"Cyber Monday" + 0.022*"music" + 0.018*"Check" + 0.013*"Photography" + 0.012*"Live" + 0.011*"Photos"
topic #23 (0.033): 0.032*"football" + 0.029*"game" + 0.026*"show" + 0.022*"haha" + 0.016*"Kanye West" + 0.015*"Nicki Minaj" + 0.014*"Film" + 0.013*"team" + 0.012*"win" + 0.012*"turkeys"
topic #14 (0.033): 0.057*"tcot" + 0.015*"Bush" + 0.013*"AM" + 0.013*"jobs" + 0.012*"Obama" + 0.011*"hanging" + 0.011*"arrested" + 0.010*"teaparty" + 0.010*"shot" + 0.010*"woman"
topic #10 (0.033): 0.070*"para" + 0.065*"un" + 0.043*"como" + 0.041*"Como" + 0.027*"en" + 0.024*"dos" + 0.021*"la" + 0.021*"es" + 0.020*"pour" + 0.016*"se"
topic diff=0.028301, rho=0.114054
PROGRESS: pass 4, at document #34000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.041*"News" + 0.022*"NFL" + 0.020*"Texas" + 0.016*"Tom DeLay" + 0.016*"You" + 0.015*"English" + 0.014*"bit.ly" + 0.012*"Report" + 0.011*"Boston" + 0.011*"songs"
topic #0 (0.033): 0.072*"Haiti" + 0.023*"UN" + 0.022*"cholera" + 0.022*"election" + 0.014*"Mexico" + 0.013*"Asian" + 0.013*"Israeli" + 0.012*"gold" + 0.012*"Gaza" + 0.012*"earthquake"
topic #17 (0.033): 0.052*"China" + 0.048*"North Korea" + 0.030*"South Korea" + 0.025*"Ireland" + 0.022*"Korea" + 0.018*"attack" + 0.016*"U.S" + 0.016*"US" + 0.015*"Irish" + 0.015*"Japan"
topic #19 (0.033): 0.052*"Leslie Nielsen" + 0.040*"Today" + 0.032*"Love" + 0.021*"right now" + 0.021*"nice" + 0.019*"Time" + 0.019*"Life" + 0.019*"need" + 0.018*"Music" + 0.015*"Actor"
topic #23 (0.033): 0.034*"football" + 0.031*"game" + 0.025*"show" + 0.020*"haha" + 0.016*"team" + 0.015*"Kanye West" + 0.014*"Nicki Minaj" + 0.013*"beautiful" + 0.013*"Film" + 0.012*"win"
topic diff=0.031713, rho=0.114054
PROGRESS: pass 4, at document #36000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.055*"UK" + 0.035*"London" + 0.028*"police" + 0.018*"students" + 0.017*"photography" + 0.017*"British" + 0.015*"protest" + 0.015*"BBC" + 0.015*"Police" + 0.014*"Read"
topic #14 (0.033): 0.055*"tcot" + 0.015*"Bush" + 0.012*"jobs" + 0.012*"Obama" + 0.012*"AM" + 0.010*"woman" + 0.010*"arrested" + 0.010*"FBI" + 0.010*"hanging" + 0.009*"GM"
topic #24 (0.033): 0.100*"who" + 0.020*"Oh" + 0.020*"Nice" + 0.020*"Arizona" + 0.018*"hate" + 0.018*"I'm going" + 0.017*"me a" + 0.017*"Yes" + 0.016*"media" + 0.015*"right"
topic #28 (0.033): 0.108*"video" + 0.070*"Video" + 0.055*"YouTube" + 0.030*"Holiday" + 0.026*"Cyber Monday" + 0.021*"music" + 0.018*"Check" + 0.015*"Photography" + 0.013*"Live" + 0.011*"Photos"
topic #9 (0.033): 0.038*"will" + 0.029*"kids" + 0.020*"work" + 0.016*"children" + 0.014*"VIDEO" + 0.013*"do" + 0.012*"talk" + 0.012*"MSNBC" + 0.011*"em" + 0.011*"play"
topic diff=0.013379, rho=0.114054
PROGRESS: pass 4, at document #38000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #15 (0.033): 0.026*"travel" + 0.025*"women" + 0.023*"USA" + 0.022*"Thailand" + 0.022*"snow" + 0.020*"here" + 0.015*"Thai" + 0.015*"sex" + 0.014*"film" + 0.013*"England"
topic #3 (0.033): 0.056*"UK" + 0.035*"London" + 0.028*"police" + 0.018*"students" + 0.016*"British" + 0.016*"photography" + 0.015*"BBC" + 0.015*"Police" + 0.014*"protest" + 0.014*"Read"
topic #24 (0.033): 0.099*"who" + 0.022*"Nice" + 0.019*"Oh" + 0.019*"Arizona" + 0.019*"hate" + 0.018*"Yes" + 0.017*"I'm going" + 0.016*"me a" + 0.015*"right" + 0.015*"media"
topic #17 (0.033): 0.054*"China" + 0.047*"North Korea" + 0.029*"South Korea" + 0.029*"Ireland" + 0.022*"Korea" + 0.019*"Irish" + 0.017*"US" + 0.017*"U.S" + 0.017*"attack" + 0.015*"Japan"
topic #29 (0.033): 0.212*"Thanksgiving" + 0.059*"Happy Thanksgiving" + 0.038*"LOL" + 0.035*"holiday" + 0.035*"family" + 0.029*"thanksgiving" + 0.028*"Turkey" + 0.027*"friends" + 0.022*"tonight" + 0.013*"Black Friday"
topic diff=0.039199, rho=0.114054
bound: at document #0
-23.171 per-word bound, 9446491.7 perplexity estimate based on a held-out corpus of 2000 documents with 26137 words
PROGRESS: pass 4, at document #40000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.070*"God" + 0.035*"Who" + 0.018*"NPR" + 0.017*"Celebrity" + 0.014*"America" + 0.014*"left" + 0.014*"local" + 0.014*"Who's" + 0.012*"help" + 0.012*"guy"
topic #8 (0.033): 0.047*"US" + 0.044*"Iran" + 0.031*"Israel" + 0.028*"Afghanistan" + 0.019*"Pakistan" + 0.016*"Russia" + 0.015*"American" + 0.015*"to win" + 0.014*"NATO" + 0.012*"Egypt"
topic #14 (0.033): 0.062*"tcot" + 0.014*"sgp" + 0.013*"Bush" + 0.013*"Obama" + 0.012*"jobs" + 0.011*"teaparty" + 0.011*"woman" + 0.010*"AM" + 0.010*"Police" + 0.010*"arrested"
topic #27 (0.033): 0.086*"blog" + 0.051*"Blogger" + 0.035*"post" + 0.021*"Amazon" + 0.018*"DVD" + 0.017*"release" + 0.016*"web" + 0.015*"cc" + 0.014*"Oprah" + 0.014*"Back"
topic #16 (0.033): 0.054*"Black Friday" + 0.025*"Sarah Palin" + 0.021*"read" + 0.021*"HIV" + 0.018*"Here" + 0.016*"newspaper" + 0.016*"gay" + 0.015*"party" + 0.014*"DWTS" + 0.014*"Canada"
topic diff=0.027520, rho=0.114054
PROGRESS: pass 4, at document #42000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.084*"TSA" + 0.054*"Obama" + 0.039*"GOP" + 0.020*"Senate" + 0.020*"ppl" + 0.016*"Congress" + 0.016*"Palin" + 0.015*"Republicans" + 0.014*"vote" + 0.014*"House"
topic #14 (0.033): 0.060*"tcot" + 0.013*"sgp" + 0.013*"Bush" + 0.013*"Obama" + 0.012*"jobs" + 0.011*"teaparty" + 0.011*"woman" + 0.010*"GM" + 0.010*"AM" + 0.010*"Ohio"
topic #17 (0.033): 0.052*"China" + 0.045*"North Korea" + 0.030*"Ireland" + 0.028*"South Korea" + 0.020*"Korea" + 0.019*"Irish" + 0.017*"US" + 0.017*"attack" + 0.016*"U.S" + 0.016*"Japan"
topic #22 (0.033): 0.065*"Blog" + 0.024*"We" + 0.024*"book" + 0.023*"re" + 0.018*"food" + 0.018*"week" + 0.014*"basketball" + 0.013*"Post" + 0.013*"WTF" + 0.013*"health"
topic #2 (0.033): 0.040*"News" + 0.027*"LMAO" + 0.021*"NFL" + 0.018*"Texas" + 0.016*"English" + 0.014*"Tom DeLay" + 0.014*"You" + 0.012*"Report" + 0.012*"songs" + 0.012*"Boston"
topic diff=0.031300, rho=0.114054
PROGRESS: pass 4, at document #44000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.205*"Thanksgiving" + 0.063*"LOL" + 0.058*"Happy Thanksgiving" + 0.034*"family" + 0.033*"holiday" + 0.030*"friends" + 0.026*"Turkey" + 0.025*"thanksgiving" + 0.023*"tonight" + 0.012*"Black Friday"
topic #23 (0.033): 0.040*"haha" + 0.031*"game" + 0.031*"football" + 0.029*"show" + 0.016*"team" + 0.014*"Nicki Minaj" + 0.014*"Kanye West" + 0.013*"win" + 0.013*"beautiful" + 0.012*"Dallas"
topic #2 (0.033): 0.039*"News" + 0.026*"LMAO" + 0.020*"NFL" + 0.018*"Texas" + 0.015*"English" + 0.014*"You" + 0.014*"Tom DeLay" + 0.011*"Report" + 0.011*"bit.ly" + 0.011*"songs"
topic #20 (0.033): 0.042*"TV" + 0.039*"live" + 0.036*"world" + 0.029*"Watch" + 0.023*"families" + 0.023*"interview" + 0.021*"climate" + 0.020*"Hope" + 0.019*"Check it" + 0.016*"Check it out"
topic #7 (0.033): 0.192*"Twitter" + 0.083*"Facebook" + 0.035*"NYC" + 0.021*"Tweet" + 0.018*"photos" + 0.015*"Chinese" + 0.012*"website" + 0.012*"social media" + 0.010*"NYTimes" + 0.010*"Thx"
topic diff=0.035281, rho=0.114054
PROGRESS: pass 4, at document #46000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.149*"video" + 0.107*"YouTube" + 0.055*"Video" + 0.027*"Holiday" + 0.024*"Cyber Monday" + 0.019*"music" + 0.016*"Check" + 0.016*"Live" + 0.011*"Photography" + 0.011*"Tonight"
topic #10 (0.033): 0.081*"para" + 0.056*"un" + 0.038*"como" + 0.033*"dos" + 0.029*"Como" + 0.023*"en" + 0.019*"pour" + 0.019*"la" + 0.017*"es" + 0.015*"ll"
topic #16 (0.033): 0.049*"Black Friday" + 0.026*"Sarah Palin" + 0.022*"read" + 0.022*"HIV" + 0.017*"DWTS" + 0.017*"Here" + 0.016*"Bristol" + 0.015*"Canada" + 0.013*"gay" + 0.013*"party"
topic #8 (0.033): 0.054*"Iran" + 0.045*"US" + 0.028*"Israel" + 0.027*"Afghanistan" + 0.018*"Pakistan" + 0.017*"to win" + 0.017*"Russia" + 0.015*"NATO" + 0.014*"American" + 0.013*"Forest"
topic #13 (0.033): 0.038*"AP" + 0.019*"DADT" + 0.016*"study" + 0.011*"risk" + 0.011*"report" + 0.011*"child" + 0.010*"Pentagon" + 0.009*"Debt" + 0.009*"prison" + 0.008*"Breaking News"
topic diff=0.110816, rho=0.114054
PROGRESS: pass 4, at document #48000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.093*"who" + 0.042*"twit" + 0.021*"Arizona" + 0.019*"Nice" + 0.018*"Oh" + 0.018*"Yes" + 0.018*"me a" + 0.017*"media" + 0.017*"I'm going" + 0.015*"right"
topic #20 (0.033): 0.042*"live" + 0.039*"TV" + 0.037*"world" + 0.029*"Watch" + 0.024*"interview" + 0.022*"Hope" + 0.022*"families" + 0.021*"climate" + 0.019*"Check it" + 0.017*"Check it out"
topic #1 (0.033): 0.073*"TSA" + 0.065*"GOP" + 0.059*"Obama" + 0.021*"Senate" + 0.018*"Palin" + 0.018*"Republicans" + 0.017*"ppl" + 0.016*"vote" + 0.016*"Congress" + 0.014*"House"
topic #11 (0.033): 0.056*"via" + 0.048*"iPad" + 0.042*"iPhone" + 0.037*"Google" + 0.037*"Apple" + 0.029*"Facebook" + 0.024*"Social Media" + 0.020*"iTunes" + 0.018*"Christmas" + 0.018*"Android"
topic #13 (0.033): 0.039*"AP" + 0.023*"DADT" + 0.016*"study" + 0.011*"risk" + 0.011*"Pentagon" + 0.011*"report" + 0.011*"child" + 0.009*"prison" + 0.009*"Debt" + 0.009*"Portland"
topic diff=0.021035, rho=0.114054
PROGRESS: pass 4, at document #50000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.038*"AP" + 0.024*"DADT" + 0.016*"study" + 0.012*"child" + 0.011*"Pentagon" + 0.011*"report" + 0.011*"risk" + 0.009*"prison" + 0.009*"NW" + 0.009*"Debt"
topic #15 (0.033): 0.029*"snow" + 0.029*"USA" + 0.026*"women" + 0.020*"here" + 0.020*"travel" + 0.014*"England" + 0.013*"Thailand" + 0.013*"film" + 0.013*"sex" + 0.012*"Paris"
topic #26 (0.033): 0.036*"India" + 0.028*"news" + 0.021*"New Zealand" + 0.017*"PM" + 0.014*"dead" + 0.014*"mine" + 0.013*"killed" + 0.013*"business" + 0.011*"energy" + 0.010*"police"
topic #16 (0.033): 0.049*"Black Friday" + 0.027*"Sarah Palin" + 0.022*"HIV" + 0.020*"read" + 0.019*"DWTS" + 0.016*"Here" + 0.016*"Canada" + 0.015*"gay" + 0.015*"Bristol" + 0.013*"party"
topic #14 (0.033): 0.071*"tcot" + 0.022*"Bush" + 0.019*"sgp" + 0.014*"jobs" + 0.014*"Obama" + 0.013*"teaparty" + 0.010*"GM" + 0.010*"arrested" + 0.009*"woman" + 0.009*"FBI"
topic diff=0.027253, rho=0.114054
PROGRESS: pass 4, at document #52000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #5 (0.033): 0.026*"season" + 0.025*"student" + 0.019*"NBC" + 0.018*"high school" + 0.015*"Wisconsin" + 0.015*"Los Angeles" + 0.015*"Good" + 0.015*"fb" + 0.014*"record" + 0.013*"start"
topic #15 (0.033): 0.030*"snow" + 0.028*"USA" + 0.025*"women" + 0.020*"here" + 0.019*"travel" + 0.015*"England" + 0.013*"sex" + 0.013*"film" + 0.013*"Paris" + 0.013*"Thailand"
topic #6 (0.033): 0.014*"U.S" + 0.013*"NYT" + 0.013*"World" + 0.012*"Economist" + 0.012*"Will" + 0.011*"News" + 0.011*"Study" + 0.010*"New York" + 0.009*"Tea Party" + 0.008*"Green"
topic #16 (0.033): 0.049*"Black Friday" + 0.028*"Sarah Palin" + 0.021*"HIV" + 0.020*"read" + 0.019*"DWTS" + 0.017*"Here" + 0.016*"Canada" + 0.016*"Bristol" + 0.015*"gay" + 0.013*"party"
topic #18 (0.033): 0.103*"WikiLeaks" + 0.092*"Wikileaks" + 0.035*"NYT" + 0.034*"wikileaks" + 0.026*"cablegate" + 0.025*"US" + 0.021*"Wiki Leaks" + 0.017*"Guardian" + 0.016*"documents" + 0.015*"Opinion"
topic diff=0.041777, rho=0.114054
PROGRESS: pass 4, at document #54000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.027*"U.S" + 0.027*"turkey" + 0.024*"President" + 0.021*"Obama" + 0.019*"White House" + 0.017*"Americans" + 0.017*"President Obama" + 0.014*"TIME" + 0.012*"American" + 0.012*"military"
topic #22 (0.033): 0.057*"Blog" + 0.025*"We" + 0.021*"book" + 0.021*"re" + 0.018*"food" + 0.017*"week" + 0.016*"WTF" + 0.016*"health" + 0.014*"basketball" + 0.014*"reason"
topic #21 (0.033): 0.056*"CNN" + 0.036*"Chicago" + 0.032*"Retweet" + 0.026*"Pope" + 0.024*"Friday" + 0.023*"cancer" + 0.016*"Illinois" + 0.016*"education" + 0.015*"Nation" + 0.014*"up"
topic #16 (0.033): 0.047*"Black Friday" + 0.027*"Sarah Palin" + 0.022*"HIV" + 0.020*"read" + 0.019*"DWTS" + 0.016*"gay" + 0.016*"Canada" + 0.015*"Bristol" + 0.015*"Here" + 0.013*"cold"
topic #11 (0.033): 0.058*"via" + 0.047*"iPad" + 0.046*"iPhone" + 0.035*"Apple" + 0.035*"Google" + 0.027*"Facebook" + 0.023*"Social Media" + 0.019*"iTunes" + 0.019*"Christmas" + 0.018*"app"
topic diff=0.017416, rho=0.114054
PROGRESS: pass 4, at document #56000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.093*"para" + 0.052*"un" + 0.038*"como" + 0.038*"dos" + 0.025*"en" + 0.021*"la" + 0.021*"pour" + 0.019*"Como" + 0.016*"se" + 0.016*"ll"
topic #15 (0.033): 0.031*"snow" + 0.026*"USA" + 0.025*"women" + 0.024*"here" + 0.018*"travel" + 0.015*"Paris" + 0.014*"film" + 0.014*"England" + 0.013*"sex" + 0.011*"Thailand"
topic #14 (0.033): 0.066*"tcot" + 0.020*"Bush" + 0.016*"sgp" + 0.014*"jobs" + 0.013*"Obama" + 0.012*"teaparty" + 0.010*"GM" + 0.010*"woman" + 0.010*"arrested" + 0.009*"Ohio"
topic #0 (0.033): 0.092*"Haiti" + 0.026*"cholera" + 0.020*"Atlantic" + 0.020*"Cholera" + 0.019*"election" + 0.019*"UN" + 0.016*"tweeted" + 0.014*"Mexico" + 0.014*"G20" + 0.012*"Israeli"
topic #2 (0.033): 0.032*"News" + 0.026*"LMAO" + 0.021*"NFL" + 0.020*"Texas" + 0.016*"Tom DeLay" + 0.015*"EUA" + 0.014*"You" + 0.014*"English" + 0.013*"Boston" + 0.013*"songs"
topic diff=0.030640, rho=0.114054
PROGRESS: pass 4, at document #58000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #21 (0.033): 0.053*"CNN" + 0.040*"Chicago" + 0.030*"Retweet" + 0.025*"Pope" + 0.025*"cancer" + 0.022*"Friday" + 0.021*"Toronto" + 0.015*"education" + 0.014*"Illinois" + 0.014*"up"
topic #5 (0.033): 0.026*"season" + 0.024*"student" + 0.022*"NBC" + 0.019*"high school" + 0.017*"Wisconsin" + 0.016*"Iowa" + 0.015*"Los Angeles" + 0.014*"Good" + 0.014*"record" + 0.014*"Willie Nelson"
topic #18 (0.033): 0.105*"WikiLeaks" + 0.087*"Wikileaks" + 0.032*"wikileaks" + 0.030*"NYT" + 0.026*"cablegate" + 0.023*"US" + 0.022*"Wiki Leaks" + 0.017*"Guardian" + 0.015*"documents" + 0.013*"cables"
topic #24 (0.033): 0.093*"who" + 0.024*"twit" + 0.022*"me a" + 0.020*"Nice" + 0.020*"Oh" + 0.018*"a man" + 0.018*"I'm going" + 0.017*"Arizona" + 0.016*"hate" + 0.016*"Yes"
topic #6 (0.033): 0.014*"U.S" + 0.013*"World" + 0.012*"NYT" + 0.011*"Will" + 0.011*"New York" + 0.011*"News" + 0.010*"Economist" + 0.010*"Study" + 0.009*"Women" + 0.008*"Tea Party"
topic diff=0.020370, rho=0.114054
bound: at document #0
-24.502 per-word bound, 23752993.3 perplexity estimate based on a held-out corpus of 2000 documents with 40495 words
PROGRESS: pass 4, at document #60000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.061*"tcot" + 0.017*"Bush" + 0.015*"jobs" + 0.015*"sgp" + 0.013*"Obama" + 0.011*"teaparty" + 0.010*"woman" + 0.010*"GM" + 0.009*"arrested" + 0.009*"Ohio"
topic #17 (0.033): 0.048*"China" + 0.045*"North Korea" + 0.031*"Ireland" + 0.029*"South Korea" + 0.020*"Korea" + 0.017*"Irish" + 0.016*"attack" + 0.016*"US" + 0.016*"Japan" + 0.014*"U.S"
topic #4 (0.033): 0.028*"turkey" + 0.027*"U.S" + 0.021*"President" + 0.020*"Obama" + 0.018*"White House" + 0.016*"Americans" + 0.016*"President Obama" + 0.013*"military" + 0.012*"American" + 0.012*"TIME"
topic #27 (0.033): 0.081*"blog" + 0.035*"post" + 0.024*"Amazon" + 0.022*"DVD" + 0.020*"Blogger" + 0.016*"release" + 0.015*"Win" + 0.015*"Oprah" + 0.015*"web" + 0.010*"PC"
topic #5 (0.033): 0.027*"season" + 0.022*"student" + 0.019*"NBC" + 0.018*"high school" + 0.016*"Good" + 0.016*"Wisconsin" + 0.015*"quote" + 0.014*"Iowa" + 0.014*"record" + 0.014*"smile"
topic diff=0.022186, rho=0.114054
PROGRESS: pass 4, at document #62000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.080*"BBC" + 0.056*"UK" + 0.035*"London" + 0.025*"police" + 0.017*"students" + 0.016*"British" + 0.014*"demo 2010" + 0.014*"Police" + 0.012*"protest" + 0.012*"Britain"
topic #14 (0.033): 0.062*"tcot" + 0.017*"Bush" + 0.014*"sgp" + 0.014*"jobs" + 0.013*"Obama" + 0.011*"teaparty" + 0.010*"woman" + 0.010*"GM" + 0.009*"arrested" + 0.009*"Ohio"
topic #17 (0.033): 0.056*"China" + 0.045*"North Korea" + 0.029*"Ireland" + 0.028*"South Korea" + 0.021*"Korea" + 0.017*"Japan" + 0.017*"Irish" + 0.016*"US" + 0.016*"U.S" + 0.015*"attack"
topic #27 (0.033): 0.079*"blog" + 0.036*"post" + 0.024*"Amazon" + 0.024*"DVD" + 0.018*"Blogger" + 0.016*"release" + 0.016*"Oprah" + 0.015*"web" + 0.015*"Win" + 0.011*"check it"
topic #26 (0.033): 0.035*"India" + 0.028*"news" + 0.019*"New Zealand" + 0.016*"dead" + 0.014*"mine" + 0.013*"PM" + 0.013*"business" + 0.013*"energy" + 0.012*"killed" + 0.012*"police"
topic diff=0.034159, rho=0.114054
PROGRESS: pass 4, at document #64000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.092*"para" + 0.050*"un" + 0.043*"como" + 0.033*"dos" + 0.021*"en" + 0.020*"pour" + 0.018*"la" + 0.016*"ll" + 0.015*"Como" + 0.015*"se"
topic #17 (0.033): 0.055*"China" + 0.044*"North Korea" + 0.028*"South Korea" + 0.028*"Ireland" + 0.021*"Korea" + 0.017*"Japan" + 0.016*"Irish" + 0.015*"US" + 0.015*"attack" + 0.015*"U.S"
topic #16 (0.033): 0.048*"Black Friday" + 0.030*"HIV" + 0.025*"Sarah Palin" + 0.020*"gay" + 0.018*"DWTS" + 0.017*"Canada" + 0.016*"read" + 0.014*"Here" + 0.012*"cold" + 0.012*"If"
topic #13 (0.033): 0.035*"AP" + 0.018*"DADT" + 0.015*"study" + 0.013*"risk" + 0.013*"child" + 0.012*"report" + 0.009*"weather" + 0.009*"friend" + 0.009*"Pentagon" + 0.009*"prison"
topic #18 (0.033): 0.103*"WikiLeaks" + 0.085*"Wikileaks" + 0.034*"wikileaks" + 0.029*"cablegate" + 0.026*"NYT" + 0.023*"US" + 0.021*"Wiki Leaks" + 0.017*"Guardian" + 0.015*"documents" + 0.011*"cables"
topic diff=0.027437, rho=0.114054
PROGRESS: pass 4, at document #66000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.043*"Blog" + 0.030*"see" + 0.026*"artist" + 0.024*"birth" + 0.023*"We" + 0.023*"heroes" + 0.021*"re" + 0.018*"food" + 0.018*"book" + 0.015*"RTs"
topic #4 (0.033): 0.028*"U.S" + 0.028*"turkey" + 0.024*"President" + 0.021*"Obama" + 0.019*"S. Korea" + 0.017*"White House" + 0.016*"President Obama" + 0.015*"Americans" + 0.013*"American" + 0.013*"military"
topic #16 (0.033): 0.045*"Black Friday" + 0.031*"HIV" + 0.025*"Sarah Palin" + 0.020*"gay" + 0.018*"DWTS" + 0.016*"Canada" + 0.015*"read" + 0.014*"Here" + 0.012*"cold" + 0.012*"Dec"
topic #2 (0.033): 0.040*"News" + 0.037*"son" + 0.021*"NFL" + 0.018*"Texas" + 0.017*"LMAO" + 0.015*"Houston" + 0.015*"Tom DeLay" + 0.013*"EUA" + 0.013*"Boston" + 0.013*"Report"
topic #20 (0.033): 0.042*"live" + 0.038*"TV" + 0.034*"world" + 0.032*"Watch" + 0.025*"Radio" + 0.023*"interview" + 0.022*"climate" + 0.021*"Hope" + 0.020*"climate change" + 0.018*"families"
topic diff=0.031841, rho=0.114054
PROGRESS: pass 4, at document #68000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #16 (0.033): 0.044*"Black Friday" + 0.030*"HIV" + 0.025*"Sarah Palin" + 0.022*"DJ" + 0.019*"gay" + 0.018*"DWTS" + 0.015*"Canada" + 0.015*"Here" + 0.014*"read" + 0.012*"Dec"
topic #8 (0.033): 0.051*"Iran" + 0.044*"US" + 0.026*"Afghanistan" + 0.025*"Israel" + 0.018*"to win" + 0.016*"Russia" + 0.016*"Pakistan" + 0.015*"Brasil" + 0.014*"American" + 0.014*"NATO"
topic #26 (0.033): 0.041*"India" + 0.030*"business" + 0.028*"news" + 0.018*"New Zealand" + 0.016*"dead" + 0.013*"mine" + 0.013*"PM" + 0.013*"energy" + 0.012*"killed" + 0.012*"police"
topic #14 (0.033): 0.057*"tcot" + 0.017*"Bush" + 0.013*"Obama" + 0.013*"jobs" + 0.012*"GM" + 0.011*"arrested" + 0.011*"sgp" + 0.010*"teaparty" + 0.010*"woman" + 0.009*"Ohio"
topic #3 (0.033): 0.061*"BBC" + 0.051*"UK" + 0.051*"photography" + 0.036*"London" + 0.025*"police" + 0.016*"students" + 0.015*"British" + 0.013*"Police" + 0.012*"demo 2010" + 0.012*"protest"
topic diff=0.017299, rho=0.114054
PROGRESS: pass 4, at document #70000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.016*"U.S" + 0.012*"World" + 0.012*"News" + 0.011*"NYT" + 0.011*"New York" + 0.010*"Will" + 0.009*"Study" + 0.009*"BP" + 0.009*"Pres" + 0.009*"Green"
topic #21 (0.033): 0.048*"CNN" + 0.038*"Chicago" + 0.032*"cancer" + 0.028*"FARC" + 0.023*"Retweet" + 0.021*"Friday" + 0.021*"Pope" + 0.016*"up" + 0.015*"education" + 0.015*"Toronto"
topic #26 (0.033): 0.040*"India" + 0.028*"news" + 0.027*"business" + 0.018*"New Zealand" + 0.016*"dead" + 0.013*"mine" + 0.013*"PM" + 0.013*"energy" + 0.012*"killed" + 0.012*"police"
topic #29 (0.033): 0.201*"Thanksgiving" + 0.067*"Happy Thanksgiving" + 0.053*"LOL" + 0.041*"thanksgiving" + 0.034*"family" + 0.028*"holiday" + 0.028*"friends" + 0.027*"Turkey" + 0.023*"tonight" + 0.014*"Happy thanksgiving"
topic #12 (0.033): 0.123*"God" + 0.039*"Who" + 0.018*"GOD" + 0.016*"help" + 0.015*"heart" + 0.015*"left" + 0.014*"guy" + 0.014*"Who's" + 0.013*"NPR" + 0.013*"America"
topic diff=0.019929, rho=0.114054
PROGRESS: pass 4, at document #72000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.038*"AP" + 0.020*"DADT" + 0.017*"study" + 0.014*"child" + 0.014*"risk" + 0.010*"weather" + 0.010*"report" + 0.010*"friend" + 0.010*"Pentagon" + 0.008*"network"
topic #22 (0.033): 0.044*"Blog" + 0.027*"see" + 0.023*"We" + 0.022*"re" + 0.021*"artist" + 0.019*"book" + 0.018*"food" + 0.017*"birth" + 0.016*"heroes" + 0.014*"WTF"
topic #26 (0.033): 0.039*"India" + 0.030*"news" + 0.025*"business" + 0.018*"New Zealand" + 0.016*"dead" + 0.014*"mine" + 0.013*"PM" + 0.013*"energy" + 0.012*"killed" + 0.011*"Australia"
topic #27 (0.033): 0.082*"blog" + 0.034*"post" + 0.020*"Amazon" + 0.019*"DVD" + 0.017*"Oprah" + 0.015*"web" + 0.015*"release" + 0.014*"check it" + 0.012*"check it out" + 0.011*"Blogger"
topic #8 (0.033): 0.049*"Iran" + 0.044*"US" + 0.026*"Afghanistan" + 0.024*"Israel" + 0.018*"Pakistan" + 0.018*"Russia" + 0.017*"to win" + 0.015*"NATO" + 0.014*"American" + 0.013*"Brasil"
topic diff=0.014388, rho=0.114054
PROGRESS: pass 4, at document #74000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.098*"who" + 0.023*"me a" + 0.021*"hate" + 0.020*"I'm going" + 0.019*"a man" + 0.018*"Nice" + 0.018*"always" + 0.018*"right" + 0.018*"Oh" + 0.017*"Yes"
topic #10 (0.033): 0.090*"para" + 0.052*"un" + 0.037*"como" + 0.031*"dos" + 0.024*"en" + 0.023*"la" + 0.019*"es" + 0.017*"pour" + 0.016*"ll" + 0.016*"se"
topic #11 (0.033): 0.058*"via" + 0.039*"iPad" + 0.036*"iPhone" + 0.032*"Google" + 0.031*"Apple" + 0.026*"Facebook" + 0.023*"iTunes" + 0.021*"Christmas" + 0.017*"Social Media" + 0.016*"Beatles"
topic #16 (0.033): 0.043*"Black Friday" + 0.027*"Sarah Palin" + 0.027*"HIV" + 0.019*"DWTS" + 0.018*"gay" + 0.017*"DJ" + 0.016*"read" + 0.015*"Canada" + 0.014*"Here" + 0.012*"newspaper"
topic #6 (0.033): 0.015*"U.S" + 0.013*"News" + 0.013*"World" + 0.011*"New York" + 0.011*"Will" + 0.010*"NYT" + 0.009*"Cancer" + 0.009*"Pres" + 0.009*"Study" + 0.008*"BP"
topic diff=0.032624, rho=0.114054
PROGRESS: pass 4, at document #76000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.104*"Haiti" + 0.032*"cholera" + 0.023*"UN" + 0.019*"Cholera" + 0.016*"election" + 0.016*"Mexico" + 0.014*"death" + 0.013*"tweeted" + 0.012*"Asian" + 0.011*"gold"
topic #13 (0.033): 0.035*"AP" + 0.019*"DADT" + 0.017*"child" + 0.017*"study" + 0.013*"risk" + 0.011*"friend" + 0.010*"report" + 0.010*"weather" + 0.009*"Pentagon" + 0.009*"prison"
topic #11 (0.033): 0.056*"via" + 0.043*"iPad" + 0.035*"iPhone" + 0.033*"Google" + 0.030*"Apple" + 0.027*"Facebook" + 0.022*"iTunes" + 0.022*"Christmas" + 0.016*"online" + 0.016*"Social Media"
topic #29 (0.033): 0.205*"Thanksgiving" + 0.073*"Happy Thanksgiving" + 0.048*"LOL" + 0.039*"thanksgiving" + 0.036*"family" + 0.029*"friends" + 0.028*"holiday" + 0.026*"Turkey" + 0.024*"tonight" + 0.012*"Black Friday"
topic #16 (0.033): 0.043*"Black Friday" + 0.028*"HIV" + 0.026*"Sarah Palin" + 0.020*"gay" + 0.018*"DWTS" + 0.017*"read" + 0.016*"DJ" + 0.014*"Canada" + 0.014*"Here" + 0.012*"newspaper"
topic diff=0.027917, rho=0.114054
PROGRESS: pass 4, at document #78000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.026*"turkey" + 0.026*"U.S" + 0.025*"President" + 0.019*"Obama" + 0.017*"S. Korea" + 0.016*"President Obama" + 0.016*"American" + 0.015*"White House" + 0.014*"Seoul" + 0.014*"Americans"
topic #22 (0.033): 0.047*"Blog" + 0.026*"see" + 0.024*"We" + 0.022*"re" + 0.018*"food" + 0.018*"book" + 0.018*"artist" + 0.015*"health" + 0.014*"Post" + 0.014*"WTF"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.072*"Happy Thanksgiving" + 0.046*"LOL" + 0.038*"thanksgiving" + 0.036*"family" + 0.031*"friends" + 0.029*"holiday" + 0.026*"Turkey" + 0.024*"tonight" + 0.012*"Black Friday"
topic #13 (0.033): 0.034*"AP" + 0.018*"DADT" + 0.017*"study" + 0.016*"child" + 0.013*"risk" + 0.010*"friend" + 0.010*"report" + 0.010*"weather" + 0.009*"Pentagon" + 0.008*"prison"
topic #18 (0.033): 0.092*"WikiLeaks" + 0.089*"Wikileaks" + 0.037*"wikileaks" + 0.037*"cablegate" + 0.023*"US" + 0.020*"Wiki Leaks" + 0.020*"NYT" + 0.015*"Guardian" + 0.014*"documents" + 0.013*"cables"
topic diff=0.029286, rho=0.114054
bound: at document #0
-22.643 per-word bound, 6550025.3 perplexity estimate based on a held-out corpus of 2000 documents with 49203 words
PROGRESS: pass 4, at document #80000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.035*"AP" + 0.018*"DADT" + 0.016*"study" + 0.014*"child" + 0.014*"risk" + 0.011*"friend" + 0.010*"weather" + 0.010*"report" + 0.010*"prison" + 0.009*"Pentagon"
topic #1 (0.033): 0.085*"TSA" + 0.059*"Obama" + 0.044*"GOP" + 0.025*"ppl" + 0.019*"Senate" + 0.018*"Palin" + 0.018*"politics" + 0.016*"Congress" + 0.014*"House" + 0.013*"vote"
topic #3 (0.033): 0.051*"UK" + 0.040*"BBC" + 0.035*"London" + 0.032*"photography" + 0.023*"police" + 0.018*"British" + 0.018*"Colombia" + 0.016*"students" + 0.015*"protest" + 0.013*"Event"
topic #19 (0.033): 0.056*"Leslie Nielsen" + 0.042*"Today" + 0.038*"Love" + 0.025*"nice" + 0.024*"right now" + 0.020*"Time" + 0.017*"Life" + 0.016*"need" + 0.016*"white" + 0.016*"Actor"
topic #15 (0.033): 0.029*"women" + 0.024*"here" + 0.022*"snow" + 0.019*"USA" + 0.018*"travel" + 0.018*"England" + 0.016*"sex" + 0.014*"film" + 0.013*"Paris" + 0.011*"baby"
topic diff=0.059514, rho=0.114054
PROGRESS: pass 4, at document #82000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #1 (0.033): 0.080*"TSA" + 0.064*"Obama" + 0.044*"GOP" + 0.025*"ppl" + 0.019*"Senate" + 0.018*"Palin" + 0.016*"Congress" + 0.015*"politics" + 0.013*"House" + 0.012*"Republicans"
topic #19 (0.033): 0.052*"Leslie Nielsen" + 0.041*"Today" + 0.036*"Love" + 0.027*"Jews" + 0.024*"nice" + 0.021*"right now" + 0.019*"Time" + 0.018*"Life" + 0.016*"need" + 0.016*"white"
topic #22 (0.033): 0.047*"Blog" + 0.028*"re" + 0.024*"see" + 0.023*"We" + 0.017*"believe" + 0.017*"book" + 0.017*"food" + 0.015*"WTF" + 0.014*"Post" + 0.014*"artist"
topic #20 (0.033): 0.041*"live" + 0.035*"world" + 0.034*"TV" + 0.029*"Watch" + 0.025*"Hope" + 0.025*"interview" + 0.023*"climate" + 0.021*"climate change" + 0.018*"Radio" + 0.018*"COP16"
topic #2 (0.033): 0.040*"News" + 0.020*"son" + 0.018*"Uruguay" + 0.018*"Texas" + 0.018*"NFL" + 0.016*"LMAO" + 0.015*"Tom DeLay" + 0.013*"Jewish" + 0.013*"English" + 0.012*"Boston"
topic diff=0.121922, rho=0.114054
PROGRESS: pass 4, at document #84000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.054*"Leslie Nielsen" + 0.043*"Today" + 0.040*"Love" + 0.025*"Jews" + 0.024*"nice" + 0.021*"right now" + 0.019*"Time" + 0.018*"Life" + 0.016*"Actor" + 0.015*"need"
topic #7 (0.033): 0.167*"Twitter" + 0.083*"Facebook" + 0.028*"Tweet" + 0.027*"NYC" + 0.018*"website" + 0.016*"Chinese" + 0.015*"COICA" + 0.015*"photos" + 0.015*"Tweets" + 0.012*"People"
topic #6 (0.033): 0.014*"U.S" + 0.013*"World" + 0.013*"News" + 0.010*"New York" + 0.010*"Will" + 0.009*"Women" + 0.009*"Study" + 0.009*"France" + 0.008*"NYT" + 0.008*"the Internet"
topic #13 (0.033): 0.043*"AP" + 0.020*"DADT" + 0.015*"study" + 0.014*"risk" + 0.013*"child" + 0.011*"Pentagon" + 0.010*"friend" + 0.010*"Debt" + 0.010*"report" + 0.009*"prison"
topic #28 (0.033): 0.142*"video" + 0.082*"YouTube" + 0.065*"Video" + 0.029*"Cyber Monday" + 0.026*"Holiday" + 0.024*"music" + 0.017*"ad" + 0.017*"Check" + 0.014*"Live" + 0.014*"Photography"
topic diff=0.018221, rho=0.114054
PROGRESS: pass 4, at document #86000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #0 (0.033): 0.082*"Haiti" + 0.029*"UN" + 0.024*"cholera" + 0.021*"Yahoo" + 0.020*"Gaza" + 0.018*"Lebanon" + 0.017*"Mexico" + 0.017*"Israeli" + 0.016*"death" + 0.015*"Cholera"
topic #15 (0.033): 0.036*"USA" + 0.029*"women" + 0.022*"here" + 0.019*"snow" + 0.018*"sex" + 0.017*"travel" + 0.015*"England" + 0.015*"Paris" + 0.012*"film" + 0.011*"baby"
topic #17 (0.033): 0.060*"China" + 0.045*"North Korea" + 0.027*"South Korea" + 0.024*"Ireland" + 0.021*"Korea" + 0.018*"US" + 0.017*"Europe" + 0.017*"attack" + 0.015*"BBC News" + 0.015*"U.S"
topic #6 (0.033): 0.014*"U.S" + 0.012*"World" + 0.012*"News" + 0.010*"New York" + 0.009*"Will" + 0.009*"Women" + 0.009*"NYT" + 0.008*"Study" + 0.008*"France" + 0.008*"BP"
topic #29 (0.033): 0.208*"Thanksgiving" + 0.083*"Happy Thanksgiving" + 0.044*"LOL" + 0.036*"family" + 0.035*"Turkey" + 0.034*"thanksgiving" + 0.034*"friends" + 0.028*"holiday" + 0.021*"tonight" + 0.012*"eat"
topic diff=0.032198, rho=0.114054
PROGRESS: pass 4, at document #88000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #27 (0.033): 0.099*"blog" + 0.045*"post" + 0.022*"Education" + 0.021*"DVD" + 0.020*"recipes" + 0.020*"Complexo do Alemão" + 0.017*"Amazon" + 0.015*"após" + 0.014*"web" + 0.013*"release"
topic #28 (0.033): 0.130*"video" + 0.079*"YouTube" + 0.063*"Video" + 0.031*"Cyber Monday" + 0.028*"Holiday" + 0.025*"music" + 0.016*"Check" + 0.015*"ad" + 0.015*"Photography" + 0.012*"Live"
topic #19 (0.033): 0.048*"Leslie Nielsen" + 0.041*"Love" + 0.039*"Today" + 0.024*"right now" + 0.023*"nice" + 0.022*"Jews" + 0.020*"Time" + 0.019*"Oregon" + 0.018*"Life" + 0.015*"Actor"
topic #22 (0.033): 0.040*"Blog" + 0.025*"re" + 0.025*"We" + 0.023*"see" + 0.018*"food" + 0.018*"believe" + 0.017*"book" + 0.014*"WTF" + 0.014*"health" + 0.013*"basketball"
topic #9 (0.033): 0.044*"will" + 0.026*"em" + 0.021*"do" + 0.021*"work" + 0.020*"kids" + 0.015*"play" + 0.014*"first" + 0.013*"talk" + 0.013*"children" + 0.013*"VIDEO"
topic diff=0.031478, rho=0.114054
PROGRESS: pass 4, at document #90000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.058*"China" + 0.043*"North Korea" + 0.025*"South Korea" + 0.024*"Ireland" + 0.024*"Korea" + 0.017*"US" + 0.016*"Reuters" + 0.016*"Europe" + 0.016*"attack" + 0.015*"U.S"
topic #6 (0.033): 0.014*"U.S" + 0.013*"World" + 0.012*"News" + 0.010*"Will" + 0.010*"New York" + 0.008*"Women" + 0.008*"Study" + 0.008*"Business" + 0.008*"NYT" + 0.007*"France"
topic #12 (0.033): 0.101*"God" + 0.039*"Who" + 0.020*"al" + 0.016*"guy" + 0.015*"heart" + 0.015*"left" + 0.013*"GOD" + 0.013*"Who's" + 0.013*"help" + 0.013*"NPR"
topic #14 (0.033): 0.073*"tcot" + 0.024*"Bush" + 0.014*"Islam" + 0.013*"Obama" + 0.013*"jobs" + 0.011*"Muslim" + 0.010*"woman" + 0.010*"arrested" + 0.010*"GM" + 0.010*"teaparty"
topic #21 (0.033): 0.046*"CNN" + 0.029*"education" + 0.028*"Chicago" + 0.023*"cancer" + 0.021*"Pope" + 0.021*"dans" + 0.021*"Venezuela" + 0.020*"Toronto" + 0.020*"Retweet" + 0.019*"Canada"
topic diff=0.024237, rho=0.114054
PROGRESS: pass 4, at document #92000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #29 (0.033): 0.209*"Thanksgiving" + 0.082*"Happy Thanksgiving" + 0.041*"LOL" + 0.035*"family" + 0.032*"friends" + 0.032*"Turkey" + 0.031*"thanksgiving" + 0.029*"holiday" + 0.020*"tonight" + 0.013*"eat"
topic #20 (0.033): 0.042*"live" + 0.036*"TV" + 0.035*"world" + 0.032*"Watch" + 0.027*"Hope" + 0.021*"interview" + 0.018*"Check it" + 0.018*"families" + 0.018*"COP16" + 0.018*"climate"
topic #26 (0.033): 0.044*"news" + 0.038*"India" + 0.022*"New Zealand" + 0.019*"business" + 0.016*"dead" + 0.014*"mine" + 0.013*"PM" + 0.013*"police" + 0.012*"killed" + 0.011*"Australia"
topic #9 (0.033): 0.041*"will" + 0.023*"em" + 0.021*"work" + 0.019*"kids" + 0.019*"do" + 0.016*"Chelsea" + 0.015*"first" + 0.014*"children" + 0.014*"play" + 0.013*"talk"
topic #11 (0.033): 0.065*"via" + 0.039*"Google" + 0.036*"iPad" + 0.032*"iPhone" + 0.031*"Facebook" + 0.026*"Apple" + 0.021*"Christmas" + 0.019*"online" + 0.018*"Social Media" + 0.017*"iTunes"
topic diff=0.023258, rho=0.114054
PROGRESS: pass 4, at document #94000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.072*"lol" + 0.034*"today" + 0.032*"people" + 0.027*"time" + 0.027*"you" + 0.020*"who" + 0.020*"ur" + 0.020*"can" + 0.018*"out" + 0.018*"love"
topic #1 (0.033): 0.078*"TSA" + 0.063*"Obama" + 0.046*"GOP" + 0.027*"ppl" + 0.019*"Senate" + 0.019*"Congress" + 0.017*"Republicans" + 0.017*"Palin" + 0.014*"politics" + 0.014*"House"
topic #29 (0.033): 0.208*"Thanksgiving" + 0.078*"Happy Thanksgiving" + 0.041*"LOL" + 0.035*"family" + 0.034*"thanksgiving" + 0.033*"friends" + 0.031*"Turkey" + 0.029*"holiday" + 0.020*"tonight" + 0.012*"eat"
topic #20 (0.033): 0.043*"live" + 0.036*"world" + 0.034*"TV" + 0.033*"Watch" + 0.026*"Hope" + 0.022*"interview" + 0.021*"families" + 0.021*"climate" + 0.021*"Check it" + 0.020*"California"
topic #12 (0.033): 0.091*"God" + 0.039*"Who" + 0.019*"al" + 0.016*"left" + 0.016*"Damn" + 0.015*"heart" + 0.014*"NPR" + 0.014*"guy" + 0.013*"Who's" + 0.012*"Singapore"
topic diff=0.018742, rho=0.114054
PROGRESS: pass 4, at document #96000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #22 (0.033): 0.036*"Blog" + 0.025*"re" + 0.023*"We" + 0.021*"see" + 0.021*"food" + 0.019*"book" + 0.017*"believe" + 0.014*"reason" + 0.014*"words" + 0.014*"week"
topic #10 (0.033): 0.112*"para" + 0.064*"un" + 0.045*"como" + 0.041*"dos" + 0.030*"en" + 0.024*"es" + 0.022*"la" + 0.020*"se" + 0.019*"pour" + 0.015*"si"
topic #4 (0.033): 0.026*"U.S" + 0.024*"turkey" + 0.024*"President" + 0.019*"Obama" + 0.018*"American" + 0.018*"Americans" + 0.016*"White House" + 0.016*"Cancun" + 0.015*"Seoul" + 0.014*"cooking"
topic #23 (0.033): 0.043*"haha" + 0.040*"game" + 0.030*"show" + 0.029*"football" + 0.018*"beautiful" + 0.017*"team" + 0.014*"hair" + 0.013*"Kanye West" + 0.013*"win" + 0.013*"net"
topic #29 (0.033): 0.206*"Thanksgiving" + 0.077*"Happy Thanksgiving" + 0.042*"LOL" + 0.036*"family" + 0.033*"friends" + 0.032*"holiday" + 0.032*"thanksgiving" + 0.031*"Turkey" + 0.020*"tonight" + 0.012*"Black Friday"
topic diff=0.018602, rho=0.114054
PROGRESS: pass 4, at document #98000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.026*"U.S" + 0.023*"President" + 0.023*"turkey" + 0.018*"American" + 0.018*"Obama" + 0.017*"Americans" + 0.016*"White House" + 0.016*"Seoul" + 0.015*"S. Korea" + 0.015*"Cancun"
topic #21 (0.033): 0.051*"CNN" + 0.030*"Venezuela" + 0.029*"education" + 0.029*"Chicago" + 0.024*"Pope" + 0.022*"cancer" + 0.018*"Retweet" + 0.017*"dans" + 0.017*"NATO" + 0.017*"Friday"
topic #16 (0.033): 0.037*"Black Friday" + 0.025*"read" + 0.024*"HIV" + 0.023*"Canada" + 0.023*"Sarah Palin" + 0.020*"newspaper" + 0.017*"DWTS" + 0.015*"gay" + 0.014*"Here" + 0.011*"cold"
topic #3 (0.033): 0.053*"UK" + 0.034*"London" + 0.030*"BBC" + 0.026*"photography" + 0.023*"students" + 0.021*"police" + 0.017*"British" + 0.016*"protest" + 0.015*"Colombia" + 0.013*"Police"
topic #19 (0.033): 0.048*"Love" + 0.040*"Leslie Nielsen" + 0.038*"Today" + 0.026*"Time" + 0.021*"nice" + 0.021*"Life" + 0.021*"right now" + 0.017*"Oregon" + 0.017*"Jews" + 0.017*"Actor"
topic diff=0.035451, rho=0.114054
bound: at document #0
-24.156 per-word bound, 18688324.0 perplexity estimate based on a held-out corpus of 2000 documents with 44486 words
PROGRESS: pass 4, at document #100000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #10 (0.033): 0.108*"para" + 0.067*"un" + 0.045*"como" + 0.034*"dos" + 0.029*"en" + 0.027*"es" + 0.022*"la" + 0.021*"pour" + 0.019*"se" + 0.016*"si"
topic #12 (0.033): 0.089*"God" + 0.037*"Who" + 0.023*"al" + 0.016*"left" + 0.016*"heart" + 0.014*"Gary McKinnon" + 0.014*"NPR" + 0.013*"Damn" + 0.013*"guy" + 0.012*"Bible"
topic #27 (0.033): 0.090*"blog" + 0.039*"post" + 0.021*"web" + 0.021*"DVD" + 0.018*"PC" + 0.017*"release" + 0.016*"Education" + 0.015*"Complexo do Alemão" + 0.013*"Amazon" + 0.012*"Oprah"
topic #9 (0.033): 0.043*"will" + 0.023*"em" + 0.021*"work" + 0.020*"do" + 0.019*"kids" + 0.014*"children" + 0.014*"Chelsea" + 0.013*"MSNBC" + 0.013*"first" + 0.013*"talk"
topic #24 (0.033): 0.097*"who" + 0.023*"me a" + 0.019*"hate" + 0.018*"Nice" + 0.018*"a man" + 0.017*"right" + 0.016*"Yes" + 0.016*"always" + 0.016*"Madrid" + 0.016*"Arizona"
topic diff=0.021343, rho=0.114054
PROGRESS: pass 4, at document #102000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #8 (0.033): 0.061*"Iran" + 0.043*"US" + 0.033*"Israel" + 0.030*"Afghanistan" + 0.029*"Pakistan" + 0.021*"Brasil" + 0.017*"Iraq" + 0.017*"NATO" + 0.015*"Egypt" + 0.014*"Russia"
topic #26 (0.033): 0.039*"India" + 0.033*"news" + 0.021*"New Zealand" + 0.020*"business" + 0.017*"PM" + 0.016*"murder" + 0.015*"dead" + 0.014*"Australia" + 0.014*"mine" + 0.013*"police"
topic #11 (0.033): 0.066*"via" + 0.040*"iPad" + 0.040*"Google" + 0.036*"iPhone" + 0.030*"Facebook" + 0.026*"Apple" + 0.021*"Free" + 0.020*"Christmas" + 0.019*"Social Media" + 0.018*"online"
topic #14 (0.033): 0.076*"tcot" + 0.016*"Bush" + 0.014*"teaparty" + 0.013*"jobs" + 0.013*"Obama" + 0.011*"Islam" + 0.011*"arrested" + 0.010*"Muslim" + 0.010*"GM" + 0.010*"woman"
topic #25 (0.033): 0.057*"lol" + 0.037*"today" + 0.031*"people" + 0.027*"time" + 0.024*"you" + 0.021*"who" + 0.021*"am" + 0.020*"out" + 0.019*"can" + 0.018*"ur"
topic diff=0.088803, rho=0.114054
PROGRESS: pass 4, at document #104000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #13 (0.033): 0.057*"AP" + 0.018*"Amanda Knox" + 0.015*"child" + 0.013*"study" + 0.013*"DADT" + 0.012*"risk" + 0.010*"law" + 0.010*"prison" + 0.010*"report" + 0.009*"Breaking News"
topic #19 (0.033): 0.051*"Leslie Nielsen" + 0.041*"Today" + 0.041*"Love" + 0.030*"white" + 0.023*"Time" + 0.020*"nice" + 0.020*"Life" + 0.019*"right now" + 0.018*"Actor" + 0.016*"need"
topic #28 (0.033): 0.110*"video" + 0.070*"Video" + 0.057*"YouTube" + 0.031*"Cyber Monday" + 0.027*"Holiday" + 0.022*"music" + 0.015*"Check" + 0.015*"Live" + 0.012*"ad" + 0.011*"Photography"
topic #27 (0.033): 0.096*"blog" + 0.039*"post" + 0.020*"DVD" + 0.019*"web" + 0.016*"release" + 0.016*"Amazon" + 0.016*"PC" + 0.015*"Oprah" + 0.014*"Education" + 0.013*"Complexo do Alemão"
topic #21 (0.033): 0.044*"CNN" + 0.034*"Venezuela" + 0.027*"Chicago" + 0.027*"cancer" + 0.025*"dans" + 0.023*"Pope" + 0.023*"education" + 0.019*"Retweet" + 0.018*"Friday" + 0.017*"NATO"
topic diff=0.091110, rho=0.114054
PROGRESS: pass 4, at document #106000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #7 (0.033): 0.177*"Twitter" + 0.083*"Facebook" + 0.030*"Tweet" + 0.025*"NYC" + 0.016*"Lmao" + 0.015*"Chinese" + 0.014*"photos" + 0.013*"Tweets" + 0.013*"website" + 0.011*"Simple"
topic #27 (0.033): 0.104*"blog" + 0.039*"post" + 0.019*"web" + 0.019*"DVD" + 0.018*"Amazon" + 0.017*"PC" + 0.015*"release" + 0.014*"Education" + 0.014*"Oprah" + 0.013*"bit"
topic #23 (0.033): 0.035*"game" + 0.034*"haha" + 0.033*"show" + 0.024*"football" + 0.022*"reply" + 0.017*"team" + 0.017*"net" + 0.014*"Dallas" + 0.014*"beautiful" + 0.013*"win"
topic #12 (0.033): 0.081*"God" + 0.037*"Who" + 0.027*"al" + 0.015*"left" + 0.015*"NPR" + 0.014*"green" + 0.014*"heart" + 0.013*"America" + 0.013*"Who's" + 0.012*"Celebrity"
topic #24 (0.033): 0.096*"who" + 0.021*"me a" + 0.020*"Nice" + 0.017*"media" + 0.017*"a man" + 0.017*"hate" + 0.017*"right" + 0.016*"Arizona" + 0.016*"actor" + 0.016*"Oh"
topic diff=0.023923, rho=0.114054
PROGRESS: pass 4, at document #108000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #4 (0.033): 0.045*"Mc" + 0.024*"U.S" + 0.023*"President" + 0.022*"turkey" + 0.021*"American" + 0.018*"Yahoo! News" + 0.017*"Obama" + 0.016*"Americans" + 0.015*"White House" + 0.014*"S. Korea"
topic #21 (0.033): 0.041*"CNN" + 0.029*"Venezuela" + 0.028*"Chicago" + 0.025*"Pope" + 0.024*"cancer" + 0.024*"Arsenal" + 0.022*"dans" + 0.021*"education" + 0.020*"Friday" + 0.018*"Retweet"
topic #11 (0.033): 0.072*"via" + 0.042*"Google" + 0.040*"iPhone" + 0.040*"iPad" + 0.032*"Facebook" + 0.028*"Apple" + 0.021*"Social Media" + 0.019*"Christmas" + 0.019*"iTunes" + 0.017*"online"
topic #13 (0.033): 0.052*"AP" + 0.017*"DADT" + 0.015*"child" + 0.015*"Amanda Knox" + 0.014*"study" + 0.012*"risk" + 0.010*"law" + 0.010*"prison" + 0.010*"report" + 0.009*"Pentagon"
topic #17 (0.033): 0.056*"China" + 0.043*"North Korea" + 0.026*"South Korea" + 0.026*"Ireland" + 0.023*"Japan" + 0.023*"Korea" + 0.017*"US" + 0.017*"Reuters" + 0.016*"U.S" + 0.016*"attack"
topic diff=0.024015, rho=0.114054
PROGRESS: pass 4, at document #110000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.050*"Leslie Nielsen" + 0.046*"Today" + 0.042*"Love" + 0.026*"Time" + 0.025*"white" + 0.019*"nice" + 0.019*"Life" + 0.019*"Actor" + 0.018*"right now" + 0.016*"need"
topic #27 (0.033): 0.083*"blog" + 0.074*"release" + 0.036*"post" + 0.024*"book" + 0.018*"DVD" + 0.018*"Amazon" + 0.016*"web" + 0.016*"PC" + 0.013*"Oprah" + 0.012*"Education"
topic #18 (0.033): 0.096*"WikiLeaks" + 0.091*"Wikileaks" + 0.042*"wikileaks" + 0.040*"cablegate" + 0.026*"US" + 0.021*"Guardian" + 0.018*"Wiki Leaks" + 0.015*"documents" + 0.013*"NYT" + 0.012*"cables"
topic #20 (0.033): 0.053*"TV" + 0.046*"live" + 0.035*"Watch" + 0.031*"world" + 0.023*"Hope" + 0.023*"interview" + 0.023*"climate" + 0.023*"families" + 0.019*"COP16" + 0.018*"Radio"
topic #21 (0.033): 0.039*"CNN" + 0.027*"Chicago" + 0.026*"Venezuela" + 0.025*"Pope" + 0.023*"cancer" + 0.022*"Arsenal" + 0.020*"education" + 0.020*"dans" + 0.019*"Retweet" + 0.019*"Friday"
topic diff=0.077375, rho=0.114054
PROGRESS: pass 4, at document #112000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #25 (0.033): 0.050*"lol" + 0.037*"today" + 0.031*"people" + 0.027*"time" + 0.023*"you" + 0.023*"out" + 0.022*"ur" + 0.020*"who" + 0.020*"can" + 0.019*"am"
topic #17 (0.033): 0.053*"China" + 0.040*"North Korea" + 0.030*"Irish" + 0.028*"Ireland" + 0.026*"South Korea" + 0.022*"Korea" + 0.022*"Japan" + 0.017*"US" + 0.017*"U.S" + 0.015*"BBC News"
topic #14 (0.033): 0.067*"tcot" + 0.026*"Bush" + 0.018*"Colombian" + 0.014*"jobs" + 0.012*"Obama" + 0.011*"arrested" + 0.011*"teaparty" + 0.011*"GM" + 0.010*"Muslim" + 0.010*"woman"
topic #16 (0.033): 0.042*"Black Friday" + 0.027*"read" + 0.023*"Sarah Palin" + 0.022*"HIV" + 0.022*"newspaper" + 0.020*"If" + 0.018*"DWTS" + 0.017*"Canada" + 0.016*"Here" + 0.014*"gay"
topic #8 (0.033): 0.051*"Iran" + 0.044*"US" + 0.031*"Israel" + 0.028*"Pakistan" + 0.028*"Afghanistan" + 0.017*"Brasil" + 0.016*"NATO" + 0.015*"Iraq" + 0.014*"Russia" + 0.013*"to win"
topic diff=0.030409, rho=0.114054
PROGRESS: pass 4, at document #114000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.078*"UK" + 0.036*"London" + 0.027*"BBC" + 0.024*"police" + 0.022*"students" + 0.018*"protest" + 0.017*"British" + 0.014*"demo 2010" + 0.014*"Police" + 0.013*"Colombia"
topic #17 (0.033): 0.057*"China" + 0.041*"North Korea" + 0.027*"South Korea" + 0.027*"Irish" + 0.026*"Ireland" + 0.024*"Korea" + 0.021*"Japan" + 0.018*"US" + 0.016*"U.S" + 0.015*"attack"
topic #23 (0.033): 0.033*"haha" + 0.032*"game" + 0.031*"football" + 0.030*"show" + 0.019*"Spanish" + 0.019*"team" + 0.015*"net" + 0.014*"Italian" + 0.014*"reply" + 0.014*"win"
topic #16 (0.033): 0.043*"Black Friday" + 0.026*"read" + 0.024*"Sarah Palin" + 0.022*"HIV" + 0.020*"newspaper" + 0.019*"If" + 0.018*"DWTS" + 0.017*"Canada" + 0.015*"Here" + 0.014*"gay"
topic #24 (0.033): 0.094*"who" + 0.020*"Nice" + 0.019*"a man" + 0.018*"me a" + 0.018*"hate" + 0.018*"Yes" + 0.016*"right" + 0.016*"Oh" + 0.015*"Madrid" + 0.015*"always"
topic diff=0.015069, rho=0.114054
PROGRESS: pass 4, at document #116000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #2 (0.033): 0.041*"News" + 0.029*"EUA" + 0.022*"LMAO" + 0.020*"NFL" + 0.018*"USD" + 0.016*"Las Vegas" + 0.015*"English" + 0.014*"You" + 0.013*"Boston" + 0.013*"Texas"
topic #4 (0.033): 0.030*"Mc" + 0.024*"President" + 0.023*"U.S" + 0.022*"American" + 0.021*"turkey" + 0.017*"White House" + 0.016*"Obama" + 0.016*"Hoy" + 0.016*"Americans" + 0.015*"President Obama"
topic #17 (0.033): 0.055*"China" + 0.039*"North Korea" + 0.028*"Ireland" + 0.027*"Irish" + 0.025*"South Korea" + 0.022*"Korea" + 0.020*"Japan" + 0.019*"US" + 0.016*"U.S" + 0.016*"BBC News"
topic #15 (0.033): 0.042*"USA" + 0.030*"women" + 0.030*"French" + 0.021*"Thailand" + 0.020*"here" + 0.018*"England" + 0.017*"travel" + 0.016*"Thai" + 0.016*"sex" + 0.014*"snow"
topic #23 (0.033): 0.033*"game" + 0.033*"haha" + 0.030*"football" + 0.029*"show" + 0.019*"Spanish" + 0.018*"team" + 0.015*"net" + 0.014*"reply" + 0.013*"Italian" + 0.013*"win"
topic diff=0.029974, rho=0.114054
PROGRESS: pass 4, at document #118000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.055*"via" + 0.048*"Google" + 0.044*"iPad" + 0.040*"iPhone" + 0.039*"Apple" + 0.037*"Facebook" + 0.021*"Android" + 0.017*"iTunes" + 0.016*"Social Media" + 0.016*"Microsoft"
topic #26 (0.033): 0.043*"India" + 0.034*"news" + 0.019*"New Zealand" + 0.018*"NZ" + 0.016*"PM" + 0.015*"killed" + 0.015*"Australia" + 0.014*"dead" + 0.014*"business" + 0.013*"energy"
topic #5 (0.033): 0.057*"fb" + 0.027*"Football" + 0.022*"season" + 0.020*"NBC" + 0.019*"student" + 0.018*"smile" + 0.016*"Los Angeles" + 0.016*"San Diego" + 0.016*"Sports" + 0.015*"high school"
topic #24 (0.033): 0.094*"who" + 0.021*"Nice" + 0.019*"a man" + 0.019*"me a" + 0.019*"hate" + 0.018*"Madrid" + 0.017*"Yes" + 0.016*"always" + 0.016*"right" + 0.015*"Arizona"
topic #25 (0.033): 0.053*"lol" + 0.036*"today" + 0.033*"people" + 0.025*"time" + 0.023*"you" + 0.021*"ur" + 0.021*"out" + 0.021*"who" + 0.020*"can" + 0.018*"us"
topic diff=0.016666, rho=0.114054
bound: at document #0
-23.295 per-word bound, 10291852.4 perplexity estimate based on a held-out corpus of 2000 documents with 33817 words
PROGRESS: pass 4, at document #120000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.020*"jajaja" + 0.016*"News" + 0.015*"U.S" + 0.013*"World" + 0.010*"Women" + 0.010*"New York" + 0.010*"Will" + 0.009*"Billion" + 0.008*"Business" + 0.008*"Study"
topic #21 (0.033): 0.040*"CNN" + 0.031*"Chicago" + 0.029*"Venezuela" + 0.025*"Retweet" + 0.025*"dans" + 0.020*"Pope" + 0.020*"cancer" + 0.018*"Friday" + 0.018*"Arsenal" + 0.017*"Toronto"
topic #7 (0.033): 0.199*"Twitter" + 0.081*"Facebook" + 0.035*"NYC" + 0.023*"Tweet" + 0.013*"Chinese" + 0.013*"Thx" + 0.012*"uploaded" + 0.012*"photos" + 0.011*"website" + 0.010*"Lmao"
topic #28 (0.033): 0.111*"video" + 0.082*"YouTube" + 0.071*"Video" + 0.029*"Cyber Monday" + 0.027*"Holiday" + 0.023*"music" + 0.016*"Check" + 0.013*"Live" + 0.011*"Photography" + 0.010*"Page"
topic #3 (0.033): 0.075*"UK" + 0.037*"London" + 0.025*"police" + 0.025*"BBC" + 0.020*"students" + 0.018*"Cuba" + 0.017*"protest" + 0.015*"British" + 0.013*"Police" + 0.012*"Colombia"
topic diff=0.030448, rho=0.114054
PROGRESS: pass 4, at document #122000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #9 (0.033): 0.042*"will" + 0.033*"em" + 0.024*"do" + 0.019*"work" + 0.017*"kids" + 0.015*"VIDEO" + 0.014*"mind" + 0.013*"children" + 0.012*"first" + 0.012*"Thank you"
topic #15 (0.033): 0.038*"USA" + 0.029*"women" + 0.024*"French" + 0.019*"here" + 0.019*"England" + 0.019*"snow" + 0.019*"Thailand" + 0.017*"travel" + 0.016*"sex" + 0.014*"Thai"
topic #5 (0.033): 0.048*"fb" + 0.024*"season" + 0.023*"Football" + 0.020*"NBC" + 0.019*"student" + 0.018*"smile" + 0.016*"quote" + 0.016*"high school" + 0.015*"Sports" + 0.015*"Wisconsin"
topic #23 (0.033): 0.036*"game" + 0.032*"show" + 0.029*"football" + 0.028*"haha" + 0.018*"team" + 0.016*"NBA" + 0.016*"Spanish" + 0.015*"win" + 0.015*"Kanye West" + 0.014*"beautiful"
topic #17 (0.033): 0.053*"China" + 0.042*"North Korea" + 0.028*"South Korea" + 0.026*"Ireland" + 0.024*"Irish" + 0.022*"Korea" + 0.019*"Japan" + 0.019*"US" + 0.017*"BBC News" + 0.016*"U.S"
topic diff=0.016688, rho=0.114054
PROGRESS: pass 4, at document #124000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #19 (0.033): 0.057*"Love" + 0.044*"Leslie Nielsen" + 0.039*"Today" + 0.021*"Time" + 0.020*"Life" + 0.019*"right now" + 0.018*"nice" + 0.018*"Music" + 0.017*"Actor" + 0.016*"white"
topic #5 (0.033): 0.045*"fb" + 0.024*"Football" + 0.024*"season" + 0.020*"student" + 0.019*"NBC" + 0.018*"smile" + 0.016*"high school" + 0.015*"Los Angeles" + 0.015*"quote" + 0.014*"Sports"
topic #12 (0.033): 0.111*"God" + 0.057*"Celebrity" + 0.037*"Who" + 0.024*"al" + 0.014*"heart" + 0.013*"NPR" + 0.013*"left" + 0.013*"GOD" + 0.011*"DREAM Act" + 0.011*"evil"
topic #24 (0.033): 0.094*"who" + 0.022*"a man" + 0.020*"Nice" + 0.020*"hate" + 0.019*"media" + 0.017*"me a" + 0.017*"always" + 0.016*"right" + 0.016*"Madrid" + 0.016*"Yes"
topic #4 (0.033): 0.024*"President" + 0.023*"American" + 0.022*"U.S" + 0.020*"Mc" + 0.019*"turkey" + 0.018*"cooking" + 0.017*"Hoy" + 0.017*"Obama" + 0.016*"White House" + 0.015*"Yahoo! News"
topic diff=0.037384, rho=0.114054
PROGRESS: pass 4, at document #126000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #14 (0.033): 0.053*"tcot" + 0.018*"Bush" + 0.014*"jobs" + 0.011*"GM" + 0.011*"Obama" + 0.010*"arrested" + 0.010*"FBI" + 0.010*"woman" + 0.010*"Ohio" + 0.009*"Muslim"
topic #16 (0.033): 0.046*"Black Friday" + 0.024*"Sarah Palin" + 0.023*"read" + 0.022*"HIV" + 0.017*"If" + 0.017*"DWTS" + 0.015*"Here" + 0.014*"Canada" + 0.014*"gay" + 0.014*"newspaper"
topic #3 (0.033): 0.083*"UK" + 0.037*"London" + 0.025*"BBC" + 0.023*"police" + 0.020*"students" + 0.019*"British" + 0.017*"protest" + 0.015*"Cuba" + 0.015*"Colombia" + 0.013*"Police"
topic #24 (0.033): 0.094*"who" + 0.024*"a man" + 0.020*"hate" + 0.020*"Nice" + 0.019*"me a" + 0.018*"always" + 0.018*"media" + 0.017*"right" + 0.017*"Oh" + 0.016*"Yes"
topic #27 (0.033): 0.084*"blog" + 0.040*"release" + 0.035*"post" + 0.020*"Amazon" + 0.019*"Oprah" + 0.018*"DVD" + 0.018*"PC" + 0.018*"recipes" + 0.017*"Complexo do Alemão" + 0.015*"web"
topic diff=0.050397, rho=0.114054
PROGRESS: pass 4, at document #128000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #12 (0.033): 0.104*"God" + 0.045*"Celebrity" + 0.038*"Who" + 0.023*"al" + 0.013*"Who's" + 0.013*"heart" + 0.012*"left" + 0.012*"guy" + 0.012*"GOD" + 0.011*"Rock"
topic #9 (0.033): 0.042*"will" + 0.024*"em" + 0.021*"kids" + 0.020*"work" + 0.019*"do" + 0.016*"children" + 0.016*"VIDEO" + 0.015*"mind" + 0.013*"soul" + 0.012*"play"
topic #29 (0.033): 0.211*"Thanksgiving" + 0.079*"Happy Thanksgiving" + 0.044*"LOL" + 0.043*"family" + 0.032*"friends" + 0.028*"holiday" + 0.026*"Turkey" + 0.025*"thanksgiving" + 0.018*"tonight" + 0.015*"Black Friday"
topic #0 (0.033): 0.078*"Haiti" + 0.041*"Amazon.com" + 0.028*"cholera" + 0.022*"UN" + 0.019*"Dutch" + 0.018*"Asian" + 0.018*"Israeli" + 0.017*"Gaza" + 0.016*"election" + 0.015*"gold"
topic #14 (0.033): 0.050*"tcot" + 0.018*"Bush" + 0.015*"jobs" + 0.012*"Obama" + 0.011*"GM" + 0.010*"Ohio" + 0.010*"Muslim" + 0.010*"arrested" + 0.009*"FBI" + 0.009*"woman"
topic diff=0.031891, rho=0.114054
PROGRESS: pass 4, at document #130000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #28 (0.033): 0.125*"video" + 0.072*"Video" + 0.068*"YouTube" + 0.024*"music" + 0.024*"Holiday" + 0.024*"Cyber Monday" + 0.021*"Check" + 0.013*"HD" + 0.011*"Live" + 0.010*"Photos"
topic #8 (0.033): 0.046*"US" + 0.040*"Iran" + 0.032*"Israel" + 0.029*"Pakistan" + 0.028*"Afghanistan" + 0.018*"Russia" + 0.017*"Brasil" + 0.017*"NATO" + 0.014*"to win" + 0.013*"Iraq"
topic #7 (0.033): 0.381*"Twitter" + 0.059*"Facebook" + 0.025*"NYC" + 0.019*"Tweet" + 0.012*"photos" + 0.010*"Chinese" + 0.010*"Lmao" + 0.009*"Lady Gaga" + 0.009*"website" + 0.008*"Thx"
topic #27 (0.033): 0.082*"blog" + 0.034*"release" + 0.033*"post" + 0.021*"DVD" + 0.020*"Oprah" + 0.020*"Amazon" + 0.017*"PC" + 0.017*"recipes" + 0.015*"bit" + 0.014*"web"
topic #6 (0.033): 0.016*"News" + 0.014*"U.S" + 0.013*"World" + 0.013*"BP" + 0.012*"jajaja" + 0.011*"New York" + 0.010*"Will" + 0.009*"Women" + 0.009*"Business" + 0.008*"Earth"
topic diff=0.020894, rho=0.114054
PROGRESS: pass 4, at document #132000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.092*"who" + 0.022*"a man" + 0.022*"hate" + 0.020*"Nice" + 0.020*"always" + 0.019*"me a" + 0.018*"Oh" + 0.017*"media" + 0.016*"Yes" + 0.016*"right"
topic #16 (0.033): 0.043*"Black Friday" + 0.022*"Sarah Palin" + 0.022*"read" + 0.021*"HIV" + 0.017*"If" + 0.015*"DWTS" + 0.015*"Here" + 0.013*"Dec" + 0.013*"Canada" + 0.013*"to show"
topic #21 (0.033): 0.066*"CNN" + 0.041*"Retweet" + 0.028*"Chicago" + 0.023*"Venezuela" + 0.021*"Pope" + 0.020*"cancer" + 0.020*"Friday" + 0.016*"education" + 0.015*"dans" + 0.014*"condoms"
topic #20 (0.033): 0.047*"live" + 0.042*"TV" + 0.035*"Watch" + 0.034*"world" + 0.033*"Hope" + 0.027*"COP16" + 0.025*"interview" + 0.021*"Check it" + 0.021*"climate" + 0.020*"families"
topic #22 (0.033): 0.074*"week" + 0.042*"Blog" + 0.024*"book" + 0.022*"re" + 0.022*"We" + 0.016*"Vintage" + 0.015*"see" + 0.015*"believe" + 0.015*"basketball" + 0.014*"food"
topic diff=0.024954, rho=0.114054
PROGRESS: pass 4, at document #134000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #17 (0.033): 0.060*"China" + 0.042*"North Korea" + 0.029*"South Korea" + 0.026*"Korea" + 0.025*"Ireland" + 0.020*"Japan" + 0.018*"US" + 0.018*"U.S" + 0.018*"Irish" + 0.014*"attack"
topic #13 (0.033): 0.041*"add" + 0.038*"Breaking News" + 0.036*"AP" + 0.015*"study" + 0.015*"Rome" + 0.014*"DADT" + 0.011*"child" + 0.010*"risk" + 0.009*"weather" + 0.009*"report"
topic #22 (0.033): 0.082*"Blog" + 0.065*"week" + 0.023*"book" + 0.023*"re" + 0.022*"We" + 0.014*"believe" + 0.014*"Vintage" + 0.014*"see" + 0.014*"basketball" + 0.014*"food"
topic #27 (0.033): 0.106*"blog" + 0.031*"post" + 0.029*"release" + 0.020*"Oprah" + 0.019*"Amazon" + 0.018*"DVD" + 0.016*"PC" + 0.015*"recipes" + 0.015*"bit" + 0.013*"Choice"
topic #11 (0.033): 0.054*"via" + 0.042*"Google" + 0.040*"iPhone" + 0.038*"Facebook" + 0.038*"iPad" + 0.033*"Apple" + 0.020*"iTunes" + 0.019*"Android" + 0.019*"Christmas" + 0.017*"Social Media"
topic diff=0.021571, rho=0.114054
PROGRESS: pass 4, at document #136000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #3 (0.033): 0.074*"UK" + 0.036*"London" + 0.022*"BBC" + 0.020*"police" + 0.019*"students" + 0.019*"British" + 0.018*"photography" + 0.016*"protest" + 0.013*"Cuba" + 0.012*"Colombia"
topic #4 (0.033): 0.024*"turkey" + 0.024*"President" + 0.021*"White House" + 0.021*"American" + 0.021*"U.S" + 0.019*"Seoul" + 0.017*"Obama" + 0.017*"Hoy" + 0.017*"President Obama" + 0.016*"cooking"
topic #22 (0.033): 0.076*"Blog" + 0.059*"week" + 0.022*"re" + 0.022*"book" + 0.022*"We" + 0.015*"believe" + 0.014*"see" + 0.014*"food" + 0.014*"basketball" + 0.014*"words"
topic #7 (0.033): 0.323*"Twitter" + 0.061*"Facebook" + 0.030*"NYC" + 0.020*"Tweet" + 0.016*"Lady Gaga" + 0.015*"Founder" + 0.014*"photos" + 0.011*"Chinese" + 0.010*"website" + 0.009*"Lmao"
topic #27 (0.033): 0.115*"blog" + 0.040*"post" + 0.026*"release" + 0.021*"Oprah" + 0.019*"DVD" + 0.018*"Amazon" + 0.016*"Choice" + 0.015*"PC" + 0.015*"recipes" + 0.014*"bit"
topic diff=0.015326, rho=0.114054
PROGRESS: pass 4, at document #138000/143749
performing inference on a chunk of 2000 documents
1999/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #11 (0.033): 0.050*"via" + 0.041*"Google" + 0.039*"Facebook" + 0.038*"iPhone" + 0.035*"iPad" + 0.030*"Apple" + 0.021*"Android" + 0.021*"Christmas" + 0.018*"iTunes" + 0.017*"online"
topic #4 (0.033): 0.023*"turkey" + 0.023*"President" + 0.021*"American" + 0.021*"U.S" + 0.020*"White House" + 0.018*"Obama" + 0.018*"Seoul" + 0.017*"Hoy" + 0.016*"President Obama" + 0.015*"cooking"
topic #27 (0.033): 0.112*"blog" + 0.039*"post" + 0.024*"release" + 0.022*"Oprah" + 0.018*"DVD" + 0.018*"Amazon" + 0.016*"Choice" + 0.015*"PC" + 0.015*"recipes" + 0.014*"web"
topic #28 (0.033): 0.144*"video" + 0.064*"Video" + 0.060*"YouTube" + 0.027*"Holiday" + 0.026*"music" + 0.022*"Cyber Monday" + 0.018*"Check" + 0.014*"Photography" + 0.013*"Live" + 0.011*"Tonight"
topic #8 (0.033): 0.045*"US" + 0.038*"Iran" + 0.035*"Israel" + 0.030*"Pakistan" + 0.028*"Afghanistan" + 0.019*"NATO" + 0.018*"Egypt" + 0.017*"Russia" + 0.013*"to win" + 0.013*"Afghan"
topic diff=0.021118, rho=0.114054
bound: at document #0
-22.269 per-word bound, 5053610.8 perplexity estimate based on a held-out corpus of 2000 documents with 78501 words
PROGRESS: pass 4, at document #140000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #6 (0.033): 0.015*"U.S" + 0.014*"News" + 0.013*"Business" + 0.013*"World" + 0.012*"New York" + 0.010*"Will" + 0.010*"Health" + 0.009*"Pres" + 0.009*"BP" + 0.009*"Women"
topic #27 (0.033): 0.106*"blog" + 0.038*"post" + 0.034*"Back" + 0.024*"release" + 0.020*"Oprah" + 0.018*"Amazon" + 0.016*"DVD" + 0.015*"MacBook Air" + 0.015*"PC" + 0.013*"Choice"
topic #3 (0.033): 0.070*"UK" + 0.036*"London" + 0.022*"BBC" + 0.021*"police" + 0.019*"students" + 0.019*"British" + 0.015*"protest" + 0.015*"photography" + 0.013*"Cuba" + 0.012*"Colombia"
topic #29 (0.033): 0.205*"Thanksgiving" + 0.071*"Happy Thanksgiving" + 0.042*"family" + 0.040*"LOL" + 0.038*"friends" + 0.031*"holiday" + 0.025*"Turkey" + 0.024*"thanksgiving" + 0.018*"tonight" + 0.017*"Black Friday"
topic #25 (0.033): 0.050*"tweet" + 0.044*"lol" + 0.033*"people" + 0.028*"today" + 0.026*"now" + 0.025*"time" + 0.024*"you" + 0.020*"can" + 0.019*"who" + 0.019*"give me"
topic diff=0.069870, rho=0.114054
PROGRESS: pass 4, at document #142000/143749
performing inference on a chunk of 2000 documents
2000/2000 documents converged within 50 iterations
updating topics
merging changes from 2000 documents into a model of 143749 documents
topic #24 (0.033): 0.087*"who" + 0.034*"college" + 0.025*"hate" + 0.020*"fat" + 0.020*"me a" + 0.020*"Oh" + 0.020*"a man" + 0.019*"Nice" + 0.018*"always" + 0.017*"Yes"
topic #15 (0.033): 0.042*"Home" + 0.029*"USA" + 0.028*"sex" + 0.024*"women" + 0.021*"here" + 0.018*"girl" + 0.017*"snow" + 0.016*"England" + 0.015*"travel" + 0.015*"School"
topic #8 (0.033): 0.043*"US" + 0.040*"Iran" + 0.032*"Israel" + 0.029*"Afghanistan" + 0.025*"Pakistan" + 0.018*"NATO" + 0.018*"Egypt" + 0.016*"Russia" + 0.014*"to win" + 0.013*"Afghan"
topic #11 (0.033): 0.062*"via" + 0.046*"iPad" + 0.043*"iPhone" + 0.038*"Google" + 0.038*"Facebook" + 0.031*"Apple" + 0.021*"Android" + 0.020*"Christmas" + 0.018*"iTunes" + 0.017*"online"
topic #23 (0.033): 0.042*"game" + 0.037*"haha" + 0.030*"show" + 0.025*"RETWEET" + 0.024*"football" + 0.019*"net" + 0.017*"team" + 0.016*"beautiful" + 0.015*"Nicki Minaj" + 0.014*"win"
topic diff=0.024851, rho=0.114054
bound: at document #0
-23.343 per-word bound, 10641685.2 perplexity estimate based on a held-out corpus of 1749 documents with 49577 words
PROGRESS: pass 4, at document #143749/143749
performing inference on a chunk of 1749 documents
1749/1749 documents converged within 50 iterations
updating topics
merging changes from 1749 documents into a model of 143749 documents
topic #23 (0.033): 0.043*"game" + 0.037*"haha" + 0.029*"show" + 0.027*"football" + 0.026*"RETWEET" + 0.019*"net" + 0.016*"team" + 0.015*"beautiful" + 0.014*"Apple iPad" + 0.014*"win"
topic #1 (0.033): 0.058*"Obama" + 0.054*"TSA" + 0.044*"GOP" + 0.031*"ppl" + 0.021*"Palin" + 0.019*"Senate" + 0.018*"Congress" + 0.016*"vote" + 0.016*"Republicans" + 0.015*"Sarah Palin"
topic #28 (0.033): 0.132*"video" + 0.073*"Video" + 0.057*"YouTube" + 0.026*"Holiday" + 0.025*"music" + 0.022*"Cyber Monday" + 0.022*"WOW" + 0.019*"Check" + 0.017*"Live" + 0.011*"Tonight"
topic #20 (0.033): 0.054*"live" + 0.041*"world" + 0.040*"Check it" + 0.036*"Check it out" + 0.036*"TV" + 0.034*"Watch" + 0.030*"Hope" + 0.027*"COP16" + 0.026*"interview" + 0.018*"families"
topic #25 (0.033): 0.048*"lol" + 0.040*"tweet" + 0.033*"people" + 0.030*"give me" + 0.028*"today" + 0.024*"time" + 0.024*"you" + 0.021*"now" + 0.020*"twitter" + 0.019*"ur"
topic diff=0.015342, rho=0.114054
starting a new internal lifecycle event log for LdaModel
LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=100000, num_topics=30, decay=0.5, chunksize=2000> in 231.75s', 'datetime': '2022-05-27T15:46:17.757263', 'gensim': '4.2.0', 'python': '3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}
starting a new internal lifecycle event log for LdaState
LdaState lifecycle event {'fname_or_handle': '../output/19/tml/gensim_30topics.model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-05-27T15:46:17.757263', 'gensim': '4.2.0', 'python': '3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}
{'uri': '../output/19/tml/gensim_30topics.model.state', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
saved ../output/19/tml/gensim_30topics.model.state
{'uri': '../output/19/tml/gensim_30topics.model.id2word', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
LdaModel lifecycle event {'fname_or_handle': '../output/19/tml/gensim_30topics.model', 'separately': "['expElogbeta', 'sstats']", 'sep_limit': 10485760, 'ignore': ['id2word', 'state', 'dispatcher'], 'datetime': '2022-05-27T15:46:17.847310', 'gensim': '4.2.0', 'python': '3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}
storing np array 'expElogbeta' to ../output/19/tml/gensim_30topics.model.expElogbeta.npy
not storing attribute id2word
not storing attribute state
not storing attribute dispatcher
{'uri': '../output/19/tml/gensim_30topics.model', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
saved ../output/19/tml/gensim_30topics.model
topic #0 (0.033): 0.078*"Haiti" + 0.026*"cholera" + 0.021*"UN" + 0.019*"Amazon.com" + 0.017*"election" + 0.016*"death" + 0.015*"Israeli" + 0.015*"Cholera" + 0.015*"Asian" + 0.014*"Gaza"
topic #1 (0.033): 0.058*"Obama" + 0.054*"TSA" + 0.044*"GOP" + 0.031*"ppl" + 0.021*"Palin" + 0.019*"Senate" + 0.018*"Congress" + 0.016*"vote" + 0.016*"Republicans" + 0.015*"Sarah Palin"
topic #2 (0.033): 0.036*"bit.ly" + 0.034*"News" + 0.024*"NFL" + 0.019*"English" + 0.017*"USD" + 0.017*"Boston" + 0.016*"Texas" + 0.015*"You" + 0.015*"LMAO" + 0.013*"songs"
topic #3 (0.033): 0.066*"UK" + 0.037*"London" + 0.021*"BBC" + 0.020*"police" + 0.019*"students" + 0.018*"protest" + 0.018*"school" + 0.018*"British" + 0.015*"Colombia" + 0.013*"failure"
topic #4 (0.033): 0.029*"President" + 0.024*"American" + 0.021*"turkey" + 0.021*"U.S" + 0.019*"Obama" + 0.018*"White House" + 0.018*"President Obama" + 0.016*"Hoy" + 0.015*"Americans" + 0.014*"Seoul"
topic #5 (0.033): 0.033*"NBC" + 0.026*"IM" + 0.023*"TODAY" + 0.022*"fb" + 0.021*"season" + 0.021*"student" + 0.018*"smile" + 0.018*"high school" + 0.017*"Football" + 0.016*"method"
topic #6 (0.033): 0.014*"News" + 0.014*"U.S" + 0.013*"Business" + 0.012*"World" + 0.011*"Bloomberg" + 0.011*"Health" + 0.011*"New York" + 0.011*"Will" + 0.009*"Women" + 0.009*"Pres"
topic #7 (0.033): 0.299*"Twitter" + 0.064*"Facebook" + 0.061*"NYC" + 0.021*"Tweet" + 0.013*"Lady Gaga" + 0.012*"photos" + 0.011*"website" + 0.010*"Chinese" + 0.010*"Founder" + 0.008*"wind"
topic #8 (0.033): 0.043*"US" + 0.038*"Iran" + 0.030*"Israel" + 0.029*"Afghanistan" + 0.024*"Pakistan" + 0.020*"NATO" + 0.020*"Russia" + 0.016*"Egypt" + 0.015*"Nato" + 0.015*"to win"
topic #9 (0.033): 0.048*"kids" + 0.041*"will" + 0.018*"work" + 0.015*"god" + 0.015*"down" + 0.014*"do" + 0.013*"mind" + 0.013*"children" + 0.013*"talk" + 0.013*"soul"
topic #10 (0.033): 0.112*"Como" + 0.072*"un" + 0.070*"para" + 0.039*"como" + 0.035*"es" + 0.028*"en" + 0.026*"la" + 0.018*"dos" + 0.017*"este" + 0.016*"más"
topic #11 (0.033): 0.062*"via" + 0.046*"iPad" + 0.046*"iPhone" + 0.041*"Google" + 0.038*"Facebook" + 0.030*"Apple" + 0.019*"Android" + 0.019*"Christmas" + 0.018*"iTunes" + 0.017*"Free"
topic #12 (0.033): 0.118*"God" + 0.031*"Who" + 0.026*"al" + 0.022*"Celebrity" + 0.017*"help" + 0.016*"TED" + 0.014*"dogs" + 0.013*"guy" + 0.013*"heart" + 0.013*"fashion"
topic #13 (0.033): 0.039*"AP" + 0.025*"DADT" + 0.025*"Breaking News" + 0.023*"add" + 0.014*"study" + 0.012*"Notre Dame" + 0.011*"child" + 0.011*"Art" + 0.010*"prison" + 0.010*"report"
topic #14 (0.033): 0.051*"hanging" + 0.046*"tcot" + 0.018*"Bush" + 0.014*"jobs" + 0.012*"Obama" + 0.009*"arrested" + 0.009*"GM" + 0.009*"Ohio" + 0.008*"job" + 0.008*"shot"
topic #15 (0.033): 0.038*"Home" + 0.032*"sex" + 0.031*"USA" + 0.023*"women" + 0.021*"here" + 0.017*"snow" + 0.016*"girl" + 0.015*"England" + 0.014*"School" + 0.014*"travel"
topic #16 (0.033): 0.068*"party" + 0.036*"Black Friday" + 0.022*"read" + 0.021*"Sarah Palin" + 0.020*"chat" + 0.019*"cash" + 0.017*"HIV" + 0.015*"DWTS" + 0.014*"gay" + 0.014*"If"
topic #17 (0.033): 0.054*"China" + 0.045*"North Korea" + 0.031*"South Korea" + 0.025*"Ireland" + 0.024*"Korea" + 0.017*"U.S" + 0.017*"Irish" + 0.017*"US" + 0.017*"Japan" + 0.014*"attack"
topic #18 (0.033): 0.088*"WikiLeaks" + 0.077*"Wikileaks" + 0.029*"wikileaks" + 0.027*"Wikipedia" + 0.024*"cablegate" + 0.023*"US" + 0.020*"secret" + 0.018*"Wiki Leaks" + 0.018*"Asia" + 0.016*"Guardian"
topic #19 (0.033): 0.049*"Love" + 0.043*"Leslie Nielsen" + 0.037*"Today" + 0.033*"need" + 0.027*"Music" + 0.024*"Time" + 0.021*"right now" + 0.021*"nice" + 0.020*"Jets" + 0.017*"Life"
topic #20 (0.033): 0.054*"live" + 0.041*"world" + 0.040*"Check it" + 0.036*"Check it out" + 0.036*"TV" + 0.034*"Watch" + 0.030*"Hope" + 0.027*"COP16" + 0.026*"interview" + 0.018*"families"
topic #21 (0.033): 0.051*"CNN" + 0.044*"Retweet" + 0.029*"speech" + 0.023*"cancer" + 0.023*"Chicago" + 0.023*"Friday" + 0.021*"Pope" + 0.019*"Venezuela" + 0.019*"condoms" + 0.017*"Toronto"
topic #22 (0.033): 0.062*"Blog" + 0.041*"week" + 0.024*"re" + 0.022*"believe" + 0.021*"book" + 0.021*"We" + 0.018*"change" + 0.017*"basketball" + 0.017*"learn" + 0.014*"see"
topic #23 (0.033): 0.043*"game" + 0.037*"haha" + 0.029*"show" + 0.027*"football" + 0.026*"RETWEET" + 0.019*"net" + 0.016*"team" + 0.015*"beautiful" + 0.014*"Apple iPad" + 0.014*"win"
topic #24 (0.033): 0.087*"who" + 0.031*"college" + 0.026*"fat" + 0.023*"hate" + 0.020*"Oh" + 0.020*"me a" + 0.020*"a man" + 0.019*"always" + 0.019*"Nice" + 0.017*"Yes"
topic #25 (0.033): 0.048*"lol" + 0.040*"tweet" + 0.033*"people" + 0.030*"give me" + 0.028*"today" + 0.024*"time" + 0.024*"you" + 0.021*"now" + 0.020*"twitter" + 0.019*"ur"
topic #26 (0.033): 0.037*"news" + 0.034*"India" + 0.020*"New Zealand" + 0.017*"business" + 0.015*"dead" + 0.015*"feeling" + 0.014*"killed" + 0.014*"mine" + 0.014*"Australia" + 0.013*"banned"
topic #27 (0.033): 0.105*"blog" + 0.074*"Back" + 0.034*"post" + 0.023*"Education" + 0.021*"release" + 0.018*"Oprah" + 0.017*"DVD" + 0.017*"Amazon" + 0.014*"web" + 0.012*"MacBook Air"
topic #28 (0.033): 0.132*"video" + 0.073*"Video" + 0.057*"YouTube" + 0.026*"Holiday" + 0.025*"music" + 0.022*"Cyber Monday" + 0.022*"WOW" + 0.019*"Check" + 0.017*"Live" + 0.011*"Tonight"
topic #29 (0.033): 0.204*"Thanksgiving" + 0.076*"Happy Thanksgiving" + 0.046*"LOL" + 0.040*"family" + 0.038*"friends" + 0.029*"holiday" + 0.024*"thanksgiving" + 0.023*"Turkey" + 0.017*"tonight" + 0.016*"go"
TopicModeling: GENSIM Topic: 0 
Words: 0.078*"Haiti" + 0.026*"cholera" + 0.021*"UN" + 0.019*"Amazon.com" + 0.017*"election" + 0.016*"death" + 0.015*"Israeli" + 0.015*"Cholera" + 0.015*"Asian" + 0.014*"Gaza"
TopicModeling: GENSIM Topic: 1 
Words: 0.058*"Obama" + 0.054*"TSA" + 0.044*"GOP" + 0.031*"ppl" + 0.021*"Palin" + 0.019*"Senate" + 0.018*"Congress" + 0.016*"vote" + 0.016*"Republicans" + 0.015*"Sarah Palin"
TopicModeling: GENSIM Topic: 2 
Words: 0.036*"bit.ly" + 0.034*"News" + 0.024*"NFL" + 0.019*"English" + 0.017*"USD" + 0.017*"Boston" + 0.016*"Texas" + 0.015*"You" + 0.015*"LMAO" + 0.013*"songs"
TopicModeling: GENSIM Topic: 3 
Words: 0.066*"UK" + 0.037*"London" + 0.021*"BBC" + 0.020*"police" + 0.019*"students" + 0.018*"protest" + 0.018*"school" + 0.018*"British" + 0.015*"Colombia" + 0.013*"failure"
TopicModeling: GENSIM Topic: 4 
Words: 0.029*"President" + 0.024*"American" + 0.021*"turkey" + 0.021*"U.S" + 0.019*"Obama" + 0.018*"White House" + 0.018*"President Obama" + 0.016*"Hoy" + 0.015*"Americans" + 0.014*"Seoul"
TopicModeling: GENSIM Topic: 5 
Words: 0.033*"NBC" + 0.026*"IM" + 0.023*"TODAY" + 0.022*"fb" + 0.021*"season" + 0.021*"student" + 0.018*"smile" + 0.018*"high school" + 0.017*"Football" + 0.016*"method"
TopicModeling: GENSIM Topic: 6 
Words: 0.014*"News" + 0.014*"U.S" + 0.013*"Business" + 0.012*"World" + 0.011*"Bloomberg" + 0.011*"Health" + 0.011*"New York" + 0.011*"Will" + 0.009*"Women" + 0.009*"Pres"
TopicModeling: GENSIM Topic: 7 
Words: 0.299*"Twitter" + 0.064*"Facebook" + 0.061*"NYC" + 0.021*"Tweet" + 0.013*"Lady Gaga" + 0.012*"photos" + 0.011*"website" + 0.010*"Chinese" + 0.010*"Founder" + 0.008*"wind"
TopicModeling: GENSIM Topic: 8 
Words: 0.043*"US" + 0.038*"Iran" + 0.030*"Israel" + 0.029*"Afghanistan" + 0.024*"Pakistan" + 0.020*"NATO" + 0.020*"Russia" + 0.016*"Egypt" + 0.015*"Nato" + 0.015*"to win"
TopicModeling: GENSIM Topic: 9 
Words: 0.048*"kids" + 0.041*"will" + 0.018*"work" + 0.015*"god" + 0.015*"down" + 0.014*"do" + 0.013*"mind" + 0.013*"children" + 0.013*"talk" + 0.013*"soul"
TopicModeling: GENSIM Topic: 10 
Words: 0.112*"Como" + 0.072*"un" + 0.070*"para" + 0.039*"como" + 0.035*"es" + 0.028*"en" + 0.026*"la" + 0.018*"dos" + 0.017*"este" + 0.016*"más"
TopicModeling: GENSIM Topic: 11 
Words: 0.062*"via" + 0.046*"iPad" + 0.046*"iPhone" + 0.041*"Google" + 0.038*"Facebook" + 0.030*"Apple" + 0.019*"Android" + 0.019*"Christmas" + 0.018*"iTunes" + 0.017*"Free"
TopicModeling: GENSIM Topic: 12 
Words: 0.118*"God" + 0.031*"Who" + 0.026*"al" + 0.022*"Celebrity" + 0.017*"help" + 0.016*"TED" + 0.014*"dogs" + 0.013*"guy" + 0.013*"heart" + 0.013*"fashion"
TopicModeling: GENSIM Topic: 13 
Words: 0.039*"AP" + 0.025*"DADT" + 0.025*"Breaking News" + 0.023*"add" + 0.014*"study" + 0.012*"Notre Dame" + 0.011*"child" + 0.011*"Art" + 0.010*"prison" + 0.010*"report"
TopicModeling: GENSIM Topic: 14 
Words: 0.051*"hanging" + 0.046*"tcot" + 0.018*"Bush" + 0.014*"jobs" + 0.012*"Obama" + 0.009*"arrested" + 0.009*"GM" + 0.009*"Ohio" + 0.008*"job" + 0.008*"shot"
TopicModeling: GENSIM Topic: 15 
Words: 0.038*"Home" + 0.032*"sex" + 0.031*"USA" + 0.023*"women" + 0.021*"here" + 0.017*"snow" + 0.016*"girl" + 0.015*"England" + 0.014*"School" + 0.014*"travel"
TopicModeling: GENSIM Topic: 16 
Words: 0.068*"party" + 0.036*"Black Friday" + 0.022*"read" + 0.021*"Sarah Palin" + 0.020*"chat" + 0.019*"cash" + 0.017*"HIV" + 0.015*"DWTS" + 0.014*"gay" + 0.014*"If"
TopicModeling: GENSIM Topic: 17 
Words: 0.054*"China" + 0.045*"North Korea" + 0.031*"South Korea" + 0.025*"Ireland" + 0.024*"Korea" + 0.017*"U.S" + 0.017*"Irish" + 0.017*"US" + 0.017*"Japan" + 0.014*"attack"
TopicModeling: GENSIM Topic: 18 
Words: 0.088*"WikiLeaks" + 0.077*"Wikileaks" + 0.029*"wikileaks" + 0.027*"Wikipedia" + 0.024*"cablegate" + 0.023*"US" + 0.020*"secret" + 0.018*"Wiki Leaks" + 0.018*"Asia" + 0.016*"Guardian"
TopicModeling: GENSIM Topic: 19 
Words: 0.049*"Love" + 0.043*"Leslie Nielsen" + 0.037*"Today" + 0.033*"need" + 0.027*"Music" + 0.024*"Time" + 0.021*"right now" + 0.021*"nice" + 0.020*"Jets" + 0.017*"Life"
TopicModeling: GENSIM Topic: 20 
Words: 0.054*"live" + 0.041*"world" + 0.040*"Check it" + 0.036*"Check it out" + 0.036*"TV" + 0.034*"Watch" + 0.030*"Hope" + 0.027*"COP16" + 0.026*"interview" + 0.018*"families"
TopicModeling: GENSIM Topic: 21 
Words: 0.051*"CNN" + 0.044*"Retweet" + 0.029*"speech" + 0.023*"cancer" + 0.023*"Chicago" + 0.023*"Friday" + 0.021*"Pope" + 0.019*"Venezuela" + 0.019*"condoms" + 0.017*"Toronto"
TopicModeling: GENSIM Topic: 22 
Words: 0.062*"Blog" + 0.041*"week" + 0.024*"re" + 0.022*"believe" + 0.021*"book" + 0.021*"We" + 0.018*"change" + 0.017*"basketball" + 0.017*"learn" + 0.014*"see"
TopicModeling: GENSIM Topic: 23 
Words: 0.043*"game" + 0.037*"haha" + 0.029*"show" + 0.027*"football" + 0.026*"RETWEET" + 0.019*"net" + 0.016*"team" + 0.015*"beautiful" + 0.014*"Apple iPad" + 0.014*"win"
TopicModeling: GENSIM Topic: 24 
Words: 0.087*"who" + 0.031*"college" + 0.026*"fat" + 0.023*"hate" + 0.020*"Oh" + 0.020*"me a" + 0.020*"a man" + 0.019*"always" + 0.019*"Nice" + 0.017*"Yes"
TopicModeling: GENSIM Topic: 25 
Words: 0.048*"lol" + 0.040*"tweet" + 0.033*"people" + 0.030*"give me" + 0.028*"today" + 0.024*"time" + 0.024*"you" + 0.021*"now" + 0.020*"twitter" + 0.019*"ur"
TopicModeling: GENSIM Topic: 26 
Words: 0.037*"news" + 0.034*"India" + 0.020*"New Zealand" + 0.017*"business" + 0.015*"dead" + 0.015*"feeling" + 0.014*"killed" + 0.014*"mine" + 0.014*"Australia" + 0.013*"banned"
TopicModeling: GENSIM Topic: 27 
Words: 0.105*"blog" + 0.074*"Back" + 0.034*"post" + 0.023*"Education" + 0.021*"release" + 0.018*"Oprah" + 0.017*"DVD" + 0.017*"Amazon" + 0.014*"web" + 0.012*"MacBook Air"
TopicModeling: GENSIM Topic: 28 
Words: 0.132*"video" + 0.073*"Video" + 0.057*"YouTube" + 0.026*"Holiday" + 0.025*"music" + 0.022*"Cyber Monday" + 0.022*"WOW" + 0.019*"Check" + 0.017*"Live" + 0.011*"Tonight"
TopicModeling: GENSIM Topic: 29 
Words: 0.204*"Thanksgiving" + 0.076*"Happy Thanksgiving" + 0.046*"LOL" + 0.040*"family" + 0.038*"friends" + 0.029*"holiday" + 0.024*"thanksgiving" + 0.023*"Turkey" + 0.017*"tonight" + 0.016*"go"
TopicModeling: Coherences:

TopicModeling: Calculating model coherence:

Setting topics to those of the model: LdaModel<num_terms=100000, num_topics=30, decay=0.5, chunksize=2000>
CorpusAccumulator accumulated stats from 1000 documents
CorpusAccumulator accumulated stats from 2000 documents
CorpusAccumulator accumulated stats from 3000 documents
CorpusAccumulator accumulated stats from 4000 documents
CorpusAccumulator accumulated stats from 5000 documents
CorpusAccumulator accumulated stats from 6000 documents
CorpusAccumulator accumulated stats from 7000 documents
CorpusAccumulator accumulated stats from 8000 documents
CorpusAccumulator accumulated stats from 9000 documents
CorpusAccumulator accumulated stats from 10000 documents
CorpusAccumulator accumulated stats from 11000 documents
CorpusAccumulator accumulated stats from 12000 documents
CorpusAccumulator accumulated stats from 13000 documents
CorpusAccumulator accumulated stats from 14000 documents
CorpusAccumulator accumulated stats from 15000 documents
CorpusAccumulator accumulated stats from 16000 documents
CorpusAccumulator accumulated stats from 17000 documents
CorpusAccumulator accumulated stats from 18000 documents
CorpusAccumulator accumulated stats from 19000 documents
CorpusAccumulator accumulated stats from 20000 documents
CorpusAccumulator accumulated stats from 21000 documents
CorpusAccumulator accumulated stats from 22000 documents
CorpusAccumulator accumulated stats from 23000 documents
CorpusAccumulator accumulated stats from 24000 documents
CorpusAccumulator accumulated stats from 25000 documents
CorpusAccumulator accumulated stats from 26000 documents
CorpusAccumulator accumulated stats from 27000 documents
CorpusAccumulator accumulated stats from 28000 documents
CorpusAccumulator accumulated stats from 29000 documents
CorpusAccumulator accumulated stats from 30000 documents
CorpusAccumulator accumulated stats from 31000 documents
CorpusAccumulator accumulated stats from 32000 documents
CorpusAccumulator accumulated stats from 33000 documents
CorpusAccumulator accumulated stats from 34000 documents
CorpusAccumulator accumulated stats from 35000 documents
CorpusAccumulator accumulated stats from 36000 documents
CorpusAccumulator accumulated stats from 37000 documents
CorpusAccumulator accumulated stats from 38000 documents
CorpusAccumulator accumulated stats from 39000 documents
CorpusAccumulator accumulated stats from 40000 documents
CorpusAccumulator accumulated stats from 41000 documents
CorpusAccumulator accumulated stats from 42000 documents
CorpusAccumulator accumulated stats from 43000 documents
CorpusAccumulator accumulated stats from 44000 documents
CorpusAccumulator accumulated stats from 45000 documents
CorpusAccumulator accumulated stats from 46000 documents
CorpusAccumulator accumulated stats from 47000 documents
CorpusAccumulator accumulated stats from 48000 documents
CorpusAccumulator accumulated stats from 49000 documents
CorpusAccumulator accumulated stats from 50000 documents
CorpusAccumulator accumulated stats from 51000 documents
CorpusAccumulator accumulated stats from 52000 documents
CorpusAccumulator accumulated stats from 53000 documents
CorpusAccumulator accumulated stats from 54000 documents
CorpusAccumulator accumulated stats from 55000 documents
CorpusAccumulator accumulated stats from 56000 documents
CorpusAccumulator accumulated stats from 57000 documents
CorpusAccumulator accumulated stats from 58000 documents
CorpusAccumulator accumulated stats from 59000 documents
CorpusAccumulator accumulated stats from 60000 documents
CorpusAccumulator accumulated stats from 61000 documents
CorpusAccumulator accumulated stats from 62000 documents
CorpusAccumulator accumulated stats from 63000 documents
CorpusAccumulator accumulated stats from 64000 documents
CorpusAccumulator accumulated stats from 65000 documents
CorpusAccumulator accumulated stats from 66000 documents
CorpusAccumulator accumulated stats from 67000 documents
CorpusAccumulator accumulated stats from 68000 documents
CorpusAccumulator accumulated stats from 69000 documents
CorpusAccumulator accumulated stats from 70000 documents
CorpusAccumulator accumulated stats from 71000 documents
CorpusAccumulator accumulated stats from 72000 documents
CorpusAccumulator accumulated stats from 73000 documents
CorpusAccumulator accumulated stats from 74000 documents
CorpusAccumulator accumulated stats from 75000 documents
CorpusAccumulator accumulated stats from 76000 documents
CorpusAccumulator accumulated stats from 77000 documents
CorpusAccumulator accumulated stats from 78000 documents
CorpusAccumulator accumulated stats from 79000 documents
CorpusAccumulator accumulated stats from 80000 documents
CorpusAccumulator accumulated stats from 81000 documents
CorpusAccumulator accumulated stats from 82000 documents
CorpusAccumulator accumulated stats from 83000 documents
CorpusAccumulator accumulated stats from 84000 documents
CorpusAccumulator accumulated stats from 85000 documents
CorpusAccumulator accumulated stats from 86000 documents
CorpusAccumulator accumulated stats from 87000 documents
CorpusAccumulator accumulated stats from 88000 documents
CorpusAccumulator accumulated stats from 89000 documents
CorpusAccumulator accumulated stats from 90000 documents
CorpusAccumulator accumulated stats from 91000 documents
CorpusAccumulator accumulated stats from 92000 documents
CorpusAccumulator accumulated stats from 93000 documents
CorpusAccumulator accumulated stats from 94000 documents
CorpusAccumulator accumulated stats from 95000 documents
CorpusAccumulator accumulated stats from 96000 documents
CorpusAccumulator accumulated stats from 97000 documents
CorpusAccumulator accumulated stats from 98000 documents
CorpusAccumulator accumulated stats from 99000 documents
CorpusAccumulator accumulated stats from 100000 documents
CorpusAccumulator accumulated stats from 101000 documents
CorpusAccumulator accumulated stats from 102000 documents
CorpusAccumulator accumulated stats from 103000 documents
CorpusAccumulator accumulated stats from 104000 documents
CorpusAccumulator accumulated stats from 105000 documents
CorpusAccumulator accumulated stats from 106000 documents
CorpusAccumulator accumulated stats from 107000 documents
CorpusAccumulator accumulated stats from 108000 documents
CorpusAccumulator accumulated stats from 109000 documents
CorpusAccumulator accumulated stats from 110000 documents
CorpusAccumulator accumulated stats from 111000 documents
CorpusAccumulator accumulated stats from 112000 documents
CorpusAccumulator accumulated stats from 113000 documents
CorpusAccumulator accumulated stats from 114000 documents
CorpusAccumulator accumulated stats from 115000 documents
CorpusAccumulator accumulated stats from 116000 documents
CorpusAccumulator accumulated stats from 117000 documents
CorpusAccumulator accumulated stats from 118000 documents
CorpusAccumulator accumulated stats from 119000 documents
CorpusAccumulator accumulated stats from 120000 documents
CorpusAccumulator accumulated stats from 121000 documents
CorpusAccumulator accumulated stats from 122000 documents
CorpusAccumulator accumulated stats from 123000 documents
CorpusAccumulator accumulated stats from 124000 documents
CorpusAccumulator accumulated stats from 125000 documents
CorpusAccumulator accumulated stats from 126000 documents
CorpusAccumulator accumulated stats from 127000 documents
CorpusAccumulator accumulated stats from 128000 documents
CorpusAccumulator accumulated stats from 129000 documents
CorpusAccumulator accumulated stats from 130000 documents
CorpusAccumulator accumulated stats from 131000 documents
CorpusAccumulator accumulated stats from 132000 documents
CorpusAccumulator accumulated stats from 133000 documents
CorpusAccumulator accumulated stats from 134000 documents
CorpusAccumulator accumulated stats from 135000 documents
CorpusAccumulator accumulated stats from 136000 documents
CorpusAccumulator accumulated stats from 137000 documents
CorpusAccumulator accumulated stats from 138000 documents
CorpusAccumulator accumulated stats from 139000 documents
CorpusAccumulator accumulated stats from 140000 documents
CorpusAccumulator accumulated stats from 141000 documents
CorpusAccumulator accumulated stats from 142000 documents
CorpusAccumulator accumulated stats from 143000 documents
CorpusAccumulator accumulated stats from 1000 documents
CorpusAccumulator accumulated stats from 2000 documents
CorpusAccumulator accumulated stats from 3000 documents
CorpusAccumulator accumulated stats from 4000 documents
CorpusAccumulator accumulated stats from 5000 documents
CorpusAccumulator accumulated stats from 6000 documents
CorpusAccumulator accumulated stats from 7000 documents
CorpusAccumulator accumulated stats from 8000 documents
CorpusAccumulator accumulated stats from 9000 documents
CorpusAccumulator accumulated stats from 10000 documents
CorpusAccumulator accumulated stats from 11000 documents
CorpusAccumulator accumulated stats from 12000 documents
CorpusAccumulator accumulated stats from 13000 documents
CorpusAccumulator accumulated stats from 14000 documents
CorpusAccumulator accumulated stats from 15000 documents
CorpusAccumulator accumulated stats from 16000 documents
CorpusAccumulator accumulated stats from 17000 documents
CorpusAccumulator accumulated stats from 18000 documents
CorpusAccumulator accumulated stats from 19000 documents
CorpusAccumulator accumulated stats from 20000 documents
CorpusAccumulator accumulated stats from 21000 documents
CorpusAccumulator accumulated stats from 22000 documents
CorpusAccumulator accumulated stats from 23000 documents
CorpusAccumulator accumulated stats from 24000 documents
CorpusAccumulator accumulated stats from 25000 documents
CorpusAccumulator accumulated stats from 26000 documents
CorpusAccumulator accumulated stats from 27000 documents
CorpusAccumulator accumulated stats from 28000 documents
CorpusAccumulator accumulated stats from 29000 documents
CorpusAccumulator accumulated stats from 30000 documents
CorpusAccumulator accumulated stats from 31000 documents
CorpusAccumulator accumulated stats from 32000 documents
CorpusAccumulator accumulated stats from 33000 documents
CorpusAccumulator accumulated stats from 34000 documents
CorpusAccumulator accumulated stats from 35000 documents
CorpusAccumulator accumulated stats from 36000 documents
CorpusAccumulator accumulated stats from 37000 documents
CorpusAccumulator accumulated stats from 38000 documents
CorpusAccumulator accumulated stats from 39000 documents
CorpusAccumulator accumulated stats from 40000 documents
CorpusAccumulator accumulated stats from 41000 documents
CorpusAccumulator accumulated stats from 42000 documents
CorpusAccumulator accumulated stats from 43000 documents
CorpusAccumulator accumulated stats from 44000 documents
CorpusAccumulator accumulated stats from 45000 documents
CorpusAccumulator accumulated stats from 46000 documents
CorpusAccumulator accumulated stats from 47000 documents
CorpusAccumulator accumulated stats from 48000 documents
CorpusAccumulator accumulated stats from 49000 documents
CorpusAccumulator accumulated stats from 50000 documents
CorpusAccumulator accumulated stats from 51000 documents
CorpusAccumulator accumulated stats from 52000 documents
CorpusAccumulator accumulated stats from 53000 documents
CorpusAccumulator accumulated stats from 54000 documents
CorpusAccumulator accumulated stats from 55000 documents
CorpusAccumulator accumulated stats from 56000 documents
CorpusAccumulator accumulated stats from 57000 documents
CorpusAccumulator accumulated stats from 58000 documents
CorpusAccumulator accumulated stats from 59000 documents
CorpusAccumulator accumulated stats from 60000 documents
CorpusAccumulator accumulated stats from 61000 documents
CorpusAccumulator accumulated stats from 62000 documents
CorpusAccumulator accumulated stats from 63000 documents
CorpusAccumulator accumulated stats from 64000 documents
CorpusAccumulator accumulated stats from 65000 documents
CorpusAccumulator accumulated stats from 66000 documents
CorpusAccumulator accumulated stats from 67000 documents
CorpusAccumulator accumulated stats from 68000 documents
CorpusAccumulator accumulated stats from 69000 documents
CorpusAccumulator accumulated stats from 70000 documents
CorpusAccumulator accumulated stats from 71000 documents
CorpusAccumulator accumulated stats from 72000 documents
CorpusAccumulator accumulated stats from 73000 documents
CorpusAccumulator accumulated stats from 74000 documents
CorpusAccumulator accumulated stats from 75000 documents
CorpusAccumulator accumulated stats from 76000 documents
CorpusAccumulator accumulated stats from 77000 documents
CorpusAccumulator accumulated stats from 78000 documents
CorpusAccumulator accumulated stats from 79000 documents
CorpusAccumulator accumulated stats from 80000 documents
CorpusAccumulator accumulated stats from 81000 documents
CorpusAccumulator accumulated stats from 82000 documents
CorpusAccumulator accumulated stats from 83000 documents
CorpusAccumulator accumulated stats from 84000 documents
CorpusAccumulator accumulated stats from 85000 documents
CorpusAccumulator accumulated stats from 86000 documents
CorpusAccumulator accumulated stats from 87000 documents
CorpusAccumulator accumulated stats from 88000 documents
CorpusAccumulator accumulated stats from 89000 documents
CorpusAccumulator accumulated stats from 90000 documents
CorpusAccumulator accumulated stats from 91000 documents
CorpusAccumulator accumulated stats from 92000 documents
CorpusAccumulator accumulated stats from 93000 documents
CorpusAccumulator accumulated stats from 94000 documents
CorpusAccumulator accumulated stats from 95000 documents
CorpusAccumulator accumulated stats from 96000 documents
CorpusAccumulator accumulated stats from 97000 documents
CorpusAccumulator accumulated stats from 98000 documents
CorpusAccumulator accumulated stats from 99000 documents
CorpusAccumulator accumulated stats from 100000 documents
CorpusAccumulator accumulated stats from 101000 documents
CorpusAccumulator accumulated stats from 102000 documents
CorpusAccumulator accumulated stats from 103000 documents
CorpusAccumulator accumulated stats from 104000 documents
CorpusAccumulator accumulated stats from 105000 documents
CorpusAccumulator accumulated stats from 106000 documents
CorpusAccumulator accumulated stats from 107000 documents
CorpusAccumulator accumulated stats from 108000 documents
CorpusAccumulator accumulated stats from 109000 documents
CorpusAccumulator accumulated stats from 110000 documents
CorpusAccumulator accumulated stats from 111000 documents
CorpusAccumulator accumulated stats from 112000 documents
CorpusAccumulator accumulated stats from 113000 documents
CorpusAccumulator accumulated stats from 114000 documents
CorpusAccumulator accumulated stats from 115000 documents
CorpusAccumulator accumulated stats from 116000 documents
CorpusAccumulator accumulated stats from 117000 documents
CorpusAccumulator accumulated stats from 118000 documents
CorpusAccumulator accumulated stats from 119000 documents
CorpusAccumulator accumulated stats from 120000 documents
CorpusAccumulator accumulated stats from 121000 documents
CorpusAccumulator accumulated stats from 122000 documents
CorpusAccumulator accumulated stats from 123000 documents
CorpusAccumulator accumulated stats from 124000 documents
CorpusAccumulator accumulated stats from 125000 documents
CorpusAccumulator accumulated stats from 126000 documents
CorpusAccumulator accumulated stats from 127000 documents
CorpusAccumulator accumulated stats from 128000 documents
CorpusAccumulator accumulated stats from 129000 documents
CorpusAccumulator accumulated stats from 130000 documents
CorpusAccumulator accumulated stats from 131000 documents
CorpusAccumulator accumulated stats from 132000 documents
CorpusAccumulator accumulated stats from 133000 documents
CorpusAccumulator accumulated stats from 134000 documents
CorpusAccumulator accumulated stats from 135000 documents
CorpusAccumulator accumulated stats from 136000 documents
CorpusAccumulator accumulated stats from 137000 documents
CorpusAccumulator accumulated stats from 138000 documents
CorpusAccumulator accumulated stats from 139000 documents
CorpusAccumulator accumulated stats from 140000 documents
CorpusAccumulator accumulated stats from 141000 documents
CorpusAccumulator accumulated stats from 142000 documents
CorpusAccumulator accumulated stats from 143000 documents
TopicModeling: Coherence value is: -3.191394226303306
TopicModeling: Topic coherences are: [-3.379854221790771, -2.000686428052567, -3.8069996221488616, -2.9544535949168322, -2.645982078390766, -4.772187634618854, -2.590541308821359, -3.658850473521215, -2.389475825284664, -2.705873946904908, -1.5413658151715444, -2.153496432711869, -4.461993988734708, -3.7165043320666675, -2.637405675771868, -3.084995875095761, -3.1446197240555005, -1.7238220432877565, -2.3322501275059344, -3.0574116207285558, -2.9842965202044542, -3.4057125898037, -2.8091300747711703, -3.398052683277595, -2.5275536483520527, -1.6760968770486493, -2.7574308876429665, -3.4326370469252008, -2.9057188289351035, -2.1889334864879726]
Dictionary lifecycle event {'fname_or_handle': '../output/19/tml/gensim_30topics_TopicModelingDictionary.mm', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-05-27T15:46:19.794674', 'gensim': '4.2.0', 'python': '3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'saving'}
{'uri': '../output/19/tml/gensim_30topics_TopicModelingDictionary.mm', 'mode': 'wb', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd': True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}
saved ../output/19/tml/gensim_30topics_TopicModelingDictionary.mm
dictionary.shape: 100000
bow_corpus.shape: 143749
totalTopics: [['Haiti', 'cholera', 'UN', 'Amazon.com', 'election', 'death', 'Israeli', 'Cholera', 'Asian', 'Gaza'], ['Obama', 'TSA', 'GOP', 'ppl', 'Palin', 'Senate', 'Congress', 'vote', 'Republicans', 'Sarah Palin'], ['bit.ly', 'News', 'NFL', 'English', 'USD', 'Boston', 'Texas', 'You', 'LMAO', 'songs'], ['UK', 'London', 'BBC', 'police', 'students', 'protest', 'school', 'British', 'Colombia', 'failure'], ['President', 'American', 'turkey', 'U.S', 'Obama', 'White House', 'President Obama', 'Hoy', 'Americans', 'Seoul'], ['NBC', 'IM', 'TODAY', 'fb', 'season', 'student', 'smile', 'high school', 'Football', 'method'], ['News', 'U.S', 'Business', 'World', 'Bloomberg', 'Health', 'New York', 'Will', 'Women', 'Pres'], ['Twitter', 'Facebook', 'NYC', 'Tweet', 'Lady Gaga', 'photos', 'website', 'Chinese', 'Founder', 'wind'], ['US', 'Iran', 'Israel', 'Afghanistan', 'Pakistan', 'NATO', 'Russia', 'Egypt', 'Nato', 'to win'], ['kids', 'will', 'work', 'god', 'down', 'do', 'mind', 'children', 'talk', 'soul'], ['Como', 'un', 'para', 'como', 'es', 'en', 'la', 'dos', 'este', 'más'], ['via', 'iPad', 'iPhone', 'Google', 'Facebook', 'Apple', 'Android', 'Christmas', 'iTunes', 'Free'], ['God', 'Who', 'al', 'Celebrity', 'help', 'TED', 'dogs', 'guy', 'heart', 'fashion'], ['AP', 'DADT', 'Breaking News', 'add', 'study', 'Notre Dame', 'child', 'Art', 'prison', 'report'], ['hanging', 'tcot', 'Bush', 'jobs', 'Obama', 'arrested', 'GM', 'Ohio', 'job', 'shot'], ['Home', 'sex', 'USA', 'women', 'here', 'snow', 'girl', 'England', 'School', 'travel'], ['party', 'Black Friday', 'read', 'Sarah Palin', 'chat', 'cash', 'HIV', 'DWTS', 'gay', 'If'], ['China', 'North Korea', 'South Korea', 'Ireland', 'Korea', 'U.S', 'Irish', 'US', 'Japan', 'attack'], ['WikiLeaks', 'Wikileaks', 'wikileaks', 'Wikipedia', 'cablegate', 'US', 'secret', 'Wiki Leaks', 'Asia', 'Guardian'], ['Love', 'Leslie Nielsen', 'Today', 'need', 'Music', 'Time', 'right now', 'nice', 'Jets', 'Life'], ['live', 'world', 'Check it', 'Check it out', 'TV', 'Watch', 'Hope', 'COP16', 'interview', 'families'], ['CNN', 'Retweet', 'speech', 'cancer', 'Chicago', 'Friday', 'Pope', 'Venezuela', 'condoms', 'Toronto'], ['Blog', 'week', 're', 'believe', 'book', 'We', 'change', 'basketball', 'learn', 'see'], ['game', 'haha', 'show', 'football', 'RETWEET', 'net', 'team', 'beautiful', 'Apple iPad', 'win'], ['who', 'college', 'fat', 'hate', 'Oh', 'me a', 'a man', 'always', 'Nice', 'Yes'], ['lol', 'tweet', 'people', 'give me', 'today', 'time', 'you', 'now', 'twitter', 'ur'], ['news', 'India', 'New Zealand', 'business', 'dead', 'feeling', 'killed', 'mine', 'Australia', 'banned'], ['blog', 'Back', 'post', 'Education', 'release', 'Oprah', 'DVD', 'Amazon', 'web', 'MacBook Air'], ['video', 'Video', 'YouTube', 'Holiday', 'music', 'Cyber Monday', 'WOW', 'Check', 'Live', 'Tonight'], ['Thanksgiving', 'Happy Thanksgiving', 'LOL', 'family', 'friends', 'holiday', 'thanksgiving', 'Turkey', 'tonight', 'go']]
Users' graph generating ...
UserSimilarity: All users size 143749
UserSimilarity: All distinct users:61224
UserSimilarity: users_topic_interests=(61224, 30)
UserSimilarity: Just one topic? False, Binary topic? False, Threshold: 0.02
1092 users have twitted in 2010-11-15 00:00:00
1266 users have twitted in 2010-11-16 00:00:00
1692 users have twitted in 2010-11-17 00:00:00
8414 users have twitted in 2010-11-18 00:00:00
14924 users have twitted in 2010-11-19 00:00:00
11778 users have twitted in 2010-11-20 00:00:00
4270 users have twitted in 2010-11-21 00:00:00
4753 users have twitted in 2010-11-22 00:00:00
13272 users have twitted in 2010-11-23 00:00:00
14471 users have twitted in 2010-11-24 00:00:00
13433 users have twitted in 2010-11-25 00:00:00
12399 users have twitted in 2010-11-26 00:00:00
6227 users have twitted in 2010-11-27 00:00:00
9516 users have twitted in 2010-11-28 00:00:00
11251 users have twitted in 2010-11-29 00:00:00
14991 users have twitted in 2010-11-30 00:00:00
0 users have twitted in 2010-12-01 00:00:00
UserSimilarity: 0 / 17
UserSimilarity: UsersTopicInterests.npy is saved for day:0 with shape: (61224, 30)
UsersGraph: There are 61224 users on 0
